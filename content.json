{"pages":[{"title":"","text":"404","link":"/404.html"},{"title":"关于我","text":"个人简介 本科计算机科学与技术专业 从事JAVA后端开发 码畜一枚 坚信代码改变世界 博客 function createtime(){var n=new Date(\"11/11/2018 00:00:00\");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum=\"0\"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum=\"0\"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum=\"0\"+snum),document.getElementById(\"timeDate\").innerHTML=\"自❤️2018.11.11❤️已运行 \"+dnum+\" 天 \",document.getElementById(\"times\").innerHTML=hnum+\" 小时 \"+mnum+\" 分 \"+snum+\" 秒！\"}var now=new Date;setInterval(\"createtime()\",250) 网站采用的Icarus主题 本站推荐索引 知识点 中华人名共和国网络安全法 Java并发知识点 其他 网易云音乐歌单分享 记录 16:24:05/07/19/2019 相看两不厌，只有敬亭山 –李白 15:06:28/06/06/2019 爱情就像在海滩上捡贝壳，不要捡最大的， 也不要捡最漂亮的，要捡就捡自己最喜欢的， 最重要的是捡到了自己喜欢的 就永远不要再去海边了。 –公众号文章 11:07/05/22/2019 无论走到哪里，都应该记住。过去都是假的，回忆是一条没有尽头的路，一切以往的春天都不复存在，就连那最坚韧而又狂乱的爱情归根结底也不过是一种转瞬即逝的现实。 –马尔克斯《百年孤独》 11:05/05/22/2019 我觉得美不是一切，它很浪费人生。美要加上滋味、加上开心、加上别的东西，才是人生的美满。 –张曼玉 15:37/05/06/2019 Under no circumstances will I give up loving you.山无棱天地合才敢与君绝。 –《mooc学院》 17:15/04/26/2019 一个人无法自成孤岛，要么至少，一个人无法自成最理想的孤岛。 –《岛上书店》 17:12/04/22/2019 你写下的每一个bug，都是人类反抗被人工智能统治的一颗子弹 17:12/04/22/2019 所谓活着并不是单纯的呼吸，心脏跳动，也不是脑电波，而是在这个世界上留下痕迹。要能看见自己一路走来的脚印，并确信那些都是自己留下的印记，这才叫活着。 –《东野圭吾》 2019计划21:59/12/31/2018 2019-flag 购买的专业书籍至少看完一遍（并发、重构、设计模式…） 微信读书每天一个小时(年终总计300小时) 坚持每周去两次健身房(多练肚子、力量、学会蝶泳) 至少完成一项 前后端分离项目 完成一项 微服务项目(类似公司使用相关技术) 不辞职 多交朋友、多换位思考、多与朋友交流沟通 居安思危，多思考关注相关专业前景，生活环境 Java基础技能强化 多买书 学习更多的新菜(至少三项) 少买电子产品，少网购，少逛数码产品 学英语记单词、学数学、多看视频教程 少玩游戏","link":"/about/index.html"},{"title":"个人相册","text":"记录生活美好瞬间 全部来自个人拍照。因图片存于github上加载速度较慢，请耐心等待一下。欢迎访问500px😊。","link":"/album/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"最新评论列表","text":"ps:latest comment exists 10 minutes delay…","link":"/comment/index.html"},{"title":"音乐欣赏","text":"温馨提示：选择喜欢的音乐双击播放，由于版权原因部分不能播放。如果喜欢歌单收藏一下，去网易云都能播放哟！","link":"/music/index.html"},{"title":"留言板","text":"畅所欲言有留必应","link":"/message/index.html"},{"title":"音乐歌单收藏","text":"温馨提示：选择喜欢的音乐双击播放，由于版权原因部分不能播放。如果喜欢歌单收藏一下，去网易云都能播放哟！","link":"/music1/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"mysql高性能优化规范方法","text":"数据库命令规范 所有数据库对象名称必须使用小写字母并用下划线分割 所有数据库对象名称禁止使用mysql保留关键字（如果表名中包含关键字查询时，需要将其用单引号括起来） 数据库对象的命名要能做到见名识意，并且最后不要超过32个字符 临时库表必须以tmp_为前缀并以日期为后缀，备份表必须以bak_为前缀并以日期(时间戳)为后缀 所有存储相同数据的列名和列类型必须一致（一般作为关联列，如果查询时关联列类型不一致会自动进行数据类型隐式转换，会造成列上的索引失效，导致查询效率降低） 数据库基本设计规范所有表必须使用Innodb存储引擎没有特殊要求（即Innodb无法满足的功能如：列存储，存储空间数据等）的情况下，所有表必须使用Innodb存储引擎（mysql5.5之前默认使用Myisam，5.6以后默认的为Innodb）。 Innodb 支持事务，支持行级锁，更好的恢复性，高并发下性能更好。 数据库和表的字符集统一使用UTF8兼容性更好，统一字符集可以避免由于字符集转换产生的乱码，不同的字符集进行比较前需要进行转换会造成索引失效，如果数据库中有存储emoji表情的需要，字符集需要采用utf8mb4字符集。 所有表和字段都需要添加注释使用comment从句添加表和列的备注，从一开始就进行数据字典的维护 尽量控制单表数据量的大小，建议控制在500万以内。500万并不是Mysql数据库的限制，过大会造成修改表结构，备份，恢复都会有很大的问题。 可以用历史数据归档（应用于日志数据），分库分表（应用于业务数据）等手段来控制数据量大小 谨慎使用Mysql分区表分区表在物理上表现为多个文件，在逻辑上表现为一个表； 谨慎选择分区键，跨分区查询效率可能更低； 建议采用物理分表的方式管理大数据。 尽量做到冷热数据分离，减小表的宽度Mysql限制每个表最多存储4096列，并且每一行数据的大小不能超过65535字节。 减少磁盘IO,保证热数据的内存缓存命中率（表越宽，把表装载进内存缓冲池时所占用的内存也就越大,也会消耗更多的IO）； 更有效的利用缓存，避免读入无用的冷数据； 经常一起使用的列放到一个表中（避免更多的关联操作）。 禁止在表中建立预留字段预留字段的命名很难做到见名识义。 预留字段无法确认存储的数据类型，所以无法选择合适的类型。 对预留字段类型的修改，会对表进行锁定。 禁止在数据库中存储图片，文件等大的二进制数据通常文件很大，会短时间内造成数据量快速增长，数据库进行数据库读取时，通常会进行大量的随机IO操作，文件很大时，IO操作很耗时。 通常存储于文件服务器，数据库只存储文件地址信息 禁止在线上做数据库压力测试禁止从开发环境，测试环境直接连接生产环境数据库 数据库字段设计规范优先选择符合存储需要的最小的数据类型原因： 列的字段越大，建立索引时所需要的空间也就越大，这样一页中所能存储的索引节点的数量也就越少也越少，在遍历时所需要的IO次数也就越多，索引的性能也就越差。 方法： 将字符串转换成数字类型存储，如：将IP地址转换成整形数据mysql提供了两个方法来处理ip地址 inet_aton 把ip转为无符号整型(4-8位) inet_ntoa 把整型的ip转为地址 插入数据前，先用inet_aton把ip地址转为整型，可以节省空间，显示数据时，使用inet_ntoa把整型的ip地址转为地址显示即可。 对于非负型的数据（如自增ID、整型IP）来说，要优先使用无符号整型来存储原因： 无符号相对于有符号可以多出一倍的存储空间 12SIGNED INT -2147483648~2147483647UNSIGNED INT 0~4294967295 VARCHAR(N)中的N代表的是字符数，而不是字节数，使用UTF8存储255个汉字 Varchar(255)=765个字节。过大的长度会消耗更多的内存。 避免使用TEXT、BLOB数据类型，最常见的TEXT类型可以存储64k的数据建议把BLOB或是TEXT列分离到单独的扩展表中Mysql内存临时表不支持TEXT、BLOB这样的大数据类型，如果查询中包含这样的数据，在排序等操作时，就不能使用内存临时表，必须使用磁盘临时表进行。而且对于这种数据，Mysql还是要进行二次查询，会使sql性能变得很差，但是不是说一定不能使用这样的数据类型。 如果一定要使用，建议把BLOB或是TEXT列分离到单独的扩展表中，查询时一定不要使用select * 而只需要取出必要的列，不需要TEXT列的数据时不要对该列进行查询。 TEXT或BLOB类型只能使用前缀索引因为MySQL对索引字段长度是有限制的，所以TEXT类型只能使用前缀索引，并且TEXT列上是不能有默认值的 避免使用ENUM类型修改ENUM值需要使用ALTER语句 ENUM类型的ORDER BY操作效率低，需要额外操作 禁止使用数值作为ENUM的枚举值 尽可能把所有列定义为NOT NULL原因： 索引NULL列需要额外的空间来保存，所以要占用更多的空间 进行比较和计算时要对NULL值做特别的处理 使用TIMESTAMP（4个字节）或DATETIME类型（8个字节）存储时间TIMESTAMP 存储的时间范围 1970-01-01 00:00:01 ~ 2038-01-19-03:14:07 TIMESTAMP 占用4字节和INT相同，但比INT可读性高 超出TIMESTAMP取值范围的使用DATETIME类型存储 经常会有人用字符串存储日期型的数据（不正确的做法） 缺点1：无法用日期函数进行计算和比较 缺点2：用字符串存储日期要占用更多的空间 同财务相关的金额类数据必须使用decimal类型 非精准浮点：float,double 精准浮点：decimal Decimal类型为精准浮点数，在计算时不会丢失精度 占用空间由定义的宽度决定，每4个字节可以存储9位数字，并且小数点要占用一个字节 可用于存储比bigint更大的整型数据 索引设计规范限制每张表上的索引数量，建议单张表索引不超过5个索引并不是越多越好！索引可以提高效率同样可以降低效率。 索引可以增加查询效率，但同样也会降低插入和更新的效率，甚至有些情况下会降低查询效率。 因为mysql优化器在选择如何优化查询时，会根据统一信息，对每一个可以用到的索引来进行评估，以生成出一个最好的执行计划，如果同时有很多个索引都可以用于查询，就会增加mysql优化器生成执行计划的时间，同样会降低查询性能。 禁止给表中的每一列都建立单独的索引5.6版本之前，一个sql只能使用到一个表中的一个索引，5.6以后，虽然有了合并索引的优化方式，但是还是远远没有使用一个联合索引的查询方式好。 每个Innodb表必须有个主键Innodb是一种索引组织表：数据的存储的逻辑顺序和索引的顺序是相同的。每个表都可以有多个索引，但是表的存储顺序只能有一种。 Innodb是按照主键索引的顺序来组织表的 不要使用更新频繁的列作为主键，不适用多列主键（相当于联合索引） 不要使用UUID,MD5,HASH,字符串列作为主键（无法保证数据的顺序增长） 主键建议使用自增ID值 常见索引列建议 出现在SELECT、UPDATE、DELETE语句的WHERE从句中的列 包含在ORDER BY、GROUP BY、DISTINCT中的字段 并不要将符合1和2中的字段的列都建立一个索引， 通常将1、2中的字段建立联合索引效果更好 多表join的关联列 如何选择索引列的顺序建立索引的目的是：希望通过索引进行数据查找，减少随机IO，增加查询性能 ，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少。 区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数） 尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO性能也就越好） 使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引） 避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间） 重复索引示例：primary key(id)、index(id)、unique index(id) 冗余索引示例：index(a,b,c)、index(a,b)、index(a) 对于频繁的查询优先考虑使用覆盖索引覆盖索引：就是包含了所有查询字段(where,select,ordery by,group by包含的字段)的索引 覆盖索引的好处： 避免Innodb表进行索引的二次查询Innodb是以聚集索引的顺序来存储的，对于Innodb来说，二级索引在叶子节点中所保存的是行的主键信息，如果是用二级索引查询数据的话，在查找到相应的键值后，还要通过主键进行二次查询才能获取我们真实所需要的数据。 而在覆盖索引中，二级索引的键值中可以获取所有的数据，避免了对主键的二次查询 ，减少了IO操作，提升了查询效率。 可以把随机IO变成顺序IO加快查询效率由于覆盖索引是按键值的顺序存储的，对于IO密集型的范围查找来说，对比随机从磁盘读取每一行的数据IO要少的多，因此利用覆盖索引在访问时也可以把磁盘的随机读取的IO转变成索引查找的顺序IO。 索引SET规范尽量避免使用外键约束 不建议使用外键约束（foreign key），但一定要在表与表之间的关联键上建立索引 外键可用于保证数据的参照完整性，但建议在业务端实现 外键会影响父表和子表的写操作从而降低性能 数据库SQL开发规范建议使用预编译语句进行数据库操作预编译语句可以重复使用这些计划，减少SQL编译所需要的时间，还可以解决动态SQL所带来的SQL注入的问题。 只传参数，比传递SQL语句更高效。 相同语句可以一次解析，多次使用，提高处理效率。 避免数据类型的隐式转换隐式转换会导致索引失效如: 1select name,phone from customer where id = '111'; 充分利用表上已经存在的索引s避免使用双%号的查询条件。如：a like '%123%'，（如果无前置%,只有后置%，是可以用到列上的索引的） 一个SQL只能利用到复合索引中的一列进行范围查询。如：有 a,b,c列的联合索引，在查询条件中有a列的范围查询，则在b,c列上的索引将不会被用到。 在定义联合索引时，如果a列要用到范围查找的话，就要把a列放到联合索引的右侧，使用left join 或 not exists来优化not in 操作，因为not in 也通常会使用索引失效。 数据库设计时，应该要对以后扩展进行考虑程序连接不同的数据库使用不同的账号，进制跨库查询 为数据库迁移和分库分表留出余地 降低业务耦合度 避免权限过大而产生的安全风险 禁止使用SELECT * 必须使用SELECT &lt;字段列表&gt; 查询原因： 消耗更多的CPU和IO以网络带宽资源 无法使用覆盖索引 可减少表结构变更带来的影响 禁止使用不含字段列表的INSERT语句如： 1insert into values ('a','b','c'); 应使用： 1insert into t(c1,c2,c3) values ('a','b','c'); 避免使用子查询，可以把子查询优化为join操作通常子查询在in子句中，且b中为简单SQL(不包含union、group by、order by、limit从句)时,才可以把子查询转化为关联查询进行优化。 子查询性能差的原因： 子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响。特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大。 由于子查询会产生大量的临时表也没有索引，所以会消耗过多的CPU和IO资源，产生大量的慢查询。 避免使用JOIN关联太多的表对于Mysql来说，是存在关联缓存的，缓存的大小可以由join_buffer_size参数进行设置。 在Mysql中，对于同一个SQL多关联（join）一个表，就会多分配一个关联缓存，如果在一个SQL中关联的表越多，所占用的内存也就越大。 如果程序中大量的使用了多表关联的操作，同时join_buffer_size设置的也不合理的情况下，就容易造成服务器内存溢出的情况，就会影响到服务器数据库性能的稳定性。 同时对于关联操作来说，会产生临时表操作，影响查询效率，Mysql最多允许关联61个表，建议不超过5个。 减少同数据库的交互次数数据库更适合处理批量操作，合并多个相同的操作到一起，可以提高处理效率。 对应同一列进行or判断时，使用in代替orin 的值不要超过500个，in 操作可以更有效的利用索引，or大多数情况下很少能利用到索引。 禁止使用order by rand() 进行随机排序order by rand()会把表中所有符合条件的数据装载到内存中，然后在内存中对所有数据根据随机生成的值进行排序，并且可能会对每一行都生成一个随机值，如果满足条件的数据集非常大，就会消耗大量的CPU和IO及内存资源。 推荐在程序中获取一个随机值，然后从数据库中获取数据的方式。 WHERE从句中禁止对列进行函数转换和计算对列进行函数转换或计算时会导致无法使用索引 不推荐： 1where date(create_time)='20190101' 推荐： 1where create_time &gt;= '20190101' and create_time &lt; '20190102' 在明显不会有重复值时使用UNION ALL 而不是UNION UNION 会把两个结果集的所有数据放到临时表中后再进行去重操作 UNION ALL 不会再对结果集进行去重操作 拆分复杂的大SQL为多个小SQL 大SQL逻辑上比较复杂，需要占用大量CPU进行计算的SQL MySQL中，一个SQL只能使用一个CPU进行计算 SQL拆分后可以通过并行执行来提高处理效率 数据库操作行为规范超100万行的批量写（UPDATE、DELETE、INSERT）操作，要分批多次进行操作大批量操作可能会造成严重的主从延迟 主从环境中,大批量操作可能会造成严重的主从延迟，大批量的写操作一般都需要执行一定长的时间，而只有当主库上执行完成后，才会在其他从库上执行，所以会造成主库与从库长时间的延迟情况 binlog日志为row格式时会产生大量的日志 大批量写操作会产生大量日志，特别是对于row格式二进制数据而言，由于在row格式中会记录每一行数据的修改，我们一次修改的数据越多，产生的日志量也就会越多，日志的传输和恢复所需要的时间也就越长，这也是造成主从延迟的一个原因。 避免产生大事务操作 大批量修改数据，一定是在一个事务中进行的，这就会造成表中大批量数据进行锁定，从而导致大量的阻塞，阻塞会对MySQL的性能产生非常大的影响。 特别是长时间的阻塞会占满所有数据库的可用连接，这会使生产环境中的其他应用无法连接到数据库，因此一定要注意大批量写操作要进行分批 对于大表使用pt-online-schema-change修改表结构 避免大表修改产生的主从延迟 避免在对表字段进行修改时进行锁表 对大表数据结构的修改一定要谨慎，会造成严重的锁表操作，尤其是生产环境，是不能容忍的。 pt-online-schema-change它会首先建立一个与原表结构相同的新表，并且在新表上进行表结构的修改，然后再把原表中的数据复制到新表中，并在原表中增加一些触发器。把原表中新增的数据也复制到新表中，在行所有数据复制完成之后，把新表命名成原表，并把原来的表删除掉。把原来一个DDL操作，分解成多个小的批次进行。 禁止为程序使用的账号赋予super权限 当达到最大连接数限制时，还运行1个有super权限的用户连接 super权限只能留给DBA处理问题的账号使用 对于程序连接数据库账号，遵循权限最小原则 程序使用数据库账号只能在一个DB下使用，不准跨库 程序使用的账号原则上不准有drop权限 文章来源 .","link":"/2019/08/21/mysql高性能优化规范方法.html"},{"title":"Java容器集合","text":"容器主要包括 Collection 和 Map 两种，Collection 存储着对象的集合，而 Map 存储着键值对（两个对象）的映射表。 概览容器主要包括 Collection 和 Map 两种，Collection 存储着对象的集合，而 Map 存储着键值对（两个对象）的映射表。 Collection 1. Set TreeSet：基于红黑树实现，支持有序性操作，例如根据一个范围查找元素的操作。但是查找效率不如 HashSet，HashSet 查找的时间复杂度为 O(1)，TreeSet 则为 O(logN)。 HashSet：基于哈希表实现，支持快速查找，但不支持有序性操作。并且失去了元素的插入顺序信息，也就是说使用 Iterator 遍历 HashSet 得到的结果是不确定的。 LinkedHashSet：具有 HashSet 的查找效率，且内部使用双向链表维护元素的插入顺序。 2. List ArrayList：基于动态数组实现，支持随机访问。 Vector：和 ArrayList 类似，但它是线程安全的。 LinkedList：基于双向链表实现，只能顺序访问，但是可以快速地在链表中间插入和删除元素。不仅如此，LinkedList 还可以用作栈、队列和双向队列。 3. Queue LinkedList：可以用它来实现双向队列。 PriorityQueue：基于堆结构实现，可以用它来实现优先队列。 Map TreeMap：基于红黑树实现。 HashMap：基于哈希表实现。 HashTable：和 HashMap 类似，但它是线程安全的，这意味着同一时刻多个线程可以同时写入 HashTable 并且不会导致数据不一致。它是遗留类，不应该去使用它。现在可以使用 ConcurrentHashMap 来支持线程安全，并且 ConcurrentHashMap 的效率会更高，因为 ConcurrentHashMap 引入了分段锁。 LinkedHashMap：使用双向链表来维护元素的顺序，顺序为插入顺序或者最近最少使用（LRU）顺序。 容器中的设计模式迭代器模式 Collection 继承了 Iterable 接口，其中的 iterator() 方法能够产生一个 Iterator 对象，通过这个对象就可以迭代遍历 Collection 中的元素。 从 JDK 1.5 之后可以使用 foreach 方法来遍历实现了 Iterable 接口的聚合对象。 123456List&lt;String&gt; list = new ArrayList&lt;&gt;();list.add(\"a\");list.add(\"b\");for (String item : list) { System.out.println(item);} 适配器模式java.util.Arrays#asList() 可以把数组类型转换为 List 类型。 12@SafeVarargspublic static &lt;T&gt; List&lt;T&gt; asList(T... a) 应该注意的是 asList() 的参数为泛型的变长参数，不能使用基本类型数组作为参数，只能使用相应的包装类型数组。 12Integer[] arr = {1, 2, 3};List list = Arrays.asList(arr); 也可以使用以下方式调用 asList()： 1List list = Arrays.asList(1, 2, 3); 源码分析如果没有特别说明，以下源码分析基于 JDK 1.8。 在 IDEA 中 double shift 调出 Search EveryWhere，查找源码文件，找到之后就可以阅读源码。 ArrayList1. 概览因为 ArrayList 是基于数组实现的，所以支持快速随机访问。RandomAccess 接口标识着该类支持快速随机访问。 12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable 数组的默认大小为 10。 1private static final int DEFAULT_CAPACITY = 10; 2. 扩容添加元素时使用 ensureCapacityInternal() 方法来保证容量足够，如果不够时，需要使用 grow() 方法进行扩容，新容量的大小为 oldCapacity + (oldCapacity &gt;&gt; 1)，也就是旧容量的 1.5 倍。 扩容操作需要调用 Arrays.copyOf() 把原数组整个复制到新数组中，这个操作代价很高，因此最好在创建 ArrayList 对象时就指定大概的容量大小，减少扩容操作的次数。 12345678910111213141516171819202122232425262728293031public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;}private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity);}private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);}private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);} 3. 删除元素需要调用 System.arraycopy() 将 index+1 后面的元素都复制到 index 位置上，该操作的时间复杂度为 O(N)，可以看出 ArrayList 删除元素的代价是非常高的。 12345678910public E remove(int index) { rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;} 4. Fail-FastmodCount 用来记录 ArrayList 结构发生变化的次数。结构发生变化是指添加或者删除至少一个元素的所有操作，或者是调整内部数组的大小，仅仅只是设置元素的值不算结构发生变化。 在进行序列化或者迭代等操作时，需要比较操作前后 modCount 是否改变，如果改变了需要抛出 ConcurrentModificationException。 123456789101112131415161718private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException{ // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. for (int i=0; i&lt;size; i++) { s.writeObject(elementData[i]); } if (modCount != expectedModCount) { throw new ConcurrentModificationException(); }} 5. 序列化ArrayList 基于数组实现，并且具有动态扩容特性，因此保存元素的数组不一定都会被使用，那么就没必要全部进行序列化。 保存元素的数组 elementData 使用 transient 修饰，该关键字声明数组默认不会被序列化。 1transient Object[] elementData; // non-private to simplify nested class access ArrayList 实现了 writeObject() 和 readObject() 来控制只序列化数组中有元素填充那部分内容。 123456789101112131415161718192021222324252627282930313233343536373839private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException { elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff s.defaultReadObject(); // Read in capacity s.readInt(); // ignored if (size &gt; 0) { // be like clone(), allocate array based upon size not capacity ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) { a[i] = s.readObject(); } }}private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException{ // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. for (int i=0; i&lt;size; i++) { s.writeObject(elementData[i]); } if (modCount != expectedModCount) { throw new ConcurrentModificationException(); }} 序列化时需要使用 ObjectOutputStream 的 writeObject() 将对象转换为字节流并输出。而 writeObject() 方法在传入的对象存在 writeObject() 的时候会去反射调用该对象的 writeObject() 来实现序列化。反序列化使用的是 ObjectInputStream 的 readObject() 方法，原理类似。 123ArrayList list = new ArrayList();ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(file));oos.writeObject(list); Vector1. 同步它的实现与 ArrayList 类似，但是使用了 synchronized 进行同步。 12345678910111213public synchronized boolean add(E e) { modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;}public synchronized E get(int index) { if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); return elementData(index);} 2. 与 ArrayList 的比较 Vector 是同步的，因此开销就比 ArrayList 要大，访问速度更慢。最好使用 ArrayList 而不是 Vector，因为同步操作完全可以由程序员自己来控制； Vector 每次扩容请求其大小的 2 倍空间，而 ArrayList 是 1.5 倍。 3. 替代方案可以使用 Collections.synchronizedList(); 得到一个线程安全的 ArrayList。 12List&lt;String&gt; list = new ArrayList&lt;&gt;();List&lt;String&gt; synList = Collections.synchronizedList(list); 也可以使用 concurrent 并发包下的 CopyOnWriteArrayList 类。 1List&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); CopyOnWriteArrayList读写分离写操作在一个复制的数组上进行，读操作还是在原始数组中进行，读写分离，互不影响。 写操作需要加锁，防止并发写入时导致写入数据丢失。 写操作结束之后需要把原始数组指向新的复制数组。 12345678910111213141516171819202122public boolean add(E e) { final ReentrantLock lock = this.lock; lock.lock(); try { Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; setArray(newElements); return true; } finally { lock.unlock(); }}final void setArray(Object[] a) { array = a;}@SuppressWarnings(\"unchecked\")private E get(Object[] a, int index) { return (E) a[index];} 适用场景CopyOnWriteArrayList 在写操作的同时允许读操作，大大提高了读操作的性能，因此很适合读多写少的应用场景。 但是 CopyOnWriteArrayList 有其缺陷： 内存占用：在写操作时需要复制一个新的数组，使得内存占用为原来的两倍左右； 数据不一致：读操作不能读取实时性的数据，因为部分写操作的数据还未同步到读数组中。 所以 CopyOnWriteArrayList 不适合内存敏感以及对实时性要求很高的场景。 LinkedList1. 概览基于双向链表实现，使用 Node 存储链表节点信息。 12345private static class Node&lt;E&gt; { E item; Node&lt;E&gt; next; Node&lt;E&gt; prev;} 每个链表存储了 first 和 last 指针： 12transient Node&lt;E&gt; first;transient Node&lt;E&gt; last; 2. 与 ArrayList 的比较 ArrayList 基于动态数组实现，LinkedList 基于双向链表实现； ArrayList 支持随机访问，LinkedList 不支持； LinkedList 在任意位置添加删除元素更快。 HashMap为了便于理解，以下源码分析以 JDK 1.7 为主。 1. 存储结构内部包含了一个 Entry 类型的数组 table。 1transient Entry[] table; Entry 存储着键值对。它包含了四个字段，从 next 字段我们可以看出 Entry 是一个链表。即数组中的每个位置被当成一个桶，一个桶存放一个链表。HashMap 使用拉链法来解决冲突，同一个链表中存放哈希值和散列桶取模运算结果相同的 Entry。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final K key; V value; Entry&lt;K,V&gt; next; int hash; Entry(int h, K k, V v, Entry&lt;K,V&gt; n) { value = v; next = n; key = k; hash = h; } public final K getKey() { return key; } public final V getValue() { return value; } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } public final boolean equals(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; Object k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null &amp;&amp; k1.equals(k2))) { Object v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null &amp;&amp; v1.equals(v2))) return true; } return false; } public final int hashCode() { return Objects.hashCode(getKey()) ^ Objects.hashCode(getValue()); } public final String toString() { return getKey() + \"=\" + getValue(); }} 2. 拉链法的工作原理1234HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;();map.put(\"K1\", \"V1\");map.put(\"K2\", \"V2\");map.put(\"K3\", \"V3\"); 新建一个 HashMap，默认大小为 16； 插入 &lt;K1,V1&gt; 键值对，先计算 K1 的 hashCode 为 115，使用除留余数法得到所在的桶下标 115%16=3。 插入 &lt;K2,V2&gt; 键值对，先计算 K2 的 hashCode 为 118，使用除留余数法得到所在的桶下标 118%16=6。 插入 &lt;K3,V3&gt; 键值对，先计算 K3 的 hashCode 为 118，使用除留余数法得到所在的桶下标 118%16=6，插在 &lt;K2,V2&gt; 前面。 应该注意到链表的插入是以头插法方式进行的，例如上面的 &lt;K3,V3&gt; 不是插在 &lt;K2,V2&gt; 后面，而是插入在链表头部。 查找需要分成两步进行： 计算键值对所在的桶； 在链表上顺序查找，时间复杂度显然和链表的长度成正比。 3. put 操作1234567891011121314151617181920212223242526public V put(K key, V value) { if (table == EMPTY_TABLE) { inflateTable(threshold); } // 键为 null 单独处理 if (key == null) return putForNullKey(value); int hash = hash(key); // 确定桶下标 int i = indexFor(hash, table.length); // 先找出是否已经存在键为 key 的键值对，如果存在的话就更新这个键值对的值为 value for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; // 插入新键值对 addEntry(hash, key, value, i); return null;} HashMap 允许插入键为 null 的键值对。但是因为无法调用 null 的 hashCode() 方法，也就无法确定该键值对的桶下标，只能通过强制指定一个桶下标来存放。HashMap 使用第 0 个桶存放键为 null 的键值对。 12345678910111213private V putForNullKey(V value) { for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) { if (e.key == null) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(0, null, value, 0); return null;} 使用链表的头插法，也就是新的键值对插在链表的头部，而不是链表的尾部。 12345678910111213141516171819202122void addEntry(int hash, K key, V value, int bucketIndex) { if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) { resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); } createEntry(hash, key, value, bucketIndex);}void createEntry(int hash, K key, V value, int bucketIndex) { Entry&lt;K,V&gt; e = table[bucketIndex]; // 头插法，链表头部指向新的键值对 table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;}Entry(int h, K k, V v, Entry&lt;K,V&gt; n) { value = v; next = n; key = k; hash = h;} 4. 确定桶下标很多操作都需要先确定一个键值对所在的桶下标。 12int hash = hash(key);int i = indexFor(hash, table.length); 4.1 计算 hash 值 1234567891011121314151617final int hash(Object k) { int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) { return sun.misc.Hashing.stringHash32((String) k); } h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);}public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value);} 4.2 取模 令 x = 1&lt;&lt;4，即 x 为 2 的 4 次方，它具有以下性质： 12x : 00010000x-1 : 00001111 令一个数 y 与 x-1 做与运算，可以去除 y 位级表示的第 4 位以上数： 123y : 10110010x-1 : 00001111y&amp;(x-1) : 00000010 这个性质和 y 对 x 取模效果是一样的： 123y : 10110010x : 00010000y%x : 00000010 我们知道，位运算的代价比求模运算小的多，因此在进行这种计算时用位运算的话能带来更高的性能。 确定桶下标的最后一步是将 key 的 hash 值对桶个数取模：hash%capacity，如果能保证 capacity 为 2 的 n 次方，那么就可以将这个操作转换为位运算。 123static int indexFor(int h, int length) { return h &amp; (length-1);} 5. 扩容-基本原理设 HashMap 的 table 长度为 M，需要存储的键值对数量为 N，如果哈希函数满足均匀性的要求，那么每条链表的长度大约为 N/M，因此平均查找次数的复杂度为 O(N/M)。 为了让查找的成本降低，应该尽可能使得 N/M 尽可能小，因此需要保证 M 尽可能大，也就是说 table 要尽可能大。HashMap 采用动态扩容来根据当前的 N 值来调整 M 值，使得空间效率和时间效率都能得到保证。 和扩容相关的参数主要有：capacity、size、threshold 和 load_factor。 参数 含义 capacity table 的容量大小，默认为 16。需要注意的是 capacity 必须保证为 2 的 n 次方。 size 键值对数量。 threshold size 的临界值，当 size 大于等于 threshold 就必须进行扩容操作。 loadFactor 装载因子，table 能够使用的比例，threshold = capacity * loadFactor。 123456789101112131415static final int DEFAULT_INITIAL_CAPACITY = 16;static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;static final float DEFAULT_LOAD_FACTOR = 0.75f;transient Entry[] table;transient int size;int threshold;final float loadFactor;transient int modCount; 从下面的添加元素代码中可以看出，当需要扩容时，令 capacity 为原来的两倍。 123456void addEntry(int hash, K key, V value, int bucketIndex) { Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); if (size++ &gt;= threshold) resize(2 * table.length);} 扩容使用 resize() 实现，需要注意的是，扩容操作同样需要把 oldTable 的所有键值对重新插入 newTable 中，因此这一步是很费时的。 123456789101112131415161718192021222324252627282930void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor);}void transfer(Entry[] newTable) { Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) { Entry&lt;K,V&gt; e = src[j]; if (e != null) { src[j] = null; do { Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } while (e != null); } }} 6. 扩容-重新计算桶下标在进行扩容时，需要把键值对重新放到对应的桶上。HashMap 使用了一个特殊的机制，可以降低重新计算桶下标的操作。 假设原数组长度 capacity 为 16，扩容之后 new capacity 为 32： 12capacity : 00010000new capacity : 00100000 对于一个 Key， 它的哈希值如果在第 5 位上为 0，那么取模得到的结果和之前一样； 如果为 1，那么得到的结果为原来的结果 +16。 7. 计算数组容量HashMap 构造函数允许用户传入的容量不是 2 的 n 次方，因为它可以自动地将传入的容量转换为 2 的 n 次方。 先考虑如何求一个数的掩码，对于 10010000，它的掩码为 11111111，可以使用以下方法得到： 123mask |= mask &gt;&gt; 1 11011000mask |= mask &gt;&gt; 2 11111110mask |= mask &gt;&gt; 4 11111111 mask+1 是大于原始数字的最小的 2 的 n 次方。 12num 10010000mask+1 100000000 以下是 HashMap 中计算数组容量的代码： 123456789static final int tableSizeFor(int cap) { int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;} 8. 链表转红黑树从 JDK 1.8 开始，一个桶存储的链表长度大于 8 时会将链表转换为红黑树。 应该是：从 JDK 1.8 开始， table的长度也就是HashMap的capacity(不是size)不能小于64而且在桶存储的链表长度为8时(准确的说是长度为7并且在继续塞第8个时),转换成红黑树,而不是超过8。 9. 与 HashTable 的比较 HashTable 使用 synchronized 来进行同步。 HashMap 可以插入键为 null 的 Entry。 HashMap 的迭代器是 fail-fast 迭代器。 HashMap 不能保证随着时间的推移 Map 中的元素次序是不变的。 ConcurrentHashMap1. 存储结构123456static final class HashEntry&lt;K,V&gt; { final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next;} ConcurrentHashMap 和 HashMap 实现上类似，最主要的差别是 ConcurrentHashMap 采用了分段锁（Segment），每个分段锁维护着几个桶（HashEntry），多个线程可以同时访问不同分段锁上的桶，从而使其并发度更高（并发度就是 Segment 的个数）。 Segment 继承自 ReentrantLock。 123456789101112131415161718static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable { private static final long serialVersionUID = 2249069246763182397L; static final int MAX_SCAN_RETRIES = Runtime.getRuntime().availableProcessors() &gt; 1 ? 64 : 1; transient volatile HashEntry&lt;K,V&gt;[] table; transient int count; transient int modCount; transient int threshold; final float loadFactor;}final Segment&lt;K,V&gt;[] segments; 默认的并发级别为 16，也就是说默认创建 16 个 Segment。 1static final int DEFAULT_CONCURRENCY_LEVEL = 16; 2. size 操作每个 Segment 维护了一个 count 变量来统计该 Segment 中的键值对个数。 12345/** * The number of elements. Accessed only either within locks * or among other volatile reads that maintain visibility. */transient int count; 在执行 size 操作时，需要遍历所有 Segment 然后把 count 累计起来。 ConcurrentHashMap 在执行 size 操作时先尝试不加锁，如果连续两次不加锁操作得到的结果一致，那么可以认为这个结果是正确的。 尝试次数使用 RETRIES_BEFORE_LOCK 定义，该值为 2，retries 初始值为 -1，因此尝试次数为 3。 如果尝试的次数超过 3 次，就需要对每个 Segment 加锁。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Number of unsynchronized retries in size and containsValue * methods before resorting to locking. This is used to avoid * unbounded retries if tables undergo continuous modification * which would make it impossible to obtain an accurate result. */static final int RETRIES_BEFORE_LOCK = 2;public int size() { // Try a few times to get accurate count. On failure due to // continuous async changes in table, resort to locking. final Segment&lt;K,V&gt;[] segments = this.segments; int size; boolean overflow; // true if size overflows 32 bits long sum; // sum of modCounts long last = 0L; // previous sum int retries = -1; // first iteration isn't retry try { for (;;) { // 超过尝试次数，则对每个 Segment 加锁 if (retries++ == RETRIES_BEFORE_LOCK) { for (int j = 0; j &lt; segments.length; ++j) ensureSegment(j).lock(); // force creation } sum = 0L; size = 0; overflow = false; for (int j = 0; j &lt; segments.length; ++j) { Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null) { sum += seg.modCount; int c = seg.count; if (c &lt; 0 || (size += c) &lt; 0) overflow = true; } } // 连续两次得到的结果一致，则认为这个结果是正确的 if (sum == last) break; last = sum; } } finally { if (retries &gt; RETRIES_BEFORE_LOCK) { for (int j = 0; j &lt; segments.length; ++j) segmentAt(segments, j).unlock(); } } return overflow ? Integer.MAX_VALUE : size;} 3. JDK 1.8 的改动JDK 1.7 使用分段锁机制来实现并发更新操作，核心类为 Segment，它继承自重入锁 ReentrantLock，并发度与 Segment 数量相等。 JDK 1.8 使用了 CAS 操作来支持更高的并发度，在 CAS 操作失败时使用内置锁 synchronized。 并且 JDK 1.8 的实现也在链表过长时会转换为红黑树。 LinkedHashMap存储结构继承自 HashMap，因此具有和 HashMap 一样的快速查找特性。 1public class LinkedHashMap&lt;K,V&gt; extends HashMap&lt;K,V&gt; implements Map&lt;K,V&gt; 内部维护了一个双向链表，用来维护插入顺序或者 LRU 顺序。 123456789/** * The head (eldest) of the doubly linked list. */transient LinkedHashMap.Entry&lt;K,V&gt; head;/** * The tail (youngest) of the doubly linked list. */transient LinkedHashMap.Entry&lt;K,V&gt; tail; accessOrder 决定了顺序，默认为 false，此时维护的是插入顺序。 1final boolean accessOrder; LinkedHashMap 最重要的是以下用于维护顺序的函数，它们会在 put、get 等方法中调用。 12void afterNodeAccess(Node&lt;K,V&gt; p) { }void afterNodeInsertion(boolean evict) { } afterNodeAccess()当一个节点被访问时，如果 accessOrder 为 true，则会将该节点移到链表尾部。也就是说指定为 LRU 顺序之后，在每次访问一个节点时，会将这个节点移到链表尾部，保证链表尾部是最近访问的节点，那么链表首部就是最近最久未使用的节点。 123456789101112131415161718192021222324void afterNodeAccess(Node&lt;K,V&gt; e) { // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; if (accessOrder &amp;&amp; (last = tail) != e) { LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else { p.before = last; last.after = p; } tail = p; ++modCount; }} afterNodeInsertion()在 put 等操作之后执行，当 removeEldestEntry() 方法返回 true 时会移除最晚的节点，也就是链表首部节点 first。 evict 只有在构建 Map 的时候才为 false，在这里为 true。 1234567void afterNodeInsertion(boolean evict) { // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) { K key = first.key; removeNode(hash(key), key, null, false, true); }} removeEldestEntry() 默认为 false，如果需要让它为 true，需要继承 LinkedHashMap 并且覆盖这个方法的实现，这在实现 LRU 的缓存中特别有用，通过移除最近最久未使用的节点，从而保证缓存空间足够，并且缓存的数据都是热点数据。 123protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) { return false;} LRU 缓存以下是使用 LinkedHashMap 实现的一个 LRU 缓存： 设定最大缓存空间 MAX_ENTRIES 为 3； 使用 LinkedHashMap 的构造函数将 accessOrder 设置为 true，开启 LRU 顺序； 覆盖 removeEldestEntry() 方法实现，在节点多于 MAX_ENTRIES 就会将最近最久未使用的数据移除。 123456789101112131415161718192021class LRUCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; { private static final int MAX_ENTRIES = 3; protected boolean removeEldestEntry(Map.Entry eldest) { return size() &gt; MAX_ENTRIES; } LRUCache() { super(MAX_ENTRIES, 0.75f, true); }}public static void main(String[] args) { LRUCache&lt;Integer, String&gt; cache = new LRUCache&lt;&gt;(); cache.put(1, \"a\"); cache.put(2, \"b\"); cache.put(3, \"c\"); cache.get(1); cache.put(4, \"d\"); System.out.println(cache.keySet());}[3, 1, 4] WeakHashMap存储结构WeakHashMap 的 Entry 继承自 WeakReference，被 WeakReference 关联的对象在下一次垃圾回收时会被回收。 WeakHashMap 主要用来实现缓存，通过使用 WeakHashMap 来引用缓存对象，由 JVM 对这部分缓存进行回收。 1private static class Entry&lt;K,V&gt; extends WeakReference&lt;Object&gt; implements Map.Entry&lt;K,V&gt; ConcurrentCacheTomcat 中的 ConcurrentCache 使用了 WeakHashMap 来实现缓存功能。 ConcurrentCache 采取的是分代缓存： 经常使用的对象放入 eden 中，eden 使用 ConcurrentHashMap 实现，不用担心会被回收（伊甸园）； 不常用的对象放入 longterm，longterm 使用 WeakHashMap 实现，这些老对象会被垃圾收集器回收。 当调用 get() 方法时，会先从 eden 区获取，如果没有找到的话再到 longterm 获取，当从 longterm 获取到就把对象放入 eden 中，从而保证经常被访问的节点不容易被回收。 当调用 put() 方法时，如果 eden 的大小超过了 size，那么就将 eden 中的所有对象都放入 longterm 中，利用虚拟机回收掉一部分不经常使用的对象。 1234567891011121314151617181920212223242526272829303132public final class ConcurrentCache&lt;K, V&gt; { private final int size; private final Map&lt;K, V&gt; eden; private final Map&lt;K, V&gt; longterm; public ConcurrentCache(int size) { this.size = size; this.eden = new ConcurrentHashMap&lt;&gt;(size); this.longterm = new WeakHashMap&lt;&gt;(size); } public V get(K k) { V v = this.eden.get(k); if (v == null) { v = this.longterm.get(k); if (v != null) this.eden.put(k, v); } return v; } public void put(K k, V v) { if (this.eden.size() &gt;= size) { this.longterm.putAll(this.eden); this.eden.clear(); } this.eden.put(k, v); }} 文章来源) .","link":"/2019/08/16/Java容器集合.html"},{"title":"中华人民共和国网络安全法","text":"《中华人民共和国网络安全法》是为保障网络安全，维护网络空间主权和国家安全、社会公共利益，保护公民、法人和其他组织的合法权益，促进经济社会信息化健康发展制定。由全国人民代表大会常务委员会于2016年11月7日发布，自2017年6月1日起施行。 中华人民共和国网络安全法中文名：中华人民共和国网络安全法 外文名：People’s Republic of China Network Security Law 发布机构：全国人民代表大会常务委员会 发布日期：2016年11月7日 实施日期：2017年6月1日 ​ 第一章 总 则 ​ 第二章 网络安全支持与促进 ​ 第三章 网络运行安全 ​ 第一节 一般规定 ​ 第二节 关键信息基础设施的运行安全 ​ 第四章 网络信息安全 ​ 第五章 监测预警与应急处置 ​ 第六章 法律责任 ​ 第七章 附 则 第一章 总 则第一条 为了保障网络安全，维护网络空间主权和国家安全、社会公共利益，保护公民、法人和其他组织的合法权益，促进经济社会信息化健康发展，制定本法。 第二条 在中华人民共和国境内建设、运营、维护和使用网络，以及网络安全的监督管理，适用本法。 第三条 国家坚持网络安全与信息化发展并重，遵循积极利用、科学发展、依法管理、确保安全的方针，推进网络基础设施建设和互联互通，鼓励网络技术创新和应用，支持培养网络安全人才，建立健全网络安全保障体系，提高网络安全保护能力。 第四条 国家制定并不断完善网络安全战略，明确保障网络安全的基本要求和主要目标，提出重点领域的网络安全政策、工作任务和措施。 第五条 国家采取措施，监测、防御、处置来源于中华人民共和国境内外的网络安全风险和威胁，保护关键信息基础设施免受攻击、侵入、干扰和破坏，依法惩治网络违法犯罪活动，维护网络空间安全和秩序。 第六条 国家倡导诚实守信、健康文明的网络行为，推动传播社会主义核心价值观，采取措施提高全社会的网络安全意识和水平，形成全社会共同参与促进网络安全的良好环境。 第七条 国家积极开展网络空间治理、网络技术研发和标准制定、打击网络违法犯罪等方面的国际交流与合作，推动构建和平、安全、开放、合作的网络空间，建立多边、民主、透明的网络治理体系。 第八条 国家网信部门负责统筹协调网络安全工作和相关监督管理工作。国务院电信主管部门、公安部门和其他有关机关依照本法和有关法律、行政法规的规定，在各自职责范围内负责网络安全保护和监督管理工作。 县级以上地方人民政府有关部门的网络安全保护和监督管理职责，按照国家有关规定确定。 第九条 网络运营者开展经营和服务活动，必须遵守法律、行政法规，尊重社会公德，遵守商业道德，诚实信用，履行网络安全保护义务，接受政府和社会的监督，承担社会责任。 第十条 建设、运营网络或者通过网络提供服务，应当依照法律、行政法规的规定和国家标准的强制性要求，采取技术措施和其他必要措施，保障网络安全、稳定运行，有效应对网络安全事件，防范网络违法犯罪活动，维护网络数据的完整性、保密性和可用性。 第十一条 网络相关行业组织按照章程，加强行业自律，制定网络安全行为规范，指导会员加强网络安全保护，提高网络安全保护水平，促进行业健康发展。 第十二条 国家保护公民、法人和其他组织依法使用网络的权利，促进网络接入普及，提升网络服务水平，为社会提供安全、便利的网络服务，保障网络信息依法有序自由流动。 任何个人和组织使用网络应当遵守宪法法律，遵守公共秩序，尊重社会公德，不得危害网络安全，不得利用网络从事危害国家安全、荣誉和利益，煽动颠覆国家政权、推翻社会主义制度，煽动分裂国家、破坏国家统一，宣扬恐怖主义、极端主义，宣扬民族仇恨、民族歧视，传播暴力、淫秽色情信息，编造、传播虚假信息扰乱经济秩序和社会秩序，以及侵害他人名誉、隐私、知识产权和其他合法权益等活动。 第十三条 国家支持研究开发有利于未成年人健康成长的网络产品和服务，依法惩治利用网络从事危害未成年人身心健康的活动，为未成年人提供安全、健康的网络环境。 第十四条 任何个人和组织有权对危害网络安全的行为向网信、电信、公安等部门举报。收到举报的部门应当及时依法作出处理；不属于本部门职责的，应当及时移送有权处理的部门。 有关部门应当对举报人的相关信息予以保密，保护举报人的合法权益。 第二章 网络安全支持与促进第十五条 国家建立和完善网络安全标准体系。国务院标准化行政主管部门和国务院其他有关部门根据各自的职责，组织制定并适时修订有关网络安全管理以及网络产品、服务和运行安全的国家标准、行业标准。 国家支持企业、研究机构、高等学校、网络相关行业组织参与网络安全国家标准、行业标准的制定。 第十六条 国务院和省、自治区、直辖市人民政府应当统筹规划，加大投入，扶持重点网络安全技术产业和项目，支持网络安全技术的研究开发和应用，推广安全可信的网络产品和服务，保护网络技术知识产权，支持企业、研究机构和高等学校等参与国家网络安全技术创新项目。 第十七条 国家推进网络安全社会化服务体系建设，鼓励有关企业、机构开展网络安全认证、检测和风险评估等安全服务。 第十八条 国家鼓励开发网络数据安全保护和利用技术，促进公共数据资源开放，推动技术创新和经济社会发展。 国家支持创新网络安全管理方式，运用网络新技术，提升网络安全保护水平。 第十九条 各级人民政府及其有关部门应当组织开展经常性的网络安全宣传教育，并指导、督促有关单位做好网络安全宣传教育工作。 大众传播媒介应当有针对性地面向社会进行网络安全宣传教育。 第二十条 国家支持企业和高等学校、职业学校等教育培训机构开展网络安全相关教育与培训，采取多种方式培养网络安全人才，促进网络安全人才交流。 第三章 网络运行安全 第一节 一般规定第二十一条 国家实行网络安全等级保护制度。网络运营者应当按照网络安全等级保护制度的要求，履行下列安全保护义务，保障网络免受干扰、破坏或者未经授权的访问，防止网络数据泄露或者被窃取、篡改： （一）制定内部安全管理制度和操作规程，确定网络安全负责人，落实网络安全保护责任； （二）采取防范计算机病毒和网络攻击、网络侵入等危害网络安全行为的技术措施； （三）采取监测、记录网络运行状态、网络安全事件的技术措施，并按照规定留存相关的网络日志不少于六个月； （四）采取数据分类、重要数据备份和加密等措施； （五）法律、行政法规规定的其他义务。 第二十二条 网络产品、服务应当符合相关国家标准的强制性要求。网络产品、服务的提供者不得设置恶意程序；发现其网络产品、服务存在安全缺陷、漏洞等风险时，应当立即采取补救措施，按照规定及时告知用户并向有关主管部门报告。 网络产品、服务的提供者应当为其产品、服务持续提供安全维护；在规定或者当事人约定的期限内，不得终止提供安全维护。 网络产品、服务具有收集用户信息功能的，其提供者应当向用户明示并取得同意；涉及用户个人信息的，还应当遵守本法和有关法律、行政法规关于个人信息保护的规定。 第二十三条 网络关键设备和网络安全专用产品应当按照相关国家标准的强制性要求，由具备资格的机构安全认证合格或者安全检测符合要求后，方可销售或者提供。国家网信部门会同国务院有关部门制定、公布网络关键设备和网络安全专用产品目录，并推动安全认证和安全检测结果互认，避免重复认证、检测。 第二十四条 网络运营者为用户办理网络接入、域名注册服务，办理固定电话、移动电话等入网手续，或者为用户提供信息发布、即时通讯等服务，在与用户签订协议或者确认提供服务时，应当要求用户提供真实身份信息。用户不提供真实身份信息的，网络运营者不得为其提供相关服务。 国家实施网络可信身份战略，支持研究开发安全、方便的电子身份认证技术，推动不同电子身份认证之间的互认。 第二十五条 网络运营者应当制定网络安全事件应急预案，及时处置系统漏洞、计算机病毒、网络攻击、网络侵入等安全风险；在发生危害网络安全的事件时，立即启动应急预案，采取相应的补救措施，并按照规定向有关主管部门报告。 第二十六条 开展网络安全认证、检测、风险评估等活动，向社会发布系统漏洞、计算机病毒、网络攻击、网络侵入等网络安全信息，应当遵守国家有关规定。 第二十七条 任何个人和组织不得从事非法侵入他人网络、干扰他人网络正常功能、窃取网络数据等危害网络安全的活动；不得提供专门用于从事侵入网络、干扰网络正常功能及防护措施、窃取网络数据等危害网络安全活动的程序、工具；明知他人从事危害网络安全的活动的，不得为其提供技术支持、广告推广、支付结算等帮助。 第二十八条 网络运营者应当为公安机关、国家安全机关依法维护国家安全和侦查犯罪的活动提供技术支持和协助。 第二十九条 国家支持网络运营者之间在网络安全信息收集、分析、通报和应急处置等方面进行合作，提高网络运营者的安全保障能力。 有关行业组织建立健全本行业的网络安全保护规范和协作机制，加强对网络安全风险的分析评估，定期向会员进行风险警示，支持、协助会员应对网络安全风险。 第三十条 网信部门和有关部门在履行网络安全保护职责中获取的信息，只能用于维护网络安全的需要，不得用于其他用途。 第二节 关键信息基础设施的运行安全第三十一条 国家对公共通信和信息服务、能源、交通、水利、金融、公共服务、电子政务等重要行业和领域，以及其他一旦遭到破坏、丧失功能或者数据泄露，可能严重危害国家安全、国计民生、公共利益的关键信息基础设施，在网络安全等级保护制度的基础上，实行重点保护。关键信息基础设施的具体范围和安全保护办法由国务院制定。 国家鼓励关键信息基础设施以外的网络运营者自愿参与关键信息基础设施保护体系。 第三十二条 按照国务院规定的职责分工，负责关键信息基础设施安全保护工作的部门分别编制并组织实施本行业、本领域的关键信息基础设施安全规划，指导和监督关键信息基础设施运行安全保护工作。 第三十三条 建设关键信息基础设施应当确保其具有支持业务稳定、持续运行的性能，并保证安全技术措施同步规划、同步建设、同步使用。 第三十四条 除本法第二十一条的规定外，关键信息基础设施的运营者还应当履行下列安全保护义务： （一）设置专门安全管理机构和安全管理负责人，并对该负责人和关键岗位的人员进行安全背景审查； （二）定期对从业人员进行网络安全教育、技术培训和技能考核； （三）对重要系统和数据库进行容灾备份； （四）制定网络安全事件应急预案，并定期进行演练； （五）法律、行政法规规定的其他义务。 第三十五条 关键信息基础设施的运营者采购网络产品和服务，可能影响国家安全的，应当通过国家网信部门会同国务院有关部门组织的国家安全审查。 第三十六条 关键信息基础设施的运营者采购网络产品和服务，应当按照规定与提供者签订安全保密协议，明确安全和保密义务与责任。 第三十七条 关键信息基础设施的运营者在中华人民共和国境内运营中收集和产生的个人信息和重要数据应当在境内存储。因业务需要，确需向境外提供的，应当按照国家网信部门会同国务院有关部门制定的办法进行安全评估；法律、行政法规另有规定的，依照其规定。 第三十八条 关键信息基础设施的运营者应当自行或者委托网络安全服务机构对其网络的安全性和可能存在的风险每年至少进行一次检测评估，并将检测评估情况和改进措施报送相关负责关键信息基础设施安全保护工作的部门。 第三十九条 国家网信部门应当统筹协调有关部门对关键信息基础设施的安全保护采取下列措施： （一）对关键信息基础设施的安全风险进行抽查检测，提出改进措施，必要时可以委托网络安全服务机构对网络存在的安全风险进行检测评估； （二）定期组织关键信息基础设施的运营者进行网络安全应急演练，提高应对网络安全事件的水平和协同配合能力； （三）促进有关部门、关键信息基础设施的运营者以及有关研究机构、网络安全服务机构等之间的网络安全信息共享； （四）对网络安全事件的应急处置与网络功能的恢复等，提供技术支持和协助。 第四章 网络信息安全第四十条 网络运营者应当对其收集的用户信息严格保密，并建立健全用户信息保护制度。 第四十一条 网络运营者收集、使用个人信息，应当遵循合法、正当、必要的原则，公开收集、使用规则，明示收集、使用信息的目的、方式和范围，并经被收集者同意。 网络运营者不得收集与其提供的服务无关的个人信息，不得违反法律、行政法规的规定和双方的约定收集、使用个人信息，并应当依照法律、行政法规的规定和与用户的约定，处理其保存的个人信息。 第四十二条 网络运营者不得泄露、篡改、毁损其收集的个人信息；未经被收集者同意，不得向他人提供个人信息。但是，经过处理无法识别特定个人且不能复原的除外。 网络运营者应当采取技术措施和其他必要措施，确保其收集的个人信息安全，防止信息泄露、毁损、丢失。在发生或者可能发生个人信息泄露、毁损、丢失的情况时，应当立即采取补救措施，按照规定及时告知用户并向有关主管部门报告。 第四十三条 个人发现网络运营者违反法律、行政法规的规定或者双方的约定收集、使用其个人信息的，有权要求网络运营者删除其个人信息；发现网络运营者收集、存储的其个人信息有错误的，有权要求网络运营者予以更正。网络运营者应当采取措施予以删除或者更正。 第四十四条 任何个人和组织不得窃取或者以其他非法方式获取个人信息，不得非法出售或者非法向他人提供个人信息。 第四十五条 依法负有网络安全监督管理职责的部门及其工作人员，必须对在履行职责中知悉的个人信息、隐私和商业秘密严格保密，不得泄露、出售或者非法向他人提供。 第四十六条 任何个人和组织应当对其使用网络的行为负责，不得设立用于实施诈骗，传授犯罪方法，制作或者销售违禁物品、管制物品等违法犯罪活动的网站、通讯群组，不得利用网络发布涉及实施诈骗，制作或者销售违禁物品、管制物品以及其他违法犯罪活动的信息。 第四十七条 网络运营者应当加强对其用户发布的信息的管理，发现法律、行政法规禁止发布或者传输的信息的，应当立即停止传输该信息，采取消除等处置措施，防止信息扩散，保存有关记录，并向有关主管部门报告。 第四十八条 任何个人和组织发送的电子信息、提供的应用软件，不得设置恶意程序，不得含有法律、行政法规禁止发布或者传输的信息。 电子信息发送服务提供者和应用软件下载服务提供者，应当履行安全管理义务，知道其用户有前款规定行为的，应当停止提供服务，采取消除等处置措施，保存有关记录，并向有关主管部门报告。 第四十九条 网络运营者应当建立网络信息安全投诉、举报制度，公布投诉、举报方式等信息，及时受理并处理有关网络信息安全的投诉和举报。 网络运营者对网信部门和有关部门依法实施的监督检查，应当予以配合。 第五十条 国家网信部门和有关部门依法履行网络信息安全监督管理职责，发现法律、行政法规禁止发布或者传输的信息的，应当要求网络运营者停止传输，采取消除等处置措施，保存有关记录；对来源于中华人民共和国境外的上述信息，应当通知有关机构采取技术措施和其他必要措施阻断传播。 第五章 监测预警与应急处置第五十一条 国家建立网络安全监测预警和信息通报制度。国家网信部门应当统筹协调有关部门加强网络安全信息收集、分析和通报工作，按照规定统一发布网络安全监测预警信息。 第五十二条 负责关键信息基础设施安全保护工作的部门，应当建立健全本行业、本领域的网络安全监测预警和信息通报制度，并按照规定报送网络安全监测预警信息。 第五十三条 国家网信部门协调有关部门建立健全网络安全风险评估和应急工作机制，制定网络安全事件应急预案，并定期组织演练。 负责关键信息基础设施安全保护工作的部门应当制定本行业、本领域的网络安全事件应急预案，并定期组织演练。 网络安全事件应急预案应当按照事件发生后的危害程度、影响范围等因素对网络安全事件进行分级，并规定相应的应急处置措施。 第五十四条 网络安全事件发生的风险增大时，省级以上人民政府有关部门应当按照规定的权限和程序，并根据网络安全风险的特点和可能造成的危害，采取下列措施： （一）要求有关部门、机构和人员及时收集、报告有关信息，加强对网络安全风险的监测； （二）组织有关部门、机构和专业人员，对网络安全风险信息进行分析评估，预测事件发生的可能性、影响范围和危害程度； （三）向社会发布网络安全风险预警，发布避免、减轻危害的措施。 第五十五条 发生网络安全事件，应当立即启动网络安全事件应急预案，对网络安全事件进行调查和评估，要求网络运营者采取技术措施和其他必要措施，消除安全隐患，防止危害扩大，并及时向社会发布与公众有关的警示信息。 第五十六条 省级以上人民政府有关部门在履行网络安全监督管理职责中，发现网络存在较大安全风险或者发生安全事件的，可以按照规定的权限和程序对该网络的运营者的法定代表人或者主要负责人进行约谈。网络运营者应当按照要求采取措施，进行整改，消除隐患。 第五十七条 因网络安全事件，发生突发事件或者生产安全事故的，应当依照《中华人民共和国突发事件应对法》、《中华人民共和国安全生产法》等有关法律、行政法规的规定处置。 第五十八条 因维护国家安全和社会公共秩序，处置重大突发社会安全事件的需要，经国务院决定或者批准，可以在特定区域对网络通信采取限制等临时措施。 第六章 法律责任第五十九条 网络运营者不履行本法第二十一条、第二十五条规定的网络安全保护义务的，由有关主管部门责令改正，给予警告；拒不改正或者导致危害网络安全等后果的，处一万元以上十万元以下罚款，对直接负责的主管人员处五千元以上五万元以下罚款。 关键信息基础设施的运营者不履行本法第三十三条、第三十四条、第三十六条、第三十八条规定的网络安全保护义务的，由有关主管部门责令改正，给予警告；拒不改正或者导致危害网络安全等后果的，处十万元以上一百万元以下罚款，对直接负责的主管人员处一万元以上十万元以下罚款。 第六十条 违反本法第二十二条第一款、第二款和第四十八条第一款规定，有下列行为之一的，由有关主管部门责令改正，给予警告；拒不改正或者导致危害网络安全等后果的，处五万元以上五十万元以下罚款，对直接负责的主管人员处一万元以上十万元以下罚款： （一）设置恶意程序的； （二）对其产品、服务存在的安全缺陷、漏洞等风险未立即采取补救措施，或者未按照规定及时告知用户并向有关主管部门报告的； （三）擅自终止为其产品、服务提供安全维护的。 第六十一条 网络运营者违反本法第二十四条第一款规定，未要求用户提供真实身份信息，或者对不提供真实身份信息的用户提供相关服务的，由有关主管部门责令改正；拒不改正或者情节严重的，处五万元以上五十万元以下罚款，并可以由有关主管部门责令暂停相关业务、停业整顿、关闭网站、吊销相关业务许可证或者吊销营业执照，对直接负责的主管人员和其他直接责任人员处一万元以上十万元以下罚款。 第六十二条 违反本法第二十六条规定，开展网络安全认证、检测、风险评估等活动，或者向社会发布系统漏洞、计算机病毒、网络攻击、网络侵入等网络安全信息的，由有关主管部门责令改正，给予警告；拒不改正或者情节严重的，处一万元以上十万元以下罚款，并可以由有关主管部门责令暂停相关业务、停业整顿、关闭网站、吊销相关业务许可证或者吊销营业执照，对直接负责的主管人员和其他直接责任人员处五千元以上五万元以下罚款。 第六十三条 违反本法第二十七条规定，从事危害网络安全的活动，或者提供专门用于从事危害网络安全活动的程序、工具，或者为他人从事危害网络安全的活动提供技术支持、广告推广、支付结算等帮助，尚不构成犯罪的，由公安机关没收违法所得，处五日以下拘留，可以并处五万元以上五十万元以下罚款；情节较重的，处五日以上十五日以下拘留，可以并处十万元以上一百万元以下罚款。 单位有前款行为的，由公安机关没收违法所得，处十万元以上一百万元以下罚款，并对直接负责的主管人员和其他直接责任人员依照前款规定处罚。 违反本法第二十七条规定，受到治安管理处罚的人员，五年内不得从事网络安全管理和网络运营关键岗位的工作；受到刑事处罚的人员，终身不得从事网络安全管理和网络运营关键岗位的工作。 第六十四条 网络运营者、网络产品或者服务的提供者违反本法第二十二条第三款、第四十一条至第四十三条规定，侵害个人信息依法得到保护的权利的，由有关主管部门责令改正，可以根据情节单处或者并处警告、没收违法所得、处违法所得一倍以上十倍以下罚款，没有违法所得的，处一百万元以下罚款，对直接负责的主管人员和其他直接责任人员处一万元以上十万元以下罚款；情节严重的，并可以责令暂停相关业务、停业整顿、关闭网站、吊销相关业务许可证或者吊销营业执照。 违反本法第四十四条规定，窃取或者以其他非法方式获取、非法出售或者非法向他人提供个人信息，尚不构成犯罪的，由公安机关没收违法所得，并处违法所得一倍以上十倍以下罚款，没有违法所得的，处一百万元以下罚款。 第六十五条 关键信息基础设施的运营者违反本法第三十五条规定，使用未经安全审查或者安全审查未通过的网络产品或者服务的，由有关主管部门责令停止使用，处采购金额一倍以上十倍以下罚款；对直接负责的主管人员和其他直接责任人员处一万元以上十万元以下罚款。 第六十六条 关键信息基础设施的运营者违反本法第三十七条规定，在境外存储网络数据，或者向境外提供网络数据的，由有关主管部门责令改正，给予警告，没收违法所得，处五万元以上五十万元以下罚款，并可以责令暂停相关业务、停业整顿、关闭网站、吊销相关业务许可证或者吊销营业执照；对直接负责的主管人员和其他直接责任人员处一万元以上十万元以下罚款。 第六十七条 违反本法第四十六条规定，设立用于实施违法犯罪活动的网站、通讯群组，或者利用网络发布涉及实施违法犯罪活动的信息，尚不构成犯罪的，由公安机关处五日以下拘留，可以并处一万元以上十万元以下罚款；情节较重的，处五日以上十五日以下拘留，可以并处五万元以上五十万元以下罚款。关闭用于实施违法犯罪活动的网站、通讯群组。 单位有前款行为的，由公安机关处十万元以上五十万元以下罚款，并对直接负责的主管人员和其他直接责任人员依照前款规定处罚。 第六十八条 网络运营者违反本法第四十七条规定，对法律、行政法规禁止发布或者传输的信息未停止传输、采取消除等处置措施、保存有关记录的，由有关主管部门责令改正，给予警告，没收违法所得；拒不改正或者情节严重的，处十万元以上五十万元以下罚款，并可以责令暂停相关业务、停业整顿、关闭网站、吊销相关业务许可证或者吊销营业执照，对直接负责的主管人员和其他直接责任人员处一万元以上十万元以下罚款。 电子信息发送服务提供者、应用软件下载服务提供者，不履行本法第四十八条第二款规定的安全管理义务的，依照前款规定处罚。 第六十九条 网络运营者违反本法规定，有下列行为之一的，由有关主管部门责令改正；拒不改正或者情节严重的，处五万元以上五十万元以下罚款，对直接负责的主管人员和其他直接责任人员，处一万元以上十万元以下罚款： （一）不按照有关部门的要求对法律、行政法规禁止发布或者传输的信息，采取停止传输、消除等处置措施的； （二）拒绝、阻碍有关部门依法实施的监督检查的； （三）拒不向公安机关、国家安全机关提供技术支持和协助的。 第七十条 发布或者传输本法第十二条第二款和其他法律、行政法规禁止发布或者传输的信息的，依照有关法律、行政法规的规定处罚。 第七十一条 有本法规定的违法行为的，依照有关法律、行政法规的规定记入信用档案，并予以公示。 第七十二条 国家机关政务网络的运营者不履行本法规定的网络安全保护义务的，由其上级机关或者有关机关责令改正；对直接负责的主管人员和其他直接责任人员依法给予处分。 第七十三条 网信部门和有关部门违反本法第三十条规定，将在履行网络安全保护职责中获取的信息用于其他用途的，对直接负责的主管人员和其他直接责任人员依法给予处分。 网信部门和有关部门的工作人员玩忽职守、滥用职权、徇私舞弊，尚不构成犯罪的，依法给予处分。 第七十四条 违反本法规定，给他人造成损害的，依法承担民事责任。 违反本法规定，构成违反治安管理行为的，依法给予治安管理处罚；构成犯罪的，依法追究刑事责任。 第七十五条 境外的机构、组织、个人从事攻击、侵入、干扰、破坏等危害中华人民共和国的关键信息基础设施的活动，造成严重后果的，依法追究法律责任；国务院公安部门和有关部门并可以决定对该机构、组织、个人采取冻结财产或者其他必要的制裁措施。 第七章 附 则第七十六条 本法下列用语的含义： （一）网络，是指由计算机或者其他信息终端及相关设备组成的按照一定的规则和程序对信息进行收集、存储、传输、交换、处理的系统。 （二）网络安全，是指通过采取必要措施，防范对网络的攻击、侵入、干扰、破坏和非法使用以及意外事故，使网络处于稳定可靠运行的状态，以及保障网络数据的完整性、保密性、可用性的能力。 （三）网络运营者，是指网络的所有者、管理者和网络服务提供者。 （四）网络数据，是指通过网络收集、存储、传输、处理和产生的各种电子数据。 （五）个人信息，是指以电子或者其他方式记录的能够单独或者与其他信息结合识别自然人个人身份的各种信息，包括但不限于自然人的姓名、出生日期、身份证件号码、个人生物识别信息、住址、电话号码等。 第七十七条 存储、处理涉及国家秘密信息的网络的运行安全保护，除应当遵守本法外，还应当遵守保密法律、行政法规的规定。 第七十八条 军事网络的安全保护，由中央军事委员会另行规定。 第七十九条 本法自2017年6月1日起施行。","link":"/2019/08/08/中华人民共和国网络安全法.html"},{"title":"Java并发相关知识点","text":"互斥同步最主要的问题就是线程阻塞和唤醒所带来的性能问题，因此这种同步也称为阻塞同步。 互斥同步属于一种悲观的并发策略，总是认为只要不去做正确的同步措施，那就肯定会出现问题。无论共享数据是否真的会出现竞争，它都要进行加锁（这里讨论的是概念模型，实际上虚拟机会优化掉很大一部分不必要的加锁）、用户态核心态转换、维护锁计数器和检查是否有被阻塞的线程需要唤醒等操作。 并发零碎知识点 线程状态转换 新建（New） 创建后尚未启动。 可运行（Runnable） 可能正在运行，也可能正在等待 CPU 时间片。 包含了操作系统线程状态中的 Running 和 Ready。 阻塞（Blocked） 等待获取一个排它锁，如果其线程释放了锁就会结束此状态。 无限期等待（Waiting） 等待其它线程显式地唤醒，否则不会被分配 CPU 时间片。 进入方法 退出方法 没有设置 Timeout 参数的 Object.wait() 方法 Object.notify() / Object.notifyAll() 没有设置 Timeout 参数的 Thread.join() 方法 被调用的线程执行完毕 LockSupport.park() 方法 LockSupport.unpark(Thread) 限期等待（Timed Waiting） 无需等待其它线程显式地唤醒，在一定时间之后会被系统自动唤醒。 调用 Thread.sleep() 方法使线程进入限期等待状态时，常常用“使一个线程睡眠”进行描述。 调用 Object.wait() 方法使线程进入限期等待或者无限期等待时，常常用“挂起一个线程”进行描述。 睡眠和挂起是用来描述行为，而阻塞和等待用来描述状态。 阻塞和等待的区别在于，阻塞是被动的，它是在等待获取一个排它锁。而等待是主动的，通过调用 Thread.sleep() 和 Object.wait() 等方法进入。 进入方法 退出方法 Thread.sleep() 方法 时间结束 设置了 Timeout 参数的 Object.wait() 方法 时间结束 / Object.notify() / Object.notifyAll() 设置了 Timeout 参数的 Thread.join() 方法 时间结束 / 被调用的线程执行完毕 LockSupport.parkNanos() 方法 LockSupport.unpark(Thread) LockSupport.parkUntil() 方法 LockSupport.unpark(Thread) 死亡（Terminated） 可以是线程结束任务之后自己结束，或者产生了异常而结束。 使用线程有三种使用线程的方法： 实现 Runnable 接口； 实现 Callable 接口； 继承 Thread 类。 实现 Runnable 和 Callable 接口的类只能当做一个可以在线程中运行的任务，不是真正意义上的线程，因此最后还需要通过 Thread 来调用。可以说任务是通过线程驱动从而执行的。 实现 Runnable 接口需要实现 run() 方法。 通过 Thread 调用 start() 方法来启动线程。 1234567891011public class MyRunnable implements Runnable { public void run() { // ... }}public static void main(String[] args) { MyRunnable instance = new MyRunnable(); Thread thread = new Thread(instance); thread.start(); //调用 thread.run()并不是开启新线程，而是在当前线程里马上调用run()方法} 实现 Callable 接口与 Runnable 相比，Callable 可以有返回值，返回值通过 FutureTask 进行封装。 123456789101112public class MyCallable implements Callable&lt;Integer&gt; { public Integer call() { return 123; }}public static void main(String[] args) throws ExecutionException, InterruptedException { MyCallable mc = new MyCallable(); FutureTask&lt;Integer&gt; ft = new FutureTask&lt;&gt;(mc); Thread thread = new Thread(ft); thread.start(); System.out.println(ft.get());} 继承 Thread 类同样也是需要实现 run() 方法，因为 Thread 类也实现了 Runable 接口。 当调用 start() 方法启动一个线程时，虚拟机会将该线程放入就绪队列中等待被调度，当一个线程被调度时会执行该线程的 run() 方法。 123456789public class MyThread extends Thread { public void run() { // ... }}public static void main(String[] args) { MyThread mt = new MyThread(); mt.start();} 实现接口 VS 继承 Thread实现接口会更好一些，因为： Java 不支持多重继承，因此继承了 Thread 类就无法继承其它类，但是可以实现多个接口； 类可能只要求可执行就行，继承整个 Thread 类开销过大。 基础线程机制ExecutorExecutor 管理多个异步任务的执行，而无需程序员显式地管理线程的生命周期。这里的异步是指多个任务的执行互不干扰，不需要进行同步操作。 主要有三种 Executor： CachedThreadPool：一个任务创建一个线程； FixedThreadPool：所有任务只能使用固定大小的线程； SingleThreadExecutor：相当于大小为 1 的 FixedThreadPool。 1234567public static void main(String[] args) { ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 5; i++) { executorService.execute(new MyRunnable()); } executorService.shutdown();} Daemon守护线程是程序运行时在后台提供服务的线程，不属于程序中不可或缺的部分。 当所有非守护线程结束时，程序也就终止，同时会杀死所有守护线程。 main() 属于非守护线程。 在线程启动之前使用 setDaemon() 方法可以将一个线程设置为守护线程。 1234public static void main(String[] args) { Thread thread = new Thread(new MyRunnable()); thread.setDaemon(true);} sleep()Thread.sleep(millisec) 方法会休眠当前正在执行的线程，millisec 单位为毫秒。 sleep() 可能会抛出 InterruptedException，因为异常不能跨线程传播回 main() 中，因此必须在本地进行处理。线程中抛出的其它异常也同样需要在本地进行处理。 1234567public void run() { try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); }} yield()对静态方法 Thread.yield() 的调用声明了当前线程已经完成了生命周期中最重要的部分，可以切换给其它线程来执行。该方法只是对线程调度器的一个建议，而且也只是建议具有相同优先级的其它线程可以运行。 123public void run() { Thread.yield();} 中断一个线程执行完毕之后会自动结束，如果在运行过程中发生异常也会提前结束。 InterruptedException通过调用一个线程的 interrupt() 来中断该线程，如果该线程处于阻塞、限期等待或者无限期等待状态，那么就会抛出 InterruptedException，从而提前结束该线程。但是不能中断 I/O 阻塞和 synchronized 锁阻塞。 对于以下代码，在 main() 中启动一个线程之后再中断它，由于线程中调用了 Thread.sleep() 方法，因此会抛出一个 InterruptedException，从而提前结束线程，不执行之后的语句。 1234567891011121314151617181920212223242526public class InterruptExample { private static class MyThread1 extends Thread { @Override public void run() { try { Thread.sleep(2000); System.out.println(\"Thread run\"); } catch (InterruptedException e) { e.printStackTrace(); } } }}public static void main(String[] args) throws InterruptedException { Thread thread1 = new MyThread1(); thread1.start(); thread1.interrupt(); System.out.println(\"Main run\");}Main runjava.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at InterruptExample.lambda$main$0(InterruptExample.java:5) at InterruptExample$$Lambda$1/713338599.run(Unknown Source) at java.lang.Thread.run(Thread.java:745) interrupted()如果一个线程的 run() 方法执行一个无限循环，并且没有执行 sleep() 等会抛出 InterruptedException 的操作，那么调用线程的 interrupt() 方法就无法使线程提前结束。 但是调用 interrupt() 方法会设置线程的中断标记，此时调用 interrupted() 方法会返回 true。因此可以在循环体中使用 interrupted() 方法来判断线程是否处于中断状态，从而提前结束线程。 123456789101112131415161718public class InterruptExample { private static class MyThread2 extends Thread { @Override public void run() { while (!interrupted()) { // .. } System.out.println(\"Thread end\"); } }}public static void main(String[] args) throws InterruptedException { Thread thread2 = new MyThread2(); thread2.start(); thread2.interrupt();}Thread end Executor 的中断操作调用 Executor 的 shutdown() 方法会等待线程都执行完毕之后再关闭，但是如果调用的是 shutdownNow() 方法，则相当于调用每个线程的 interrupt() 方法。 以下使用 Lambda 创建线程，相当于创建了一个匿名内部线程。 123456789101112131415161718192021public static void main(String[] args) { ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; { try { Thread.sleep(2000); System.out.println(\"Thread run\"); } catch (InterruptedException e) { e.printStackTrace(); } }); executorService.shutdownNow(); System.out.println(\"Main run\");}Main runjava.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at ExecutorInterruptExample.lambda$main$0(ExecutorInterruptExample.java:9) at ExecutorInterruptExample$$Lambda$1/1160460865.run(Unknown Source) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 如果只想中断 Executor 中的一个线程，可以通过使用 submit() 方法来提交一个线程，它会返回一个 Future&lt;?&gt; 对象，通过调用该对象的 cancel(true) 方法就可以中断线程。 1234Future&lt;?&gt; future = executorService.submit(() -&gt; { // ..});future.cancel(true); 互斥同步Java 提供了两种锁机制来控制多个线程对共享资源的互斥访问，第一个是 JVM 实现的 synchronized，而另一个是 JDK 实现的 ReentrantLock。 synchronized1. 同步一个代码块 12345public void func() { synchronized (this) { // ... }} 它只作用于同一个对象，如果调用两个对象上的同步代码块，就不会进行同步。 对于以下代码，使用 ExecutorService 执行了两个线程，由于调用的是同一个对象的同步代码块，因此这两个线程会进行同步，当一个线程进入同步语句块时，另一个线程就必须等待。 1234567891011121314151617public class SynchronizedExample { public void func1() { synchronized (this) { for (int i = 0; i &lt; 10; i++) { System.out.print(i + \" \"); } } }}public static void main(String[] args) { SynchronizedExample e1 = new SynchronizedExample(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; e1.func1()); executorService.execute(() -&gt; e1.func1());}0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 对于以下代码，两个线程调用了不同对象的同步代码块，因此这两个线程就不需要同步。从输出结果可以看出，两个线程交叉执行。 12345678public static void main(String[] args) { SynchronizedExample e1 = new SynchronizedExample(); SynchronizedExample e2 = new SynchronizedExample(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; e1.func1()); executorService.execute(() -&gt; e2.func1());}0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 2. 同步一个方法 123public synchronized void func () { // ...} 它和同步代码块一样，作用于同一个对象。 3. 同步一个类 12345public void func() { synchronized (SynchronizedExample.class) { // ... }} 作用于整个类，也就是说两个线程调用同一个类的不同对象上的这种同步语句，也会进行同步。 123456789101112131415161718public class SynchronizedExample { public void func2() { synchronized (SynchronizedExample.class) { for (int i = 0; i &lt; 10; i++) { System.out.print(i + \" \"); } } }}public static void main(String[] args) { SynchronizedExample e1 = new SynchronizedExample(); SynchronizedExample e2 = new SynchronizedExample(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; e1.func2()); executorService.execute(() -&gt; e2.func2());}0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 4. 同步一个静态方法 123public synchronized static void fun() { // ...} 作用于整个类。 ReentrantLockReentrantLock 是 java.util.concurrent（J.U.C）包中的锁。 12345678910111213141516171819202122public class LockExample { private Lock lock = new ReentrantLock(); public void func() { lock.lock(); try { for (int i = 0; i &lt; 10; i++) { System.out.print(i + \" \"); } } finally { lock.unlock(); // 确保释放锁，从而避免发生死锁。 } }}public static void main(String[] args) { LockExample lockExample = new LockExample(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; lockExample.func()); executorService.execute(() -&gt; lockExample.func());}0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 比较1. 锁的实现 synchronized 是 JVM 实现的，而 ReentrantLock 是 JDK 实现的。 2. 性能 新版本 Java 对 synchronized 进行了很多优化，例如自旋锁等，synchronized 与 ReentrantLock 大致相同。 3. 等待可中断 当持有锁的线程长期不释放锁的时候，正在等待的线程可以选择放弃等待，改为处理其他事情。 ReentrantLock 可中断，而 synchronized 不行。 4. 公平锁 公平锁是指多个线程在等待同一个锁时，必须按照申请锁的时间顺序来依次获得锁。 synchronized 中的锁是非公平的，ReentrantLock 默认情况下也是非公平的，但是也可以是公平的。 5. 锁绑定多个条件 一个 ReentrantLock 可以同时绑定多个 Condition 对象。 使用选择除非需要使用 ReentrantLock 的高级功能，否则优先使用 synchronized。这是因为 synchronized 是 JVM 实现的一种锁机制，JVM 原生地支持它，而 ReentrantLock 不是所有的 JDK 版本都支持。并且使用 synchronized 不用担心没有释放锁而导致死锁问题，因为 JVM 会确保锁的释放。 线程之间的协作当多个线程可以一起工作去解决某个问题时，如果某些部分必须在其它部分之前完成，那么就需要对线程进行协调。 join()在线程中调用另一个线程的 join() 方法，会将当前线程挂起，而不是忙等待，直到目标线程结束。 对于以下代码，虽然 b 线程先启动，但是因为在 b 线程中调用了 a 线程的 join() 方法，b 线程会等待 a 线程结束才继续执行，因此最后能够保证 a 线程的输出先于 b 线程的输出。 1234567891011121314151617181920212223242526272829303132333435363738394041public class JoinExample { private class A extends Thread { @Override public void run() { System.out.println(\"A\"); } } private class B extends Thread { private A a; B(A a) { this.a = a; } @Override public void run() { try { a.join(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"B\"); } } public void test() { A a = new A(); B b = new B(a); b.start(); a.start(); }}public static void main(String[] args) { JoinExample example = new JoinExample(); example.test();}AB wait() notify() notifyAll()调用 wait() 使得线程等待某个条件满足，线程在等待时会被挂起，当其他线程的运行使得这个条件满足时，其它线程会调用 notify() 或者 notifyAll() 来唤醒挂起的线程。 它们都属于 Object 的一部分，而不属于 Thread。 只能用在同步方法或者同步控制块中使用，否则会在运行时抛出 IllegalMonitorStateException。 使用 wait() 挂起期间，线程会释放锁。这是因为，如果没有释放锁，那么其它线程就无法进入对象的同步方法或者同步控制块中，那么就无法执行 notify() 或者 notifyAll() 来唤醒挂起的线程，造成死锁。 123456789101112131415161718192021222324public class WaitNotifyExample { public synchronized void before() { System.out.println(\"before\"); notifyAll(); } public synchronized void after() { try { wait(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"after\"); }}public static void main(String[] args) { ExecutorService executorService = Executors.newCachedThreadPool(); WaitNotifyExample example = new WaitNotifyExample(); executorService.execute(() -&gt; example.after()); executorService.execute(() -&gt; example.before());}beforeafter wait() 和 sleep() 的区别 wait() 是 Object 的方法，而 sleep() 是 Thread 的静态方法； wait() 会释放锁，sleep() 不会。 await() signal() signalAll()java.util.concurrent 类库中提供了 Condition 类来实现线程之间的协调，可以在 Condition 上调用 await() 方法使线程等待，其它线程调用 signal() 或 signalAll() 方法唤醒等待的线程。 相比于 wait() 这种等待方式，await() 可以指定等待的条件，因此更加灵活。 使用 Lock 来获取一个 Condition 对象。 1234567891011121314151617181920212223242526272829303132333435public class AwaitSignalExample { private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); public void before() { lock.lock(); try { System.out.println(\"before\"); condition.signalAll(); } finally { lock.unlock(); } } public void after() { lock.lock(); try { condition.await(); System.out.println(\"after\"); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } }}public static void main(String[] args) { ExecutorService executorService = Executors.newCachedThreadPool(); AwaitSignalExample example = new AwaitSignalExample(); executorService.execute(() -&gt; example.after()); executorService.execute(() -&gt; example.before());}beforeafter J.U.C - AQSjava.util.concurrent（J.U.C）大大提高了并发性能，AQS 被认为是 J.U.C 的核心。 CountDownLatch用来控制一个或者多个线程等待多个线程。 维护了一个计数器 cnt，每次调用 countDown() 方法会让计数器的值减 1，减到 0 的时候，那些因为调用 await() 方法而在等待的线程就会被唤醒。 12345678910111213141516171819public class CountdownLatchExample { public static void main(String[] args) throws InterruptedException { final int totalThread = 10; CountDownLatch countDownLatch = new CountDownLatch(totalThread); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalThread; i++) { executorService.execute(() -&gt; { System.out.print(\"run..\"); countDownLatch.countDown(); }); } countDownLatch.await(); System.out.println(\"end\"); executorService.shutdown(); }}run..run..run..run..run..run..run..run..run..run..end CyclicBarrier用来控制多个线程互相等待，只有当多个线程都到达时，这些线程才会继续执行。 和 CountdownLatch 相似，都是通过维护计数器来实现的。线程执行 await() 方法之后计数器会减 1，并进行等待，直到计数器为 0，所有调用 await() 方法而在等待的线程才能继续执行。 CyclicBarrier 和 CountdownLatch 的一个区别是，CyclicBarrier 的计数器通过调用 reset() 方法可以循环使用，所以它才叫做循环屏障。 CyclicBarrier 有两个构造函数，其中 parties 指示计数器的初始值，barrierAction 在所有线程都到达屏障的时候会执行一次。 12345678910public CyclicBarrier(int parties, Runnable barrierAction) { if (parties &lt;= 0) throw new IllegalArgumentException(); this.parties = parties; this.count = parties; this.barrierCommand = barrierAction;}public CyclicBarrier(int parties) { this(parties, null);} 123456789101112131415161718192021public class CyclicBarrierExample { public static void main(String[] args) { final int totalThread = 10; CyclicBarrier cyclicBarrier = new CyclicBarrier(totalThread); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalThread; i++) { executorService.execute(() -&gt; { System.out.print(\"before..\"); try { cyclicBarrier.await(); } catch (InterruptedException | BrokenBarrierException e) { e.printStackTrace(); } System.out.print(\"after..\"); }); } executorService.shutdown(); }}before..before..before..before..before..before..before..before..before..before..after..after..after..after..after..after..after..after..after..after.. SemaphoreSemaphore 类似于操作系统中的信号量，可以控制对互斥资源的访问线程数。 以下代码模拟了对某个服务的并发请求，每次只能有 3 个客户端同时访问，请求总数为 10。 1234567891011121314151617181920212223public class SemaphoreExample { public static void main(String[] args) { final int clientCount = 3; final int totalRequestCount = 10; Semaphore semaphore = new Semaphore(clientCount); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalRequestCount; i++) { executorService.execute(()-&gt;{ try { semaphore.acquire(); System.out.print(semaphore.availablePermits() + \" \"); } catch (InterruptedException e) { e.printStackTrace(); } finally { semaphore.release(); } }); } executorService.shutdown(); }}2 1 2 2 2 2 2 1 2 2 J.U.C - 其它组件FutureTask在介绍 Callable 时我们知道它可以有返回值，返回值通过 Future 进行封装。FutureTask 实现了 RunnableFuture 接口，该接口继承自 Runnable 和 Future 接口，这使得 FutureTask 既可以当做一个任务执行，也可以有返回值。 12public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt;public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; FutureTask 可用于异步获取执行结果或取消执行任务的场景。当一个计算任务需要执行很长时间，那么就可以用 FutureTask 来封装这个任务，主线程在完成自己的任务之后再去获取结果。 1234567891011121314151617181920212223242526272829303132public class FutureTaskExample { public static void main(String[] args) throws ExecutionException, InterruptedException { FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;Integer&gt;(new Callable&lt;Integer&gt;() { @Override public Integer call() throws Exception { int result = 0; for (int i = 0; i &lt; 100; i++) { Thread.sleep(10); result += i; } return result; } }); Thread computeThread = new Thread(futureTask); computeThread.start(); Thread otherThread = new Thread(() -&gt; { System.out.println(\"other task is running...\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } }); otherThread.start(); System.out.println(futureTask.get()); }}other task is running...4950 BlockingQueuejava.util.concurrent.BlockingQueue 接口有以下阻塞队列的实现： FIFO 队列 ：LinkedBlockingQueue、ArrayBlockingQueue（固定长度） 优先级队列 ：PriorityBlockingQueue 提供了阻塞的 take() 和 put() 方法：如果队列为空 take() 将阻塞，直到队列中有内容；如果队列为满 put() 将阻塞，直到队列有空闲位置。 使用 BlockingQueue 实现生产者消费者问题 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class ProducerConsumer { private static BlockingQueue&lt;String&gt; queue = new ArrayBlockingQueue&lt;&gt;(5); private static class Producer extends Thread { @Override public void run() { try { queue.put(\"product\"); } catch (InterruptedException e) { e.printStackTrace(); } System.out.print(\"produce..\"); } } private static class Consumer extends Thread { @Override public void run() { try { String product = queue.take(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.print(\"consume..\"); } }}public static void main(String[] args) { for (int i = 0; i &lt; 2; i++) { Producer producer = new Producer(); producer.start(); } for (int i = 0; i &lt; 5; i++) { Consumer consumer = new Consumer(); consumer.start(); } for (int i = 0; i &lt; 3; i++) { Producer producer = new Producer(); producer.start(); }}produce..produce..consume..consume..produce..consume..produce..consume..produce..consume.. ForkJoin主要用于并行计算中，和 MapReduce 原理类似，都是把大的计算任务拆分成多个小任务并行计算。 12345678910111213141516171819202122232425262728293031323334353637public class ForkJoinExample extends RecursiveTask&lt;Integer&gt; { private final int threshold = 5; private int first; private int last; public ForkJoinExample(int first, int last) { this.first = first; this.last = last; } @Override protected Integer compute() { int result = 0; if (last - first &lt;= threshold) { // 任务足够小则直接计算 for (int i = first; i &lt;= last; i++) { result += i; } } else { // 拆分成小任务 int middle = first + (last - first) / 2; ForkJoinExample leftTask = new ForkJoinExample(first, middle); ForkJoinExample rightTask = new ForkJoinExample(middle + 1, last); leftTask.fork(); rightTask.fork(); result = leftTask.join() + rightTask.join(); } return result; }}public static void main(String[] args) throws ExecutionException, InterruptedException { ForkJoinExample example = new ForkJoinExample(1, 10000); ForkJoinPool forkJoinPool = new ForkJoinPool(); Future result = forkJoinPool.submit(example); System.out.println(result.get());} ForkJoin 使用 ForkJoinPool 来启动，它是一个特殊的线程池，线程数量取决于 CPU 核数。 1public class ForkJoinPool extends AbstractExecutorService ForkJoinPool 实现了工作窃取算法来提高 CPU 的利用率。每个线程都维护了一个双端队列，用来存储需要执行的任务。工作窃取算法允许空闲的线程从其它线程的双端队列中窃取一个任务来执行。窃取的任务必须是最晚的任务，避免和队列所属线程发生竞争。例如下图中，Thread2 从 Thread1 的队列中拿出最晚的 Task1 任务，Thread1 会拿出 Task2 来执行，这样就避免发生竞争。但是如果队列中只有一个任务时还是会发生竞争。 线程不安全示例如果多个线程对同一个共享数据进行访问而不采取同步操作的话，那么操作的结果是不一致的。 以下代码演示了 1000 个线程同时对 cnt 执行自增操作，操作结束之后它的值有可能小于 1000。 12345678910111213141516171819202122232425262728public class ThreadUnsafeExample { private int cnt = 0; public void add() { cnt++; } public int get() { return cnt; }}public static void main(String[] args) throws InterruptedException { final int threadSize = 1000; ThreadUnsafeExample example = new ThreadUnsafeExample(); final CountDownLatch countDownLatch = new CountDownLatch(threadSize); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; threadSize; i++) { executorService.execute(() -&gt; { example.add(); countDownLatch.countDown(); }); } countDownLatch.await(); executorService.shutdown(); System.out.println(example.get());}997 Java 内存模型Java 内存模型试图屏蔽各种硬件和操作系统的内存访问差异，以实现让 Java 程序在各种平台下都能达到一致的内存访问效果。 主内存与工作内存处理器上的寄存器的读写的速度比内存快几个数量级，为了解决这种速度矛盾，在它们之间加入了高速缓存。 加入高速缓存带来了一个新的问题：缓存一致性。如果多个缓存共享同一块主内存区域，那么多个缓存的数据可能会不一致，需要一些协议来解决这个问题。 所有的变量都存储在主内存中，每个线程还有自己的工作内存，工作内存存储在高速缓存或者寄存器中，保存了该线程使用的变量的主内存副本拷贝。 线程只能直接操作工作内存中的变量，不同线程之间的变量值传递需要通过主内存来完成。 内存间交互操作Java 内存模型定义了 8 个操作来完成主内存和工作内存的交互操作。 内存模型三大特性1. 原子性 Java 内存模型保证了 read、load、use、assign、store、write、lock 和 unlock 操作具有原子性，例如对一个 int 类型的变量执行 assign 赋值操作，这个操作就是原子性的。但是 Java 内存模型允许虚拟机将没有被 volatile 修饰的 64 位数据（long，double）的读写操作划分为两次 32 位的操作来进行，即 load、store、read 和 write 操作可以不具备原子性。 有一个错误认识就是，int 等原子性的类型在多线程环境中不会出现线程安全问题。前面的线程不安全示例代码中，cnt 属于 int 类型变量，1000 个线程对它进行自增操作之后，得到的值为 997 而不是 1000。 为了方便讨论，将内存间的交互操作简化为 3 个：load、assign、store。 下图演示了两个线程同时对 cnt 进行操作，load、assign、store 这一系列操作整体上看不具备原子性，那么在 T1 修改 cnt 并且还没有将修改后的值写入主内存，T2 依然可以读入旧值。可以看出，这两个线程虽然执行了两次自增运算，但是主内存中 cnt 的值最后为 1 而不是 2。因此对 int 类型读写操作满足原子性只是说明 load、assign、store 这些单个操作具备原子性。 AtomicInteger 能保证多个线程修改的原子性。 使用 AtomicInteger 重写之前线程不安全的代码之后得到以下线程安全实现： 12345678910111213141516171819202122232425262728public class AtomicExample { private AtomicInteger cnt = new AtomicInteger(); public void add() { cnt.incrementAndGet(); } public int get() { return cnt.get(); }}public static void main(String[] args) throws InterruptedException { final int threadSize = 1000; AtomicExample example = new AtomicExample(); // 只修改这条语句 final CountDownLatch countDownLatch = new CountDownLatch(threadSize); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; threadSize; i++) { executorService.execute(() -&gt; { example.add(); countDownLatch.countDown(); }); } countDownLatch.await(); executorService.shutdown(); System.out.println(example.get());}1000 除了使用原子类之外，也可以使用 synchronized 互斥锁来保证操作的原子性。它对应的内存间交互操作为：lock 和 unlock，在虚拟机实现上对应的字节码指令为 monitorenter 和 monitorexit。 12345678910111213141516171819202122232425262728public class AtomicSynchronizedExample { private int cnt = 0; public synchronized void add() { cnt++; } public synchronized int get() { return cnt; }}public static void main(String[] args) throws InterruptedException { final int threadSize = 1000; AtomicSynchronizedExample example = new AtomicSynchronizedExample(); final CountDownLatch countDownLatch = new CountDownLatch(threadSize); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; threadSize; i++) { executorService.execute(() -&gt; { example.add(); countDownLatch.countDown(); }); } countDownLatch.await(); executorService.shutdown(); System.out.println(example.get());}1000 2. 可见性 可见性指当一个线程修改了共享变量的值，其它线程能够立即得知这个修改。Java 内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值来实现可见性的。 主要有三种实现可见性的方式： volatile synchronized，对一个变量执行 unlock 操作之前，必须把变量值同步回主内存。 final，被 final 关键字修饰的字段在构造器中一旦初始化完成，并且没有发生 this 逃逸（其它线程通过 this 引用访问到初始化了一半的对象），那么其它线程就能看见 final 字段的值。 对前面的线程不安全示例中的 cnt 变量使用 volatile 修饰，不能解决线程不安全问题，因为 volatile 并不能保证操作的原子性。 3. 有序性 有序性是指：在本线程内观察，所有操作都是有序的。在一个线程观察另一个线程，所有操作都是无序的，无序是因为发生了指令重排序。在 Java 内存模型中，允许编译器和处理器对指令进行重排序，重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。 volatile 关键字通过添加内存屏障的方式来禁止指令重排，即重排序时不能把后面的指令放到内存屏障之前。 也可以通过 synchronized 来保证有序性，它保证每个时刻只有一个线程执行同步代码，相当于是让线程顺序执行同步代码。 先行发生原则上面提到了可以用 volatile 和 synchronized 来保证有序性。除此之外，JVM 还规定了先行发生原则，让一个操作无需控制就能先于另一个操作完成。 1. 单一线程原则 Single Thread rule 在一个线程内，在程序前面的操作先行发生于后面的操作。 2. 管程锁定规则 Monitor Lock Rule ​ 一个 unlock 操作先行发生于后面对同一个锁的 lock 操作。 3.volatile 变量规则 Volatile Variable Rule 对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作。 4. 线程启动规则 Thread Start Rule Thread 对象的 start() 方法调用先行发生于此线程的每一个动作。 5. 线程加入规则 Thread Join Rule Thread 对象的结束先行发生于 join() 方法返回。 6. 线程中断规则 Thread Interruption Rule 对线程 interrupt() 方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过 interrupted() 方法检测到是否有中断发生。 7. 对象终结规则 Finalizer Rule 一个对象的初始化完成（构造函数执行结束）先行发生于它的 finalize() 方法的开始。 8. 传递性 Transitivity 如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那么操作 A 先行发生于操作 C。 线程安全多个线程不管以何种方式访问某个类，并且在主调代码中不需要进行同步，都能表现正确的行为。 线程安全有以下几种实现方式： 不可变不可变（Immutable）的对象一定是线程安全的，不需要再采取任何的线程安全保障措施。只要一个不可变的对象被正确地构建出来，永远也不会看到它在多个线程之中处于不一致的状态。多线程环境下，应当尽量使对象成为不可变，来满足线程安全。 不可变的类型： final 关键字修饰的基本数据类型 String 枚举类型 Number 部分子类，如 Long 和 Double 等数值包装类型，BigInteger 和 BigDecimal 等大数据类型。但同为 Number 的原子类 AtomicInteger 和 AtomicLong 则是可变的。 对于集合类型，可以使用 Collections.unmodifiableXXX() 方法来获取一个不可变的集合。 12345678910public class ImmutableExample { public static void main(String[] args) { Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;(); Map&lt;String, Integer&gt; unmodifiableMap = Collections.unmodifiableMap(map); unmodifiableMap.put(\"a\", 1); }}Exception in thread \"main\" java.lang.UnsupportedOperationException at java.util.Collections$UnmodifiableMap.put(Collections.java:1457) at ImmutableExample.main(ImmutableExample.java:9) Collections.unmodifiableXXX() 先对原始的集合进行拷贝，需要对集合进行修改的方法都直接抛出异常。 123public V put(K key, V value) { throw new UnsupportedOperationException();} 互斥同步synchronized 和 ReentrantLock。 非阻塞同步互斥同步最主要的问题就是线程阻塞和唤醒所带来的性能问题，因此这种同步也称为阻塞同步。 互斥同步属于一种悲观的并发策略，总是认为只要不去做正确的同步措施，那就肯定会出现问题。无论共享数据是否真的会出现竞争，它都要进行加锁（这里讨论的是概念模型，实际上虚拟机会优化掉很大一部分不必要的加锁）、用户态核心态转换、维护锁计数器和检查是否有被阻塞的线程需要唤醒等操作。 1. CAS 随着硬件指令集的发展，我们可以使用基于冲突检测的乐观并发策略：先进行操作，如果没有其它线程争用共享数据，那操作就成功了，否则采取补偿措施（不断地重试，直到成功为止）。这种乐观的并发策略的许多实现都不需要将线程阻塞，因此这种同步操作称为非阻塞同步。 乐观锁需要操作和冲突检测这两个步骤具备原子性，这里就不能再使用互斥同步来保证了，只能靠硬件来完成。硬件支持的原子性操作最典型的是：比较并交换（Compare-and-Swap，CAS）。CAS 指令需要有 3 个操作数，分别是内存地址 V、旧的预期值 A 和新值 B。当执行操作时，只有当 V 的值等于 A，才将 V 的值更新为 B。 2. AtomicInteger J.U.C 包里面的整数原子类 AtomicInteger 的方法调用了 Unsafe 类的 CAS 操作。 以下代码使用了 AtomicInteger 执行了自增的操作。 12345private AtomicInteger cnt = new AtomicInteger();public void add() { cnt.incrementAndGet();} 以下代码是 incrementAndGet() 的源码，它调用了 Unsafe 的 getAndAddInt() 。 123public final int incrementAndGet() { return unsafe.getAndAddInt(this, valueOffset, 1) + 1;} 以下代码是 getAndAddInt() 源码，var1 指示对象内存地址，var2 指示该字段相对对象内存地址的偏移，var4 指示操作需要加的数值，这里为 1。通过 getIntVolatile(var1, var2) 得到旧的预期值，通过调用 compareAndSwapInt() 来进行 CAS 比较，如果该字段内存地址中的值等于 var5，那么就更新内存地址为 var1+var2 的变量为 var5+var4。 可以看到 getAndAddInt() 在一个循环中进行，发生冲突的做法是不断的进行重试。 12345678public final int getAndAddInt(Object var1, long var2, int var4) { int var5; do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5;} 3. ABA 如果一个变量初次读取的时候是 A 值，它的值被改成了 B，后来又被改回为 A，那 CAS 操作就会误认为它从来没有被改变过。 J.U.C 包提供了一个带有标记的原子引用类 AtomicStampedReference 来解决这个问题，它可以通过控制变量值的版本来保证 CAS 的正确性。大部分情况下 ABA 问题不会影响程序并发的正确性，如果需要解决 ABA 问题，改用传统的互斥同步可能会比原子类更高效。 无同步方案要保证线程安全，并不是一定就要进行同步。如果一个方法本来就不涉及共享数据，那它自然就无须任何同步措施去保证正确性。 1. 栈封闭 多个线程访问同一个方法的局部变量时，不会出现线程安全问题，因为局部变量存储在虚拟机栈中，属于线程私有的。 123456789101112131415161718public class StackClosedExample { public void add100() { int cnt = 0; for (int i = 0; i &lt; 100; i++) { cnt++; } System.out.println(cnt); }}public static void main(String[] args) { StackClosedExample example = new StackClosedExample(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; example.add100()); executorService.execute(() -&gt; example.add100()); executorService.shutdown();}100100 2. 线程本地存储（Thread Local Storage） 如果一段代码中所需要的数据必须与其他代码共享，那就看看这些共享数据的代码是否能保证在同一个线程中执行。如果能保证，我们就可以把共享数据的可见范围限制在同一个线程之内，这样，无须同步也能保证线程之间不出现数据争用的问题。 符合这种特点的应用并不少见，大部分使用消费队列的架构模式（如“生产者-消费者”模式）都会将产品的消费过程尽量在一个线程中消费完。其中最重要的一个应用实例就是经典 Web 交互模型中的“一个请求对应一个服务器线程”（Thread-per-Request）的处理方式，这种处理方式的广泛应用使得很多 Web 服务端应用都可以使用线程本地存储来解决线程安全问题。 可以使用 java.lang.ThreadLocal 类来实现线程本地存储功能。 对于以下代码，thread1 中设置 threadLocal 为 1，而 thread2 设置 threadLocal 为 2。过了一段时间之后，thread1 读取 threadLocal 依然是 1，不受 thread2 的影响。 12345678910111213141516171819202122public class ThreadLocalExample { public static void main(String[] args) { ThreadLocal threadLocal = new ThreadLocal(); Thread thread1 = new Thread(() -&gt; { threadLocal.set(1); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(threadLocal.get()); threadLocal.remove(); }); Thread thread2 = new Thread(() -&gt; { threadLocal.set(2); threadLocal.remove(); }); thread1.start(); thread2.start(); }}1 为了理解 ThreadLocal，先看以下代码： 12345678910111213141516public class ThreadLocalExample1 { public static void main(String[] args) { ThreadLocal threadLocal1 = new ThreadLocal(); ThreadLocal threadLocal2 = new ThreadLocal(); Thread thread1 = new Thread(() -&gt; { threadLocal1.set(1); threadLocal2.set(1); }); Thread thread2 = new Thread(() -&gt; { threadLocal1.set(2); threadLocal2.set(2); }); thread1.start(); thread2.start(); }} 它所对应的底层结构图为： 每个 Thread 都有一个 ThreadLocal.ThreadLocalMap 对象。 123/* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ThreadLocal.ThreadLocalMap threadLocals = null; 当调用一个 ThreadLocal 的 set(T value) 方法时，先得到当前线程的 ThreadLocalMap 对象，然后将 ThreadLocal-&gt;value 键值对插入到该 Map 中。 12345678public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);} get() 方法类似。 12345678910111213public T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings(\"unchecked\") T result = (T)e.value; return result; } } return setInitialValue();} ThreadLocal 从理论上讲并不是用来解决多线程并发问题的，因为根本不存在多线程竞争。 在一些场景 (尤其是使用线程池) 下，由于 ThreadLocal.ThreadLocalMap 的底层数据结构导致 ThreadLocal 有内存泄漏的情况，应该尽可能在每次使用 ThreadLocal 后手动调用 remove()，以避免出现 ThreadLocal 经典的内存泄漏甚至是造成自身业务混乱的风险。 3. 可重入代码（Reentrant Code） 这种代码也叫做纯代码（Pure Code），可以在代码执行的任何时刻中断它，转而去执行另外一段代码（包括递归调用它本身），而在控制权返回后，原来的程序不会出现任何错误。 可重入代码有一些共同的特征，例如不依赖存储在堆上的数据和公用的系统资源、用到的状态量都由参数中传入、不调用非可重入的方法等。 锁优化这里的锁优化主要是指 JVM 对 synchronized 的优化。 自旋锁互斥同步进入阻塞状态的开销都很大，应该尽量避免。在许多应用中，共享数据的锁定状态只会持续很短的一段时间。自旋锁的思想是让一个线程在请求一个共享数据的锁时执行忙循环（自旋）一段时间，如果在这段时间内能获得锁，就可以避免进入阻塞状态。 自旋锁虽然能避免进入阻塞状态从而减少开销，但是它需要进行忙循环操作占用 CPU 时间，它只适用于共享数据的锁定状态很短的场景。 在 JDK 1.6 中引入了自适应的自旋锁。自适应意味着自旋的次数不再固定了，而是由前一次在同一个锁上的自旋次数及锁的拥有者的状态来决定。 锁消除锁消除是指对于被检测出不可能存在竞争的共享数据的锁进行消除。 锁消除主要是通过逃逸分析来支持，如果堆上的共享数据不可能逃逸出去被其它线程访问到，那么就可以把它们当成私有数据对待，也就可以将它们的锁进行消除。 对于一些看起来没有加锁的代码，其实隐式的加了很多锁。例如下面的字符串拼接代码就隐式加了锁： 123public static String concatString(String s1, String s2, String s3) { return s1 + s2 + s3;} String 是一个不可变的类，编译器会对 String 的拼接自动优化。在 JDK 1.5 之前，会转化为 StringBuffer 对象的连续 append() 操作： 1234567public static String concatString(String s1, String s2, String s3) { StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); sb.append(s3); return sb.toString();} 每个 append() 方法中都有一个同步块。虚拟机观察变量 sb，很快就会发现它的动态作用域被限制在 concatString() 方法内部。也就是说，sb 的所有引用永远不会逃逸到 concatString() 方法之外，其他线程无法访问到它，因此可以进行消除。 锁粗化如果一系列的连续操作都对同一个对象反复加锁和解锁，频繁的加锁操作就会导致性能损耗。 上一节的示例代码中连续的 append() 方法就属于这类情况。如果虚拟机探测到由这样的一串零碎的操作都对同一个对象加锁，将会把加锁的范围扩展（粗化）到整个操作序列的外部。对于上一节的示例代码就是扩展到第一个 append() 操作之前直至最后一个 append() 操作之后，这样只需要加锁一次就可以了。 轻量级锁JDK 1.6 引入了偏向锁和轻量级锁，从而让锁拥有了四个状态：无锁状态（unlocked）、偏向锁状态（biasble）、轻量级锁状态（lightweight locked）和重量级锁状态（inflated）。 以下是 HotSpot 虚拟机对象头的内存布局，这些数据被称为 Mark Word。其中 tag bits 对应了五个状态，这些状态在右侧的 state 表格中给出。除了 marked for gc 状态，其它四个状态已经在前面介绍过了。 下图左侧是一个线程的虚拟机栈，其中有一部分称为 Lock Record 的区域，这是在轻量级锁运行过程创建的，用于存放锁对象的 Mark Word。而右侧就是一个锁对象，包含了 Mark Word 和其它信息。 轻量级锁是相对于传统的重量级锁而言，它使用 CAS 操作来避免重量级锁使用互斥量的开销。对于绝大部分的锁，在整个同步周期内都是不存在竞争的，因此也就不需要都使用互斥量进行同步，可以先采用 CAS 操作进行同步，如果 CAS 失败了再改用互斥量进行同步。 当尝试获取一个锁对象时，如果锁对象标记为 0 01，说明锁对象的锁未锁定（unlocked）状态。此时虚拟机在当前线程的虚拟机栈中创建 Lock Record，然后使用 CAS 操作将对象的 Mark Word 更新为 Lock Record 指针。如果 CAS 操作成功了，那么线程就获取了该对象上的锁，并且对象的 Mark Word 的锁标记变为 00，表示该对象处于轻量级锁状态。 如果 CAS 操作失败了，虚拟机首先会检查对象的 Mark Word 是否指向当前线程的虚拟机栈，如果是的话说明当前线程已经拥有了这个锁对象，那就可以直接进入同步块继续执行，否则说明这个锁对象已经被其他线程线程抢占了。如果有两条以上的线程争用同一个锁，那轻量级锁就不再有效，要膨胀为重量级锁。 偏向锁偏向锁的思想是偏向于让第一个获取锁对象的线程，这个线程在之后获取该锁就不再需要进行同步操作，甚至连 CAS 操作也不再需要。 当锁对象第一次被线程获得的时候，进入偏向状态，标记为 1 01。同时使用 CAS 操作将线程 ID 记录到 Mark Word 中，如果 CAS 操作成功，这个线程以后每次进入这个锁相关的同步块就不需要再进行任何同步操作。 当有另外一个线程去尝试获取这个锁对象时，偏向状态就宣告结束，此时撤销偏向（Revoke Bias）后恢复到未锁定状态或者轻量级锁状态。 多线程开发良好的实践 给线程起个有意义的名字，这样可以方便找 Bug。 缩小同步范围，从而减少锁争用。例如对于 synchronized，应该尽量使用同步块而不是同步方法。 多用同步工具少用 wait() 和 notify()。首先，CountDownLatch, CyclicBarrier, Semaphore 和 Exchanger 这些同步类简化了编码操作，而用 wait() 和 notify() 很难实现复杂控制流；其次，这些同步类是由最好的企业编写和维护，在后续的 JDK 中还会不断优化和完善。 使用 BlockingQueue 实现生产者消费者问题。 多用并发集合少用同步集合，例如应该使用 ConcurrentHashMap 而不是 Hashtable。 使用本地变量和不可变类来保证线程安全。 使用线程池而不是直接创建线程，这是因为创建线程代价很高，线程池可以有效地利用有限的线程来启动任务。 参考自来源感谢作者的辛勤整理，转载收藏，如有侵权，请评论说明，立即处理。","link":"/2019/08/07/Java并发相关知识点.html"},{"title":"Java 垃圾回收","text":"摘要之前上学的时候有这个一个梗，说在食堂里吃饭，吃完把餐盘端走清理的，是 C++ 程序员，吃完直接就走的，是 Java 程序员。🤔 之前上学的时候有这个一个梗，说在食堂里吃饭，吃完把餐盘端走清理的，是 C++ 程序员，吃完直接就走的，是 Java 程序员。🤔 确实，在 Java 的世界里，似乎我们不用对垃圾回收那么的专注，很多初学者不懂 GC，也依然能写出一个能用甚至还不错的程序或系统。但其实这并不代表 Java 的 GC 就不重要。相反，它是那么的重要和复杂，以至于出了问题，那些初学者除了打开 GC 日志，看着一堆0101的天文，啥也做不了。😯 今天我们就从头到尾完整地聊一聊 Java 的垃圾回收 什么是垃圾回收 垃圾回收（Garbage Collection，GC），顾名思义就是释放垃圾占用的空间，防止内存泄露。有效的使用可以使用的内存，对内存堆中已经死亡的或者长时间没有使用的对象进行清除和回收。 Java 语言出来之前，大家都在拼命的写 C 或者 C++ 的程序，而此时存在一个很大的矛盾，C++ 等语言创建对象要不断的去开辟空间，不用的时候又需要不断的去释放控件，既要写构造函数，又要写析构函数，很多时候都在重复的 allocated，然后不停的析构。于是，有人就提出，能不能写一段程序实现这块功能，每次创建，释放控件的时候复用这段代码，而无需重复的书写呢？ 1960年，基于 MIT 的 Lisp 首先提出了垃圾回收的概念，而这时 Java 还没有出世呢！所以实际上 GC 并不是Java的专利，GC 的历史远远大于 Java 的历史！ 怎么定义垃圾 既然我们要做垃圾回收，首先我们得搞清楚垃圾的定义是什么，哪些内存是需要回收的。 引用计数算法引用计数算法（Reachability Counting）是通过在对象头中分配一个空间来保存该对象被引用的次数（Reference Count）。如果该对象被其它对象引用，则它的引用计数加1，如果删除对该对象的引用，那么它的引用计数就减1，当该对象的引用计数为0时，那么该对象就会被回收。 1String m = new String(\"jack\"); 先创建一个字符串，这时候”jack”有一个引用，就是 m。 然后将 m 设置为 null，这时候”jack”的引用次数就等于0了，在引用计数算法中，意味着这块内容就需要被回收了。 1m = null; 引用计数算法是将垃圾回收分摊到整个应用程序的运行当中了，而不是在进行垃圾收集时，要挂起整个应用的运行，直到对堆中所有对象的处理都结束。因此，采用引用计数的垃圾收集不属于严格意义上的”Stop-The-World”的垃圾收集机制。 看似很美好，但我们知道JVM的垃圾回收就是”Stop-The-World”的，那是什么原因导致我们最终放弃了引用计数算法呢？看下面的例子。 1234567891011121314151617public class ReferenceCountingGC { public Object instance; public ReferenceCountingGC(String name){}}public static void testGC(){ ReferenceCountingGC a = new ReferenceCountingGC(\"objA\"); ReferenceCountingGC b = new ReferenceCountingGC(\"objB\"); a.instance = b; b.instance = a; a = null; b = null;} 定义2个对象 相互引用 置空各自的声明引用 我们可以看到，最后这2个对象已经不可能再被访问了，但由于他们相互引用着对方，导致它们的引用计数永远都不会为0，通过引用计数算法，也就永远无法通知GC收集器回收它们。 可达性分析算法 可达性分析算法（Reachability Analysis）的基本思路是，通过一些被称为引用链（GC Roots）的对象作为起点，从这些节点开始向下搜索，搜索走过的路径被称为（Reference Chain)，当一个对象到 GC Roots 没有任何引用链相连时（即从 GC Roots 节点到该节点不可达），则证明该对象是不可用的。 通过可达性算法，成功解决了引用计数所无法解决的问题-“循环依赖”，只要你无法与 GC Root 建立直接或间接的连接，系统就会判定你为可回收对象。那这样就引申出了另一个问题，哪些属于 GC Root。 Java 内存区域在 Java 语言中，可作为 GC Root 的对象包括以下4种： 虚拟机栈（栈帧中的本地变量表）中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈中 JNI（即一般说的 Native 方法）引用的对象 虚拟机栈（栈帧中的本地变量表）中引用的对象此时的 s，即为 GC Root，当s置空时，localParameter 对象也断掉了与 GC Root 的引用链，将被回收。 12345678public class StackLocalParameter { public StackLocalParameter(String name){}}public static void testGC(){ StackLocalParameter s = new StackLocalParameter(\"localParameter\"); s = null;} 方法区中类静态属性引用的对象s 为 GC Root，s 置为 null，经过 GC 后，s 所指向的 properties 对象由于无法与 GC Root 建立关系被回收。 而 m 作为类的静态属性，也属于 GC Root，parameter 对象依然与 GC root 建立着连接，所以此时 parameter 对象并不会被回收。 12345678910public class MethodAreaStaicProperties { public static MethodAreaStaicProperties m; public MethodAreaStaicProperties(String name){}}public static void testGC(){ MethodAreaStaicProperties s = new MethodAreaStaicProperties(\"properties\"); s.m = new MethodAreaStaicProperties(\"parameter\"); s = null;} 方法区中常量引用的对象m 即为方法区中的常量引用，也为 GC Root，s 置为 null 后，final 对象也不会因没有与 GC Root 建立联系而被回收。 123456789public class MethodAreaStaicProperties { public static final MethodAreaStaicProperties m = MethodAreaStaicProperties(\"final\"); public MethodAreaStaicProperties(String name){}}public static void testGC(){ MethodAreaStaicProperties s = new MethodAreaStaicProperties(\"staticProperties\"); s = null;} 本地方法栈中引用的对象任何 Native 接口都会使用某种本地方法栈，实现的本地方法接口是使用 C 连接模型的话，那么它的本地方法栈就是 C 栈。当线程调用 Java 方法时，虚拟机会创建一个新的栈帧并压入 Java 栈。然而当它调用的是本地方法时，虚拟机会保持 Java 栈不变，不再在线程的 Java 栈中压入新的帧，虚拟机只是简单地动态连接并直接调用指定的本地方法。 怎么回收垃圾在确定了哪些垃圾可以被回收后，垃圾收集器要做的事情就是开始进行垃圾回收，但是这里面涉及到一个问题是：如何高效地进行垃圾回收。由于Java虚拟机规范并没有对如何实现垃圾收集器做出明确的规定，因此各个厂商的虚拟机可以采用不同的方式来实现垃圾收集器，这里我们讨论几种常见的垃圾收集算法的核心思想。 标记 — 清除算法 标记清除算法（Mark-Sweep）是最基础的一种垃圾回收算法，它分为2部分，先把内存区域中的这些对象进行标记，哪些属于可回收标记出来，然后把这些垃圾拎出来清理掉。就像上图一样，清理掉的垃圾就变成未使用的内存区域，等待被再次使用。 这逻辑再清晰不过了，并且也很好操作，但它存在一个很大的问题，那就是内存碎片。 上图中等方块的假设是 2M，小一些的是 1M，大一些的是 4M。等我们回收完，内存就会切成了很多段。我们知道开辟内存空间时，需要的是连续的内存区域，这时候我们需要一个 2M的内存区域，其中有2个 1M 是没法用的。这样就导致，其实我们本身还有这么多的内存的，但却用不了。 复制算法 复制算法（Copying）是在标记清除算法上演化而来，解决标记清除算法的内存碎片问题。它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。保证了内存的连续可用，内存分配时也就不用考虑内存碎片等复杂情况，逻辑清晰，运行高效。 上面的图很清楚，也很明显的暴露了另一个问题，合着我这140平的大三房，只能当70平米的小两房来使？代价实在太高。 标记整理算法 标记整理算法（Mark-Compact）标记过程仍然与标记 — 清除算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，再清理掉端边界以外的内存区域。 标记整理算法一方面在标记-清除算法上做了升级，解决了内存碎片的问题，也规避了复制算法只能利用一半内存区域的弊端。看起来很美好，但从上图可以看到，它对内存变动更频繁，需要整理所有存活对象的引用地址，在效率上比复制算法要差很多。 分代收集算法分代收集算法（Generational Collection）严格来说并不是一种思想或理论，而是融合上述3种基础的算法思想，而产生的针对不同情况所采用不同算法的一套组合拳。对象存活周期的不同将内存划分为几块。一般是把 Java 堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用标记-清理或者标记 — 整理算法来进行回收。so，另一个问题来了，那内存区域到底被分为哪几块，每一块又有什么特别适合什么算法呢？ 内存模型与回收策略 Java 堆（Java Heap）是JVM所管理的内存中最大的一块，堆又是垃圾收集器管理的主要区域，这里我们主要分析一下 Java 堆的结构。 Java 堆主要分为2个区域-年轻代与老年代，其中年轻代又分 Eden 区和 Survivor 区，其中 Survivor 区又分 From 和 To 2个区。可能这时候大家会有疑问，为什么需要 Survivor 区，为什么Survivor 还要分2个区。不着急，我们从头到尾，看看对象到底是怎么来的，而它又是怎么没的。 Eden 区IBM 公司的专业研究表明，有将近98%的对象是朝生夕死，所以针对这一现状，大多数情况下，对象会在新生代 Eden 区中进行分配，当 Eden 区没有足够空间进行分配时，虚拟机会发起一次 Minor GC，Minor GC 相比 Major GC 更频繁，回收速度也更快。 通过 Minor GC 之后，Eden 会被清空，Eden 区中绝大部分对象会被回收，而那些无需回收的存活对象，将会进到 Survivor 的 From 区（若 From 区不够，则直接进入 Old 区）。 Survivor 区Survivor 区相当于是 Eden 区和 Old 区的一个缓冲，类似于我们交通灯中的黄灯。Survivor 又分为2个区，一个是 From 区，一个是 To 区。每次执行 Minor GC，会将 Eden 区和 From 存活的对象放到 Survivor 的 To 区（如果 To 区不够，则直接进入 Old 区）。 为啥需要？不就是新生代到老年代么，直接 Eden 到 Old 不好了吗，为啥要这么复杂。想想如果没有 Survivor 区，Eden 区每进行一次 Minor GC，存活的对象就会被送到老年代，老年代很快就会被填满。而有很多对象虽然一次 Minor GC 没有消灭，但其实也并不会蹦跶多久，或许第二次，第三次就需要被清除。这时候移入老年区，很明显不是一个明智的决定。 所以，Survivor 的存在意义就是减少被送到老年代的对象，进而减少 Major GC 的发生。Survivor 的预筛选保证，只有经历16次 Minor GC 还能在新生代中存活的对象，才会被送到老年代。 为啥需要俩？设置两个 Survivor 区最大的好处就是解决内存碎片化。 我们先假设一下，Survivor 如果只有一个区域会怎样。Minor GC 执行后，Eden 区被清空了，存活的对象放到了 Survivor 区，而之前 Survivor 区中的对象，可能也有一些是需要被清除的。问题来了，这时候我们怎么清除它们？在这种场景下，我们只能标记清除，而我们知道标记清除最大的问题就是内存碎片，在新生代这种经常会消亡的区域，采用标记清除必然会让内存产生严重的碎片化。因为 Survivor 有2个区域，所以每次 Minor GC，会将之前 Eden 区和 From 区中的存活对象复制到 To 区域。第二次 Minor GC 时，From 与 To 职责兑换，这时候会将 Eden 区和 To 区中的存活对象再复制到 From 区域，以此反复。 这种机制最大的好处就是，整个过程中，永远有一个 Survivor space 是空的，另一个非空的 Survivor space 是无碎片的。那么，Survivor 为什么不分更多块呢？比方说分成三个、四个、五个?显然，如果 Survivor 区再细分下去，每一块的空间就会比较小，容易导致 Survivor 区满，两块 Survivor 区可能是经过权衡之后的最佳方案。 Old 区老年代占据着2/3的堆内存空间，只有在 Major GC 的时候才会进行清理，每次 GC 都会触发“Stop-The-World”。内存越大，STW 的时间也越长，所以内存也不仅仅是越大就越好。由于复制算法在对象存活率较高的老年代会进行很多次的复制操作，效率很低，所以老年代这里采用的是标记 — 整理算法。 除了上述所说，在内存担保机制下，无法安置的对象会直接进到老年代，以下几种情况也会进入老年代。 大对象大对象指需要大量连续内存空间的对象，这部分对象不管是不是“朝生夕死”，都会直接进到老年代。这样做主要是为了避免在 Eden 区及2个 Survivor 区之间发生大量的内存复制。当你的系统有非常多“朝生夕死”的大对象时，得注意了。 长期存活对象虚拟机给每个对象定义了一个对象年龄（Age）计数器。正常情况下对象会不断的在 Survivor 的 From 区与 To 区之间移动，对象在 Survivor 区中每经历一次 Minor GC，年龄就增加1岁。当年龄增加到15岁时，这时候就会被转移到老年代。当然，这里的15，JVM 也支持进行特殊设置。 动态对象年龄虚拟机并不重视要求对象年龄必须到15岁，才会放入老年区，如果 Survivor 空间中相同年龄所有对象大小的总合大于 Survivor 空间的一半，年龄大于等于该年龄的对象就可以直接进去老年区，无需等你“成年”。 这其实有点类似于负载均衡，轮询是负载均衡的一种，保证每台机器都分得同样的请求。看似很均衡，但每台机的硬件不通，健康状况不同，我们还可以基于每台机接受的请求数，或每台机的响应时间等，来调整我们的负载均衡算法。 参考自 .","link":"/2019/07/24/Java-垃圾回收.html"},{"title":"Java BIO NIO AIO区别与使用","text":"摘要BIO 全称Block-IO 是一种同步且阻塞的通信模式。是一个比较传统的通信方式，模式简单，使用方便。但并发处理能力低，通信耗时，依赖网速。 Java BIO使用BIO实现文件的读取和写入。 123456789101112131415161718192021222324252627282930313233343536//Initializes The ObjectUser1 user = new User1();user.setName(\"hollis\");user.setAge(23);System.out.println(user);//Write Obj to FileObjectOutputStream oos = null;try { oos = new ObjectOutputStream(new FileOutputStream(\"tempFile\")); oos.writeObject(user);} catch (IOException e) { e.printStackTrace();} finally { IOUtils.closeQuietly(oos);}//Read Obj from FileFile file = new File(\"tempFile\");ObjectInputStream ois = null;try { ois = new ObjectInputStream(new FileInputStream(file)); User1 newUser = (User1) ois.readObject(); System.out.println(newUser);} catch (IOException e) { e.printStackTrace();} catch (ClassNotFoundException e) { e.printStackTrace();} finally { IOUtils.closeQuietly(ois); try { FileUtils.forceDelete(file); } catch (IOException e) { e.printStackTrace(); }} Java NIOJava NIO，全程 Non-Block IO ，是Java SE 1.4版以后，针对网络传输效能优化的新功能。是一种非阻塞同步的通信模式。NIO 与原来的 I/O 有同样的作用和目的, 他们之间最重要的区别是数据打包和传输的方式。原来的 I/O 以流的方式处理数据，而 NIO 以块的方式处理数据。面向流的 I/O 系统一次一个字节地处理数据。一个输入流产生一个字节的数据，一个输出流消费一个字节的数据。面向块的 I/O 系统以块的形式处理数据。每一个操作都在一步中产生或者消费一个数据块。按块处理数据比按(流式的)字节处理数据要快得多。但是面向块的 I/O 缺少一些面向流的 I/O 所具有的优雅性和简单性。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364static void readNIO() { String pathname = \"C:\\\\Users\\\\adew\\\\Desktop\\\\jd-gui.cfg\"; FileInputStream fin = null; try { fin = new FileInputStream(new File(pathname)); FileChannel channel = fin.getChannel(); int capacity = 100;// 字节 ByteBuffer bf = ByteBuffer.allocate(capacity); int length = -1; while ((length = channel.read(bf)) != -1) { bf.clear(); byte[] bytes = bf.array(); System.out.write(bytes, 0, length); System.out.println(); } channel.close(); } catch (FileNotFoundException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } finally { if (fin != null) { try { fin.close(); } catch (IOException e) { e.printStackTrace(); } } } } static void writeNIO() { String filename = \"out.txt\"; FileOutputStream fos = null; try { fos = new FileOutputStream(new File(filename)); FileChannel channel = fos.getChannel(); ByteBuffer src = Charset.forName(\"utf8\").encode(\"你好你好你好你好你好\"); int length = 0; while ((length = channel.write(src)) != 0) { System.out.println(\"写入长度:\" + length); } } catch (FileNotFoundException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } finally { if (fos != null) { try { fos.close(); } catch (IOException e) { e.printStackTrace(); } } } } Java AIOJava AIO，全程 Asynchronous IO，是异步非阻塞的IO。是一种非阻塞异步的通信模式。在NIO的基础上引入了新的异步通道的概念，并提供了异步文件通道和异步套接字通道的实现。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class ReadFromFile { public static void main(String[] args) throws Exception { Path file = Paths.get(\"/usr/a.txt\"); AsynchronousFileChannel channel = AsynchronousFileChannel.open(file); ByteBuffer buffer = ByteBuffer.allocate(100_000); Future&lt;Integer&gt; result = channel.read(buffer, 0); while (!result.isDone()) { ProfitCalculator.calculateTax(); } Integer bytesRead = result.get(); System.out.println(\"Bytes read [\" + bytesRead + \"]\"); }}class ProfitCalculator { public ProfitCalculator() { } public static void calculateTax() { }}public class WriteToFile { public static void main(String[] args) throws Exception { AsynchronousFileChannel fileChannel = AsynchronousFileChannel.open( Paths.get(\"/asynchronous.txt\"), StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE); CompletionHandler&lt;Integer, Object&gt; handler = new CompletionHandler&lt;Integer, Object&gt;() { @Override public void completed(Integer result, Object attachment) { System.out.println(\"Attachment: \" + attachment + \" \" + result + \" bytes written\"); System.out.println(\"CompletionHandler Thread ID: \" + Thread.currentThread().getId()); } @Override public void failed(Throwable e, Object attachment) { System.err.println(\"Attachment: \" + attachment + \" failed with:\"); e.printStackTrace(); } }; System.out.println(\"Main Thread ID: \" + Thread.currentThread().getId()); fileChannel.write(ByteBuffer.wrap(\"Sample\".getBytes()), 0, \"First Write\", handler); fileChannel.write(ByteBuffer.wrap(\"Box\".getBytes()), 0, \"Second Write\", handler); }} 三种IO的区别首先，我们站在宏观的角度，重新画一下重点： BIO （Blocking I/O）：同步阻塞I/O模式。 NIO （New I/O）：同步非阻塞模式。 AIO （Asynchronous I/O）：异步非阻塞I/O模型。 同步请求，A调用B，B的处理是同步的，在处理完之前他不会通知A，只有处理完之后才会明确的通知A。 异步请求，A调用B，B的处理是异步的，B在接到请求后先告诉A我已经接到请求了，然后异步去处理，处理完之后通过回调等方式再通知A。 所以说，同步和异步最大的区别就是被调用方的执行方式和返回时机。同步指的是被调用方做完事情之后再返回，异步指的是被调用方先返回，然后再做事情，做完之后再想办法通知调用方。 阻塞请求，A调用B，A一直等着B的返回，别的事情什么也不干。 非阻塞请求，A调用B，A不用一直等着B的返回，先去忙别的事情了。 所以说，阻塞非阻塞最大的区别就是在被调用方返回结果之前的这段时间内，调用方是否一直等待。阻塞指的是调用方一直等待别的事情什么都不做。非阻塞指的是调用方先去忙别的事情。 同步阻塞模式：这种模式下，我们的工作模式是先来到厨房，开始烧水，并坐在水壶面前一直等着水烧开。 同步非阻塞模式：这种模式下，我们的工作模式是先来到厨房，开始烧水，但是我们不一直坐在水壶前面等，而是回到客厅看电视，然后每隔几分钟到厨房看一下水有没有烧开。 异步非阻塞I/O模型：这种模式下，我们的工作模式是先来到厨房，开始烧水，我们不一一直坐在水壶前面等，也不隔一段时间去看一下，而是在客厅看电视，水壶上面有个开关，水烧开之后他会通知我。 阻塞VS非阻塞：人是否坐在水壶前面一直等。 同步VS异步：水壶是不是在水烧开之后主动通知人。 适用场景BIO方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序直观简单易理解。 NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。 AIO方式适用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持。 参考自 .","link":"/2019/07/19/Java-BIO-NIO-AIO区别与使用.html"},{"title":"常见sql错误写法","text":"摘要sql语句应用不当，可能仅仅一个顺序的不同，往往会带来成千上万倍的耗时问题。 1、LIMIT 语句分页查询是最常用的场景之一，但也通常也是最容易出问题的地方。比如对于下面简单的语句，一般 DBA 想到的办法是在 type, name, create_time 字段上加组合索引。这样条件排序都能有效的利用到索引，性能迅速提升。 123456SELECT * FROM operation WHERE type = 'SQLStats' AND name = 'SlowLog' ORDER BY create_time LIMIT 1000, 10; 好吧，可能90%以上的 DBA 解决该问题就到此为止。但当 LIMIT 子句变成 “LIMIT 1000000,10” 时，程序员仍然会抱怨：我只取10条记录为什么还是慢？ 要知道数据库也并不知道第1000000条记录从什么地方开始，即使有索引也需要从头计算一次。出现这种性能问题，多数情形下是程序员偷懒了。 在前端数据浏览翻页，或者大数据分批导出等场景下，是可以将上一页的最大值当成参数作为查询条件的。SQL 重新设计如下： 123456SELECT * FROM operation WHERE type = 'SQLStats' AND name = 'SlowLog' AND create_time &gt; '2017-03-16 14:00:00' ORDER BY create_time limit 10; 在新设计下查询时间基本固定，不会随着数据量的增长而发生变化。 2、隐式转换SQL语句中查询变量和字段定义类型不匹配是另一个常见的错误。比如下面的语句： 123456mysql&gt; explain extended SELECT * &gt; FROM my_balance b &gt; WHERE b.bpn = 14000000123 &gt; AND b.isverified IS NULL ;mysql&gt; show warnings;| Warning | 1739 | Cannot use ref access on index 'bpn' due to type or collation conversion on field 'bpn' 其中字段 bpn 的定义为 varchar(20)，MySQL 的策略是将字符串转换为数字之后再比较。函数作用于表字段，索引失效。 上述情况可能是应用程序框架自动填入的参数，而不是程序员的原意。现在应用框架很多很繁杂，使用方便的同时也小心它可能给自己挖坑。 3、关联更新、删除虽然 MySQL5.6 引入了物化特性，但需要特别注意它目前仅仅针对查询语句的优化。对于更新或删除需要手工重写成 JOIN。 比如下面 UPDATE 语句，MySQL 实际执行的是循环/嵌套子查询（DEPENDENT SUBQUERY)，其执行时间可想而知。 1234567891011UPDATE operation o SET status = 'applying' WHERE o.id IN (SELECT id FROM (SELECT o.id, o.status FROM operation o WHERE o.group = 123 AND o.status NOT IN ( 'done' ) ORDER BY o.parent, o.id LIMIT 1) t); 执行计划： 1234567+----+--------------------+-------+-------+---------------+---------+---------+-------+------+-----------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+--------------------+-------+-------+---------------+---------+---------+-------+------+-----------------------------------------------------+| 1 | PRIMARY | o | index | | PRIMARY | 8 | | 24 | Using where; Using temporary || 2 | DEPENDENT SUBQUERY | | | | | | | | Impossible WHERE noticed after reading const tables || 3 | DERIVED | o | ref | idx_2,idx_5 | idx_5 | 8 | const | 1 | Using where; Using filesort |+----+--------------------+-------+-------+---------------+---------+---------+-------+------+-----------------------------------------------------+ 重写为 JOIN 之后，子查询的选择模式从 DEPENDENT SUBQUERY 变成 DERIVED，执行速度大大加快，从7秒降低到2毫秒。 1234567891011UPDATE operation o JOIN (SELECT o.id, o.status FROM operation o WHERE o.group = 123 AND o.status NOT IN ( 'done' ) ORDER BY o.parent, o.id LIMIT 1) t ON o.id = t.id SET status = 'applying' 执行计划简化为： 123456+----+-------------+-------+------+---------------+-------+---------+-------+------+-----------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+-------+---------+-------+------+-----------------------------------------------------+| 1 | PRIMARY | | | | | | | | Impossible WHERE noticed after reading const tables || 2 | DERIVED | o | ref | idx_2,idx_5 | idx_5 | 8 | const | 1 | Using where; Using filesort |+----+-------------+-------+------+---------------+-------+---------+-------+------+-----------------------------------------------------+ 4、混合排序MySQL 不能利用索引进行混合排序。但在某些场景，还是有机会使用特殊方法提升性能的。 123456SELECT * FROM my_order o INNER JOIN my_appraise a ON a.orderid = o.id ORDER BY a.is_reply ASC, a.appraise_time DESC LIMIT 0, 20 执行计划显示为全表扫描： 123456+----+-------------+-------+--------+-------------+---------+---------+---------------+---------+-+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra +----+-------------+-------+--------+-------------+---------+---------+---------------+---------+-+| 1 | SIMPLE | a | ALL | idx_orderid | NULL | NULL | NULL | 1967647 | Using filesort || 1 | SIMPLE | o | eq_ref | PRIMARY | PRIMARY | 122 | a.orderid | 1 | NULL |+----+-------------+-------+--------+---------+---------+---------+-----------------+---------+-+ 由于 is_reply 只有0和1两种状态，我们按照下面的方法重写后，执行时间从1.58秒降低到2毫秒。 12345678910111213141516171819SELECT * FROM ((SELECT * FROM my_order o INNER JOIN my_appraise a ON a.orderid = o.id AND is_reply = 0 ORDER BY appraise_time DESC LIMIT 0, 20) UNION ALL (SELECT * FROM my_order o INNER JOIN my_appraise a ON a.orderid = o.id AND is_reply = 1 ORDER BY appraise_time DESC LIMIT 0, 20)) t ORDER BY is_reply ASC, appraisetime DESC LIMIT 20; 5、EXISTS语句MySQL 对待 EXISTS 子句时，仍然采用嵌套子查询的执行方式。如下面的 SQL 语句： 1234567891011SELECT *FROM my_neighbor n LEFT JOIN my_neighbor_apply sra ON n.id = sra.neighbor_id AND sra.user_id = 'xxx' WHERE n.topic_status &lt; 4 AND EXISTS(SELECT 1 FROM message_info m WHERE n.id = m.neighbor_id AND m.inuser = 'xxx') AND n.topic_type &lt;&gt; 5 执行计划为： 1234567+----+--------------------+-------+------+-----+------------------------------------------+---------+-------+---------+ -----+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+--------------------+-------+------+ -----+------------------------------------------+---------+-------+---------+ -----+| 1 | PRIMARY | n | ALL | | NULL | NULL | NULL | 1086041 | Using where || 1 | PRIMARY | sra | ref | | idx_user_id | 123 | const | 1 | Using where || 2 | DEPENDENT SUBQUERY | m | ref | | idx_message_info | 122 | const | 1 | Using index condition; Using where |+----+--------------------+-------+------+ -----+------------------------------------------+---------+-------+---------+ -----+ 去掉 exists 更改为 join，能够避免嵌套子查询，将执行时间从1.93秒降低为1毫秒。 12345678910SELECT *FROM my_neighbor n INNER JOIN message_info m ON n.id = m.neighbor_id AND m.inuser = 'xxx' LEFT JOIN my_neighbor_apply sra ON n.id = sra.neighbor_id AND sra.user_id = 'xxx' WHERE n.topic_status &lt; 4 AND n.topic_type &lt;&gt; 5 新的执行计划： 1234567+----+-------------+-------+--------+ -----+------------------------------------------+---------+ -----+------+ -----+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+ -----+------------------------------------------+---------+ -----+------+ -----+| 1 | SIMPLE | m | ref | | idx_message_info | 122 | const | 1 | Using index condition || 1 | SIMPLE | n | eq_ref | | PRIMARY | 122 | ighbor_id | 1 | Using where || 1 | SIMPLE | sra | ref | | idx_user_id | 123 | const | 1 | Using where |+----+-------------+-------+--------+ -----+------------------------------------------+---------+ -----+------+ -----+ 6、条件下推外部查询条件不能够下推到复杂的视图或子查询的情况有： 聚合子查询； 含有 LIMIT 的子查询； UNION 或 UNION ALL 子查询； 输出字段中的子查询； 如下面的语句，从执行计划可以看出其条件作用于聚合子查询之后： 123456SELECT * FROM (SELECT target, Count(*) FROM operation GROUP BY target) t WHERE target = 'rm-xxxx' 123456+----+-------------+------------+-------+---------------+-------------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+---------------+-------------+---------+-------+------+-------------+| 1 | PRIMARY | &lt;derived2&gt; | ref | &lt;auto_key0&gt; | &lt;auto_key0&gt; | 514 | const | 2 | Using where || 2 | DERIVED | operation | index | idx_4 | idx_4 | 519 | NULL | 20 | Using index |+----+-------------+------------+-------+---------------+-------------+---------+-------+------+-------------+ 确定从语义上查询条件可以直接下推后，重写如下： 12345SELECT target, Count(*) FROM operation WHERE target = 'rm-xxxx' GROUP BY target 执行计划变为： 12345+----+-------------+-----------+------+---------------+-------+---------+-------+------+--------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+-------+---------+-------+------+--------------------+| 1 | SIMPLE | operation | ref | idx_4 | idx_4 | 514 | const | 1 | Using where; Using index |+----+-------------+-----------+------+---------------+-------+---------+-------+------+--------------------+ 关于 MySQL 外部条件不能下推的详细解释说明请参考文章： 7、提前缩小范围先上初始 SQL 语句： 12345678910SELECT * FROM my_order o LEFT JOIN my_userinfo u ON o.uid = u.uid LEFT JOIN my_productinfo p ON o.pid = p.pid WHERE ( o.display = 0 ) AND ( o.ostaus = 1 ) ORDER BY o.selltime DESC LIMIT 0, 15 该SQL语句原意是：先做一系列的左连接，然后排序取前15条记录。从执行计划也可以看出，最后一步估算排序记录数为90万，时间消耗为12秒。 1234567+----+-------------+-------+--------+---------------+---------+---------+-----------------+--------+----------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+---------------+---------+---------+-----------------+--------+----------------------------------------------------+| 1 | SIMPLE | o | ALL | NULL | NULL | NULL | NULL | 909119 | Using where; Using temporary; Using filesort || 1 | SIMPLE | u | eq_ref | PRIMARY | PRIMARY | 4 | o.uid | 1 | NULL || 1 | SIMPLE | p | ALL | PRIMARY | NULL | NULL | NULL | 6 | Using where; Using join buffer (Block Nested Loop) |+----+-------------+-------+--------+---------------+---------+---------+-----------------+--------+----------------------------------------------------+ 由于最后 WHERE 条件以及排序均针对最左主表，因此可以先对 my_order 排序提前缩小数据量再做左连接。SQL 重写后如下，执行时间缩小为1毫秒左右。 123456789101112131415SELECT * FROM (SELECT * FROM my_order o WHERE ( o.display = 0 ) AND ( o.ostaus = 1 ) ORDER BY o.selltime DESC LIMIT 0, 15) o LEFT JOIN my_userinfo u ON o.uid = u.uid LEFT JOIN my_productinfo p ON o.pid = p.pid ORDER BY o.selltime DESClimit 0, 15 再检查执行计划：子查询物化后（select_type=DERIVED)参与 JOIN。虽然估算行扫描仍然为90万，但是利用了索引以及 LIMIT 子句后，实际执行时间变得很小。 12345678+----+-------------+------------+--------+---------------+---------+---------+-------+--------+----------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+--------+---------------+---------+---------+-------+--------+----------------------------------------------------+| 1 | PRIMARY | &lt;derived2&gt; | ALL | NULL | NULL | NULL | NULL | 15 | Using temporary; Using filesort || 1 | PRIMARY | u | eq_ref | PRIMARY | PRIMARY | 4 | o.uid | 1 | NULL || 1 | PRIMARY | p | ALL | PRIMARY | NULL | NULL | NULL | 6 | Using where; Using join buffer (Block Nested Loop) || 2 | DERIVED | o | index | NULL | idx_1 | 5 | NULL | 909112 | Using where |+----+-------------+------------+--------+---------------+---------+---------+-------+--------+----------------------------------------------------+ 8、中间结果集下推再来看下面这个已经初步优化过的例子(左连接中的主表优先作用查询条件)： 1234567891011121314SELECT a.*, c.allocated FROM ( SELECT resourceid FROM my_distribute d WHERE isdelete = 0 AND cusmanagercode = '1234567' ORDER BY salecode limit 20) a LEFT JOIN ( SELECT resourcesid， sum(ifnull(allocation, 0) * 12345) allocated FROM my_resources GROUP BY resourcesid) c ON a.resourceid = c.resourcesid 那么该语句还存在其它问题吗？不难看出子查询 c 是全表聚合查询，在表数量特别大的情况下会导致整个语句的性能下降。 其实对于子查询 c，左连接最后结果集只关心能和主表 resourceid 能匹配的数据。因此我们可以重写语句如下，执行时间从原来的2秒下降到2毫秒。 123456789101112131415161718192021SELECT a.*, c.allocated FROM ( SELECT resourceid FROM my_distribute d WHERE isdelete = 0 AND cusmanagercode = '1234567' ORDER BY salecode limit 20) a LEFT JOIN ( SELECT resourcesid， sum(ifnull(allocation, 0) * 12345) allocated FROM my_resources r, ( SELECT resourceid FROM my_distribute d WHERE isdelete = 0 AND cusmanagercode = '1234567' ORDER BY salecode limit 20) a WHERE r.resourcesid = a.resourcesid GROUP BY resourcesid) c ON a.resourceid = c.resourcesid 但是子查询 a 在我们的SQL语句中出现了多次。这种写法不仅存在额外的开销，还使得整个语句显的繁杂。使用 WITH 语句再次重写： 123456789101112131415161718WITH a AS ( SELECT resourceid FROM my_distribute d WHERE isdelete = 0 AND cusmanagercode = '1234567' ORDER BY salecode limit 20)SELECT a.*, c.allocated FROM a LEFT JOIN ( SELECT resourcesid， sum(ifnull(allocation, 0) * 12345) allocated FROM my_resources r, a WHERE r.resourcesid = a.resourcesid GROUP BY resourcesid) c ON a.resourceid = c.resourcesid 总结数据库编译器产生执行计划，决定着SQL的实际执行方式。但是编译器只是尽力服务，所有数据库的编译器都不是尽善尽美的。 上述提到的多数场景，在其它数据库中也存在性能问题。了解数据库编译器的特性，才能避规其短处，写出高性能的SQL语句。 程序员在设计数据模型以及编写SQL语句时，要把算法的思想或意识带进来。 编写复杂SQL语句要养成使用 WITH 语句的习惯。简洁且思路清晰的SQL语句也能减小数据库的负担 。转自","link":"/2019/07/05/常见sql错误写法.html"},{"title":"mysql索引优化方案","text":"摘要mysql自带优化：先执行explain sql，在执行explain extended sql，得到优化结果，show warnings显示优化后的结果sql. 索引基数基数是数据列所包含的不同值的数量，例如，某个数据列包含值 1、3、7、4、7、3，那么它的基数就是 4。索引的基数相对于数据表行数较高（也就是说，列中包含很多不同的值，重复的值很少）的时候，它的工作效果最好。如果某数据列含有很多不同的年龄，索引会很快地分辨数据行；如果某个数据列用于记录性别（只有“M”和“F”两种值），那么索引的用处就不大；如果值出现的几率几乎相等，那么无论搜索哪个值都可能得到一半的数据行。在这些情况下，最好根本不要使用索引，因为查询优化器发现某个值出现在表的数据行中的百分比很高的时候，它一般会忽略索引，进行全表扫描。惯用的百分比界线是“30%”。 索引失效原因索引失效的原因有如下几点： 对索引列运算，运算包括（+、-、*、/、！、&lt;&gt;、%、like’%_’（% 放在前面）。 类型错误，如字段类型为 varchar，where 条件用 number。 对索引应用内部函数，这种情况下应该要建立基于函数的索引。例如 select * from template t where ROUND (t.logicdb_id) = 1，此时应该建 ROUND (t.logicdb_id) 为索引。 MySQL 8.0 开始支持函数索引，5.7 可以通过虚拟列的方式来支持，之前只能新建一个 ROUND (t.logicdb_id) 列然后去维护。 如果条件有 or，即使其中有条件带索引也不会使用（这也是为什么建议少使用 or 的原因），如果想使用 or，又想索引有效，只能将 or 条件中的每个列加上索引。 如果列类型是字符串，那一定要在条件中数据使用引号，否则不使用索引。 B-tree 索引 is null 不会走，is not null 会走，位图索引 is null，is not null 都会走。 组合索引遵循最左原则。 索引的建立索引的建立需要注意以下几点： 最重要的肯定是根据业务经常查询的语句。 尽量选择区分度高的列作为索引，区分度的公式是$$COUNT(DISTINCT空格col) / COUNT(*):表示字段不重复的比率，比率越大我们扫描的记录数就越少。$$ 如果业务中唯一特性最好建立唯一键，一方面可以保证数据的正确性，另一方面索引的效率能大大提高。 EXPLIAN 中有用的信息EXPLIAN 基本用法如下： desc 或者 explain 加上你的 SQL。 explain extended 加上你的 SQL，然后通过 show warnings 可以查看实际执行的语句，这一点也是非常有用的，很多时候不同的写法经 SQL 分析后，实际执行的代码是一样的。 提高性能的特性EXPLIAN 提高性能的特性如下： 索引覆盖(covering index)：需要查询的数据在索引上都可以查到不需要回表 EXTRA 列显示 using index。 ICP特性(Index Condition Pushdown)：本来 index 仅仅是 data access 的一种访问模式，存数引擎通过索引回表获取的数据会传递到 MySQL Server 层进行 where 条件过滤。 5.6 版本开始当 ICP 打开时，如果部分 where 条件能使用索引的字段，MySQL Server 会把这部分下推到引擎层，可以利用 index 过滤的 where 条件在存储引擎层进行数据过滤。 EXTRA 显示 using index condition。需要了解 MySQL 的架构图分为 Server 和存储引擎层。 索引合并(index merge)：对多个索引分别进行条件扫描，然后将它们各自的结果进行合并(intersect/union)。 一般用 or 会用到，如果是 AND 条件，考虑建立复合索引。EXPLAIN 显示的索引类型会显示 index_merge，EXTRA 会显示具体的合并算法和用到的索引。 Extra字段Extra 字段使用： using filesort：说明 MySQL 会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取。 MySQL 中无法利用索引完成的排序操作称为“文件排序”，其实不一定是文件排序，内部使用的是快排。 using temporary：使用了临时表保存中间结果，MySQL 在对查询结果排序时使用临时表。常见于排序 order by 和分组查询 group by。 using index：表示相应的 SELECT 操作中使用了覆盖索引（Covering Index），避免访问了表的数据行，效率不错。 impossible where：where 子句的值总是 false，不能用来获取任何元组。 select tables optimized away：在没有 group by 子句的情况下基于索引优化 MIN/MAX 操作或者对于 MyISAM 存储引擎优化 COUNT(*) 操作，不必等到执行阶段再进行计算，查询执行计划生成的阶段即完成优化。 distinct：优化 distinct 操作，在找到第一匹配的元组后即停止找同样值的操作。 using filesort、using temporary 这两项出现时需要注意下，这两项是十分耗费性能的 在使用 group by 的时候，虽然没有使用 order by，如果没有索引，是可能同时出现 using filesort，using temporary 的。因为 group by 就是先排序在分组，如果没有排序的需要，可以加上一个 order by NULL 来避免排序，这样 using filesort 就会去除，能提升一点性能。 type字段 system：表只有一行记录（等于系统表），这是 const 类型的特例，平时不会出现。 const：如果通过索引依次就找到了，const 用于比较主键索引或者 unique 索引。因为只能匹配一行数据，所以很快。如果将主键置于 where 列表中，MySQL 就能将该查询转换为一个常量。 eq_ref：唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配。常见于主键或唯一索引扫描。 ref：非唯一性索引扫描，返回匹配某个单独值的所有行。本质上也是一种索引访问，它返回所有匹配某个单独值的行，然而它可能会找到多个符合条件的行，所以它应该属于查找和扫描的混合体。 range：只检索给定范围的行，使用一个索引来选择行。key 列显示使用了哪个索引，一般就是在你的 where 语句中出现 between、&lt;、&gt;、in 等的查询。 这种范围扫描索引比全表扫描要好，因为只需要开始于缩印的某一点，而结束于另一点，不用扫描全部索引。 index：Full Index Scan ，index 与 ALL 的区别为 index 类型只遍历索引树，这通常比 ALL 快，因为索引文件通常比数据文件小。 也就是说虽然 ALL 和 index 都是读全表，但 index 是从索引中读取的，而 ALL 是从硬盘读取的。 all：Full Table Scan，遍历全表获得匹配的行。 字段类型和编码 MySQL 返回字符串长度 CHARACTER_LENGTH(同CHAR_LENGTH)方法返回的是字符数，LENGTH 函数返回的是字节数，一个汉字三个字节。 varchar 等字段建立索引长度计算语句 select count(distinct left(test,5))/count(*) from table；越趋近 1 越好。 MySQL 的 utf8 MySQL 的 utf8 最大是 3 个字节不支持 emoji 表情符号，必须只用 utf8mb4。需要在 MySQL 配置文件中配置客户端字符集为 utf8mb4。 JDBC 的连接串不支持配置 characterEncoding=utf8mb4，最好的办法是在连接池中指定初始化 SQL。例如：hikari 连接池，其他连接池类似 spring . datasource . hikari . connection - init - sql =set names utf8mb4。否则需要每次执行 SQL 前都先执行 set names utf8mb4。 MySQL 排序规则一般使用 _bin 和 _genera_ci： utf8_genera_ci 不区分大小写，ci 为 case insensitive 的缩写，即大小写不敏感。 utf8_general_cs 区分大小写，cs 为 case sensitive 的缩写，即大小写敏感，但是目前 MySQL 版本中已经不支持类似于 _genera_cs 的排序规则，直接使用 utf8_bin 替代。 utf8_bin 将字符串中的每一个字符用二进制数据存储，区分大小写。 那么，同样是区分大小写，utf8_general_cs 和 utf8_bin 有什么区别？ cs 为 case sensitive 的缩写，即大小写敏感；bin 的意思是二进制，也就是二进制编码比较。 utf8_general_cs 排序规则下，即便是区分了大小写，但是某些西欧的字符和拉丁字符是不区分的，比如 ä=a，但是有时并不需要 ä=a，所以才有 utf8_bin。 utf8_bin 的特点在于使用字符的二进制的编码进行运算，任何不同的二进制编码都是不同的，因此在 utf8_bin 排序规则下：ä&lt;&gt;a。 SQL语句总结常用但容易忘的 如果有主键或者唯一键冲突则不插入：insert ignore into。 如果有主键或者唯一键冲突则更新，注意这个会影响自增的增量：INSERT INTO room_remarks(room_id,room_remarks)VALUE(1,”sdf”) ON DUPLICATE KEY UPDATE room_remarks = “234”。 如果有就用新的替代，values 如果不包含自增列，自增列的值会变化：REPLACE INTO room_remarks(room_id,room_remarks) VALUE(1,”sdf”)。 备份表：CREATE TABLE user_info SELECT * FROM user_info。 复制表结构：CREATE TABLE user_v2 LIKE user。 从查询语句中导入：INSERT INTO user_v2 SELECT * FROM user 或者 INSERT INTO user_v2(id,num) SELECT id,num FROM user。 连表更新：UPDATE user a, room b SET a.num=a.num+1 WHERE a.room_id=b.id。 连表删除：DELETE user FROM user,black WHERE user.id=black.id。 锁相关锁相关(作为了解，很少用)： 共享锁：select id from tb_test where id = 1 lock in share mode。 排它锁：select id from tb_test where id = 1 for update。 优化时用到 强制使用某个索引：select * from table force index(idx_user) limit 2。 禁止使用某个索引：select * from table ignore index(idx_user) limit 2。 禁用缓存(在测试时去除缓存的影响)：select SQL_NO_CACHE from table limit 2。 查看状态 查看字符集：SHOW VARIABLES LIKE ‘character_set%’。 查看排序规则：SHOW VARIABLES LIKE ‘collation%’。 SQL编写注意 where 语句的解析顺序是从右到左，条件尽量放 where 不要放 having。 采用延迟关联(deferred join)技术优化超多分页场景，比如 limit 10000,10,延迟关联可以避免回表。 distinct 语句非常损耗性能，可以通过 group by 来优化。 连表尽量不要超过三个表。 踩坑 如果有自增列，truncate 语句会把自增列的基数重置为 0，有些场景用自增列作为业务上的 ID 需要十分重视。 聚合函数会自动滤空，比如 a 列的类型是 int 且全部是 NULL，则 SUM(a) 返回的是 NULL 而不是 0。 MySQL 判断 null 相等不能用 “a=null”，这个结果永远为 UnKnown，where 和 having 中，UnKnown 永远被视为 false，check 约束中，UnKnown 就会视为 true 来处理。所以要用“a is null”处理。 千万大表在线修改MySQL 在表数据量很大的时候，如果修改表结构会导致锁表，业务请求被阻塞。MySQL 在 5.6 之后引入了在线更新，但是在某些情况下还是会锁表，所以一般都采用 PT 工具( Percona Toolkit)。如对表添加索引： 123pt-online-schema-change --user='root' --host='localhost' --ask-pass --alter \"add index idx_user_id(room_id,create_time)\" D=fission_show_room_v2,t=room_favorite_info --execute 慢查询日志 有时候如果线上请求超时，应该去关注下慢查询日志，慢查询的分析很简单，先找到慢查询日志文件的位置，然后利用 mysqldumpslow 去分析。 查询慢查询日志信息可以直接通过执行 SQL 命令查看相关变量，常用的 SQL 如下： mysqldumpslow 的工具十分简单，我主要用到的参数如下： -t：限制输出的行数，我一般取前十条就够了。 -s：根据什么来排序默认是平均查询时间 at，我还经常用到 c 查询次数，因为查询次数很频繁但是时间不高也是有必要优化的，还有 t 查询时间，查看那个语句特别卡。 -v：输出详细信息。 例子：mysqldumpslow -v -s t -t 10 mysql_slow.log.2018-11-20-0500。 一些数据库性能的思考 在对公司慢查询日志做优化的时候，很多时候可能是忘了建索引，像这种问题很容易解决，加个索引就行了。但是有几种情况就不是简单加索引能解决了 业务代码循环读数据库 ​ 考虑这样一个场景，获取用户粉丝列表信息，加入分页是十个，其实像这样的 SQL 是十分简单的，通过连表查询性能也很高。 ​ 但是有时候，很多开发采用了取出一串 ID，然后循环读每个 ID 的信息，这样如果 ID 很多对数据库的压力是很大的，而且性能也很低。 统计 SQL ​ 很多时候，业务上都会有排行榜这种，发现公司有很多地方直接采用数据库做计算，在对一些大表做聚合运算的时候，经常超过五秒，这些 SQL 一般很长而且很难优化。像这种场景，如果业务允许（比如一致性要求不高或者是隔一段时间才统计的），可以专门在从库里面做统计。另外我建议还是采用 Redis 缓存来处理这种业务。 超大分页 ​ 在慢查询日志中发现了一些超大分页的慢查询如 Limit 40000，1000，因为 MySQL 的分页是在 Server 层做的，可以采用延迟关联在减少回表。但是看了相关的业务代码正常的业务逻辑是不会出现这样的请求的，所以很有可能是有恶意用户在刷接口，最好在开发的时候也对接口加上校验拦截这些恶意请求。","link":"/2019/07/03/mysql索引优化方案.html"},{"title":"一次数据库的死锁问题排查过程","text":"摘要某天晚上，同事正在发布，突然线上大量报警，很多是关于数据库死锁的，报警提示信息如下： 现象某天晚上，同事正在发布，突然线上大量报警，很多是关于数据库死锁的，报警提示信息如下： 123456789{\"errorCode\":\"SYSTEM_ERROR\",\"errorMsg\":\"nested exception is org.apache.ibatis.exceptions.PersistenceException: Error updating database. Cause: ERR-CODE: [TDDL-4614][ERR_EXECUTE_ON_MYSQL] Deadlock found when trying to get lock; The error occurred while setting parameters\\n### SQL: update fund_transfer_stream set gmt_modified=now(),state = ? where fund_transfer_order_no = ? and seller_id = ? and state = 'NEW' 通过报警，我们基本可以定位到发生死锁的数据库以及数据库表。先来介绍下本文案例中涉及到的数据库相关信息。 背景情况我们使用的数据库是Mysql 5.7，引擎是InnoDB，事务隔离级别是READ-COMMITED。 数据库版本查询方法： 1SELECT version(); 引擎查询方法： 1show create table fund_transfer_stream; 建表语句中会显示存储引擎信息，形如：ENGINE=InnoDB 事务隔离级别查询方法： 1select @@tx_isolation; 事务隔离级别设置方法（只对当前Session生效）： 1set session transaction isolation level read committed; PS：注意，如果数据库是分库的，以上几条SQL语句需要在单库上执行，不要在逻辑库执行。 发生死锁的表结构及索引情况（隐去了部分无关字段和索引）： 1234567891011121314CREATE TABLE `fund_transfer_stream` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键', `gmt_create` datetime NOT NULL COMMENT '创建时间', `gmt_modified` datetime NOT NULL COMMENT '修改时间', `pay_scene_name` varchar(256) NOT NULL COMMENT '支付场景名称', `pay_scene_version` varchar(256) DEFAULT NULL COMMENT '支付场景版本', `identifier` varchar(256) NOT NULL COMMENT '唯一性标识', `seller_id` varchar(64) NOT NULL COMMENT '卖家Id', `state` varchar(64) DEFAULT NULL COMMENT '状态', `fund_transfer_order_no` varchar(256) DEFAULT NULL COMMENT '资金平台返回的状态', PRIMARY KEY (`id`),UNIQUE KEY `uk_scene_identifier` (KEY `idx_seller` (`seller_id`), KEY `idx_seller_transNo` (`seller_id`,`fund_transfer_order_no`(20)) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='资金流水'; 该数据库共有三个索引，1个聚簇索引（主键索引），2个非聚簇索（非主键索引）引。 聚簇索引： 1PRIMARY KEY (`id`) 非聚簇索引： 123KEY `idx_seller` (`seller_id`),KEY `idx_seller_transNo` (`seller_id`,`fund_transfer_order_no`(20)) 以上两个索引，其实idx_seller_transNo已经覆盖到了idx_seller，由于历史原因，因为该表以seller_id分表，所以是先有的idx_seller，后有的idx_seller_transNo 死锁日志当数据库发生死锁时，可以通过以下命令获取死锁日志： 1show engine innodb status 发生死锁，第一时间查看死锁日志，得到死锁日志内容如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243Transactions deadlock detected, dumping detailed information.2019-03-19T21:44:23.516263+08:00 5877341 [Note] InnoDB: *** (1) TRANSACTION:TRANSACTION 173268495, ACTIVE 0 sec fetching rowsmysql tables in use 1, locked 1LOCK WAIT 304 lock struct(s), heap size 41168, 6 row lock(s), undo log entries 1MySQL thread id 5877358, OS thread handle 47356539049728, query id 557970181 11.183.244.150 fin_instant_app updatingupdate `fund_transfer_stream` set `gmt_modified` = NOW(), `state` = 'PROCESSING' where ((`state` = 'NEW') AND (`seller_id` = '38921111') AND (`fund_transfer_order_no` = '99010015000805619031958363857'))2019-03-19T21:44:23.516321+08:00 5877341 [Note] InnoDB: *** (1) HOLDS THE LOCK(S):RECORD LOCKS space id 173 page no 13726 n bits 248 index idx_seller_transNo of table `xxx`.`fund_transfer_stream` trx id 173268495 lock_mode X locks rec but not gapRecord lock, heap no 168 PHYSICAL RECORD: n_fields 3; compact format; info bits 02019-03-19T21:44:23.516565+08:00 5877341 [Note] InnoDB: *** (1) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 173 page no 12416 n bits 128 index PRIMARY of table `xxx`.`fund_transfer_stream` trx id 173268495 lock_mode X locks rec but not gap waitingRecord lock, heap no 56 PHYSICAL RECORD: n_fields 17; compact format; info bits 02019-03-19T21:44:23.517793+08:00 5877341 [Note] InnoDB: *** (2) TRANSACTION:TRANSACTION 173268500, ACTIVE 0 sec fetching rows, thread declared inside InnoDB 81mysql tables in use 1, locked 1302 lock struct(s), heap size 41168, 2 row lock(s), undo log entries 1MySQL thread id 5877341, OS thread handle 47362313119488, query id 557970189 11.131.81.107 fin_instant_app updatingupdate `fund_transfer_stream_0056` set `gmt_modified` = NOW(), `state` = 'PROCESSING' where ((`state` = 'NEW') AND (`seller_id` = '38921111') AND (`fund_transfer_order_no` = '99010015000805619031957477256'))2019-03-19T21:44:23.517855+08:00 5877341 [Note] InnoDB: *** (2) HOLDS THE LOCK(S):RECORD LOCKS space id 173 page no 12416 n bits 128 index PRIMARY of table `fin_instant_0003`.`fund_transfer_stream_0056` trx id 173268500 lock_mode X locks rec but not gapRecord lock, heap no 56 PHYSICAL RECORD: n_fields 17; compact format; info bits 02019-03-19T21:44:23.519053+08:00 5877341 [Note] InnoDB: *** (2) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 173 page no 13726 n bits 248 index idx_seller_transNo of table `fin_instant_0003`.`fund_transfer_stream_0056` trx id 173268500 lock_mode X locks rec but not gap waitingRecord lock, heap no 168 PHYSICAL RECORD: n_fields 3; compact format; info bits 02019-03-19T21:44:23.519297+08:00 5877341 [Note] InnoDB: *** WE ROLL BACK TRANSACTION (2) 简单解读一下死锁日志，可以得到以下信息： 1、导致死锁的两条SQL语句分别是： 123update `fund_transfer_stream_0056` set `gmt_modified` = NOW(), `state` = 'PROCESSING' where ((`state` = 'NEW') AND (`seller_id` = '38921111') AND (`fund_transfer_order_no` = '99010015000805619031957477256')) 和 123update `fund_transfer_stream_0056` set `gmt_modified` = NOW(), `state` = 'PROCESSING' where ((`state` = 'NEW') AND (`seller_id` = '38921111') AND (`fund_transfer_order_no` = '99010015000805619031958363857')) 2、事务1，持有索引idx_seller_transNo的锁，在等待获取PRIMARY的锁。 3、事务2，持有PRIMARY的锁，在等待获取idx_seller_transNo的锁。 4、因事务1和事务2之间发生循环等待，故发生死锁。 5、事务1和事务2当前持有的锁均为：lock_mode X locks rec but not gap 两个事务对记录加的都是X 锁，No Gap锁，即对当行记录加锁，并为加间隙锁。 X锁：排他锁、又称写锁。若事务T对数据对象A加上X锁，事务T可以读A也可以修改A，其他事务不能再对A加任何锁，直到T释放A上的锁。这保证了其他事务在T释放A上的锁之前不能再读取和修改A。 与之对应的是S锁：共享锁，又称读锁，若事务T对数据对象A加上S锁，则事务T可以读A但不能修改A，其他事务只能再对A加S锁，而不能加X锁，直到T释放A上的S锁。这保证了其他事务可以读A，但在T释放A上的S锁之前不能对A做任何修改。 Gap Lock：间隙锁，锁定一个范围，但不包括记录本身。GAP锁的目的，是为了防止同一事务的两次当前读，出现幻读的情况。 Next-Key Lock：1+2，锁定一个范围，并且锁定记录本身。对于行的查询，都是采用该方法，主要目的是解决幻读的问题。 详见：https://www.cnblogs.com/zhoujinyi/p/3435982.html 、 https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html 问题排查根据我们目前已知的数据库相关信息，以及死锁的日志，我们基本可以做一些简单的判定。 首先，此次死锁一定是和Gap锁以及Next-Key Lock没有关系的。因为我们的数据库隔离级别是RC（READ-COMMITED）的，这种隔离级别是不会添加Gap锁的。前面的死锁日志也提到这一点。 然后，就要翻代码了，看看我们的代码中事务到底是怎么做的。核心代码及SQL如下： 12345@Transactional(rollbackFor = Exception.class)public int doProcessing(String sellerId, Long id, String fundTransferOrderNo) { fundTreansferStreamDAO.updateFundStreamId(sellerId, id, fundTransferOrderNo); return fundTreansferStreamDAO.updateStatus(sellerId, fundTransferOrderNo, FundTransferStreamState.PROCESSING.name());} 该代码的目的是先后修改同一条记录的两个不同字段，updateFundStreamId SQL： 123update fund_transfer_stream set gmt_modified=now(),fund_transfer_order_no = #{fundTransferOrderNo} where id = #{id} and seller_id = #{sellerId} updateStatus SQL： 1234update fund_transfer_stream set gmt_modified=now(),state = #{state} where fund_transfer_order_no = #{fundTransferOrderNo} and seller_id = #{sellerId} and state = 'NEW' 可以看到，我们的同一个事务中执行了两条Update语句，这里分别查看下两条SQL的执行计划： updateFundStreamId执行的时候使用到的是PRIMARY索引。 updateStatus执行的时候使用到的是idx_seller_transNo索引。 通过执行计划，我们发现updateStatus其实是有两个索引可以用的，执行的时候真正使用的是idx_seller_transNo索引。这是因为MySQL查询优化器是基于代价（cost-based）的查询方式。因此，在查询过程中，最重要的一部分是根据查询的SQL语句，依据多种索引，计算查询需要的代价，从而选择最优的索引方式生成查询计划。 我们查询执行计划是在死锁发生之后做的，事后查询的执行计划和发绳死锁那一刻的索引使用情况并不一定相同的。但是，我们结合死锁日志，也可以定位到以上两条SQL语句执行的时候使用到的索引。即updateFundStreamId执行的时候使用到的是PRIMARY索引，updateStatus执行的时候使用到的是idx_seller_transNo索引。 有了以上这些已知信息，我们就可以开始排查死锁原因及其背后的原理了。通过分析死锁日志，再结合我们的代码以及数据库建表语句，我们发现主要问题出在我们的idx_seller_transNo索引上面： 1KEY `idx_seller_transNo` (`seller_id`,`fund_transfer_order_no`(20)) 索引创建语句中，我们使用了前缀索引，为了节约索引空间，提高索引效率，我们只选择了fund_transfer_order_no字段的前20位作为索引值。 因为fund_transfer_order_no只是普通索引，而非唯一性索引。又因为在一种特殊情况下，会有同一个用户的两个fund_transfer_order_no的前20位相同，这就导致两条不同的记录的索引值一样（因为seller_id 和fund_transfer_order_no(20)都相同 ）。 就如本文中的例子，发生死锁的两条记录的fund_transfer_order_no字段的值：99010015000805619031958363857和99010015000805619031957477256这两个就是前20位相同的。 那么为什么fund_transfer_order_no的前20位相同会导致死锁呢？ 加锁原理我们就拿本次的案例来看一下MySql数据库加锁的原理是怎样的，本文的死锁背后又发生了什么。 我们在数据库上模拟死锁场景，执行顺序如下： 事务1 事务2 执行结果 begin update fund_transfer_stream set gmt_modified=now(),fund_transfer_order_no = ‘99010015000805619031958363857’ where id = 1 and seller_id = 3111095611; 执行成功 begin update fund_transfer_stream set gmt_modified=now(),fund_transfer_order_no = ‘99010015000805619031957477256’ where id = 2 and seller_id = 3111095611; 执行成功 update fund_transfer_stream set gmt_modified = NOW(), state = ‘PROCESSING’ where ((state = ‘NEW’) AND (seller_id = ‘3111095611’) AND (fund_transfer_order_no = ‘99010015000805619031958363857’)); 阻塞 update fund_transfer_stream set gmt_modified = NOW(), state = ‘PROCESSING’ where ((state = ‘NEW’) AND (seller_id = ‘3111095611’) AND (fund_transfer_order_no = ‘99010015000805619031957477256’)); 死锁 我们知道，在MySQL中，行级锁并不是直接锁记录，而是锁索引。索引分为主键索引和非主键索引两种，如果一条sql语句操作了主键索引，MySQL就会锁定这条主键索引；如果一条语句操作了非主键索引，MySQL会先锁定该非主键索引，再锁定相关的主键索引。 主键索引的叶子节点存的是整行数据。在InnoDB中，主键索引也被称为聚簇索引（clustered index） 非主键索引的叶子节点的内容是主键的值，在InnoDB中，非主键索引也被称为非聚簇索引（secondary index） 所以，本文的示例中涉及到的索引结构（索引是B+树，简化成表格了）如图： 死锁的发生与否，并不在于事务中有多少条SQL语句，死锁的关键在于：两个(或以上)的Session加锁的顺序不一致。那么接下来就看下上面的例子中两个事务的加锁顺序是怎样的： 下图是分解图，每一条SQL执行的时候加锁情况： 结合以上两张图，我们发现了导致死锁的原因： 事务1执行update1占用PRIMARY = 1的锁 ——&gt; 事务2执行update1 占有PRIMARY = 2的锁； 事务1执行update2占有idx_seller_transNo = (3111095611，99010015000805619031)的锁，尝试占有PRIMARY = 2锁失败（阻塞）； 事务2执行update2尝试占有idx_seller_transNo = (3111095611，99010015000805619031)的锁失败（死锁）； 事务在以非主键索引为where条件进行Update的时候，会先对该非主键索引加锁，然后再查询该非主键索引对应的主键索引都有哪些，再对这些主键索引进行加锁。） 解决方法至此，我们分析清楚了导致死锁的根本原理以及其背后的原理。那么这个问题解决起来就不难了。 可以从两方面入手，分别是修改索引和修改代码（包含SQL语句）。 修改索引：只要我们把前缀索引 idx_seller_transNo中fund_transfer_order_no的前缀长度修改下就可以了。比如改成50。即可避免死锁。 但是，改了idx_seller_transNo的前缀长度后，可以解决死锁的前提条件是update语句真正执行的时候，会用到fund_transfer_order_no索引。如果MySQL查询优化器在代价分析之后，决定使用索引 KEY idx_seller(seller_id)，那么还是会存在死锁问题。原理和本文类似。 所以，根本解决办法就是改代码： 12* 所有update都通过主键ID进行。* 在同一个事务中，避免出现多条update语句修改同一条记录。 总结与思考在死锁发生之后的一周内，我几乎每天都会抽空研究一会，问题早早的就定位到了，修改方案也有了，但是其中原理一直没搞清楚。 前前后后做过很多中种推断及假设，又都被自己一次次推翻。最终还是要靠实践来验证自己的想法。于是我自己在本地安装了数据库，实战的做了些测试，并实时查看数据库锁情况。show engine innodb status ;可以查看锁情况。最终才搞清楚原理。 简单说几点思考： 1、遇到问题，不要猜！！！亲手复现下问题，然后再来分析。 2、不要忽略上下文！！！我刚开始就是只关注死锁日志，一直忽略了代码中的事务其实还执行了另外一条SQL语句（updateFundStreamId）。 3、理论知识再充足，关键时刻不一定想的起来！！！ 4、坑都是自己埋的！！！ 参考资料：MySQL 加锁处理分析 innodb 事务隔离级别 《MySql实战45讲》 MySQL中的行级锁,表级锁,页级锁 查看原文","link":"/2019/06/25/一次数据库的死锁问题排查过程.html"},{"title":"博客图片上传picgo工具github图传使用","text":"摘要对于每一个写博客的人来说，图片是至关重要。这一路经历了多次图片的烦恼，之前选择了微博个人文章那里粘贴图片的方式上传，感觉也挺方便的。但是由于新浪的图片显示问题，如果header中不设置 标签就不能异步访问图片，导致图裂，那之恶心。然而设置之后又与网站访客统计的插件冲突，使之不能统计，真是神仙打架。无赖之下使用了PicGo工具，使用后感觉真XX方便！ PicGo工具下载安装配置下载 .PicGo下载 github网站提供三个版本的下载，MacOs、linux、windows覆盖市面上90%系统，还是很给力了。 我是mac用户，直接使用brew cask来安装PicGo: brew cask install picgo，简直方便到爆。 配置 PicGo配置(使用github图传，免费方便，同时配合github.io博客真是方便) 选上必填的就ok,一开始不知道token的设置，附赠token获取方法 图片上传相关的设置 链接格式：选择适合自己的，一般用户md文件中，选第一个，然后就可以疯狂使用了。 使用github图传，获取token在github-&gt;setting-&gt;developer settings 选择generate new token 勾选好之后生成就好了 使用 PicGo使用，简直方便 1).默认网页上直接右键复制图片 2).点击等待中的图片，开始上传 3).上传完之后有个提示，同时粘贴板也会自动粘贴上 4).直接粘贴到想要的地方 或者也可以直接截图，然后点击图片里的图片上传，很方便 PicGo上传动图gif 如果直接复制网页上的动图，去上传的话是截取的某帧，是静图。应该下载到本地，然后在拖进去上传就可以了。","link":"/2019/06/20/博客图片上传picgo工具github图传使用.html"},{"title":"阿里一年的成长经历","text":"摘要任何工作一定对个人都是有提升的，但是不会总结的人，在每个项目/需求中成长的东西都是散的，久而久之就忘了。通过充分的总结之后，犯过的错误我们不会二次再犯，理清楚的业务的来龙去脉铭记在心，对自己是一种提升，分享给别人对别人也是很大的帮助。失败者失败的原因各有不同，成功者的做事方式总是相似的，从宏观角度去看，我认为总结就是成功者之所以能成功，很重要一个原因。 应当如何面对线上的异常/故障看起来毫无意义的一个问题，碰到线上异常/故障如何面对，排查解决了不就好了，但是这真的只是第一层 最近在想“消防”这个词语很有意思，它其实是两层意思： “消”是消除问题 “防”是防止问题 即“消防”这个词语表达的意思应该是先消除问题再防止相同的问题再次发生。其实线上的异常/故障也是同样的道理，我们应当先及时止血，把问题处理掉，然后深挖问题，探究根因，举几个例子： 假设是某段代码的空指针异常导致的，那么是否考虑加强Code Review，或者使用findbugs插件去自动扫描代码中可能的异常？ 假设是线上某个配置修改导致的，那么是否今后变更的修改必须有人双重检查一遍才可以修改？ 假设是本地内存中某些值因为系统重启丢失导致的，那么是否引入定时任务，定时把值写入本地内存中？ 假设是某段代码逻辑没测试到导致的，那么是否可以反思总结为什么这段逻辑没有测试到，未来的测试应该如何改进？ 根据我过往的经验，太多公司、太多团队处理线上的问题仅仅满足于把问题处理完就完事，忽略了对问题的复盘，这对团队/对公司的发展都是不利的。 什么是真正的技术能力之前加了几个技术微信群，看到很多技术朋友在兴高采烈地讨论各种源码，spring源码我彻底撸了一遍、最近深入学习了dubbo底层实现方式，当然曾经的我也是这样的，记得学习volatile的时候一直挖到了volatile在硬件层面上的实现方式，但是这真的说明技术能力强吗？从今天的思考去看这个问题，我认为这更多反应的是一个人的学习能力、钻研能力以及对技术的热情，除此之外再体现不出太多其他东西了。 这个话题，可能是这一年思考的最多个的一个点，钻研是好事，但是实际上大多时候的深入钻研并不在实际工作中有用，且研究得越深，忘得越快，因为研究得越深，那么这个技术点关联的技术点就越多，边边角角的忘了，核心的东西不容易串起来。那么什么是真正的技术能力，我画一张图概括一下： 技术能力=解决问题的能力(解决当下问题+解决长远问题) 简而言之，技术能力 = 解决问题的能力，那么同样都在解决问题，大家之间的技术高低又有什么区分呢？我认为有以下几个层次： 第一层级，解决当下问题 第二层级，以优雅且可复用的方式解决当下问题 第三层级，解决的问题不仅仅能满足当下，还能满足未来一段时间 其实从这个角度上来看，不同的技术能力，在工作过程中区分度是很明显的： 写的代码是否存在异常风险，多线程运行下是否存在线程安全问题，某段代码是否会导致内存泄露 写的代码是否优雅可复用，设计的框架是否足够符合开闭原则，代码结构层次是否清晰明了 针对特定的场景，技术选型、库表结构设计是否足够合理，今天你设计的框架是只能用一年，还是未来三年五年都可以持续使用 来了一个大的需求，就比如做一个App的会员体系功能好了，是否可以在充分分析需求后，精确将需求划分为几个特定的子模块并梳理清楚模块之间的关系 越厉害的人，在代码设计与开发过程中，越能看到想到一些别人看不到想不到的问题，这叫做高屋建瓴；当代码运行出现问题的时候，有人1小时排查出问题，有人1分钟发现问题，这叫做举重若轻。 因此我认为解决问题的能力才是技术能力的真正体现，这一年对技术的探究我也从研究源码更多的转变去学习设计模式、去学习分布式环境下各种NoSql的选型对比、去学习使用Lambda让代码更简洁，往真正在实际工作中解决问题的方向去努力。 另外，抛开这个点，这两天我在思考，还有一个体现技术能力的点，就是学习能力。现实中的全栈是很少的，互联网这个行业的程序员的方向通常有几类： 服务端 前端 移动端 AI 嵌入式 大数据 在同一类中，基础知识、基本概念、思维方向是一致的，更多可能差异在开发工具、语言上，我精通Java，但是如果明天有一个需求，使用nodejs、scala、go更好，那么是否可以快速学习、快速上手？甚至明天有一个需求需要写前端代码，是否可以快速开发、无bug上线？ 所以，解决问题的能力 + 学习能力，是我认为真正的技术能力，不过说到底，学习能力某种程度上也只是为了解决问题而已。 不要造轮子曾几何时，当我们看着github上这么多优秀的源代码的时候，默默立誓，这辈子我一定要写出一个牛逼的框架，开源在网上。 曾几何时，公司招聘的时候，技术负责人激情满满地介绍着公司内部自研了多少系统并在线上投入使用。 很多对技术有追求的朋友，进入一家公司可能时时刻刻在寻找机会去做一些自己造轮子的事情，但是就如同前面所说的，衡量真正好技术的标准就是能否实实在在地解决问题，自己造轮子风险高、周期长，且需要长时间的验证、排坑才能达到比较好的效果。 随便举几个例子，在互联网发展的今天： 数据库连接池有dbcp、c3p0、druid 本地缓存有ehcache、要用中心缓存有redis、tail 服务化有dubbo、跨语言可以用thrift 分布式任务调度可以考虑schedulex 搜索可以选es、solr 更高级一点图片存储可以用七牛、im可以用融云/环信、音视频这块声网做得比较成熟，所有这些都提供了各个开发版本的sdk，接入简单 只要你有的技术方面的需求，绝大多数业界已经有了成熟的解决方案了，根本不需要去专门自己搞一套。因此我认为轻易一定不要造轮子，如果一定要造轮子，那么请想清楚下面几个问题： 你要做的事情是否当前已经有了类似解决方案？ 如果有，那么你自己做的这一套东西和类似解决方案的差异点在哪里？假设不用你这套，基于已有的解决方案稍加改造是否就能达到目的？ 如果没有，那么为什么之前没有？是你们公司这种场景是独一无二的？还是这种场景对应的解决方案根本就是不可行的所以之前没人去搞？ 如果想清楚了这些问题，那么就去干吧。 去提升看问题的高度过去有太多人在我的公众号或者博客下反馈了一个问题：在这个公司，整天做着增删改查的工作，对自己一点都没有提高。 对于这种看法，说难听点就是四个字—-目光短浅。我们看：如果以普通的视角去看，那么一颗树那也就只是一棵树而已，但是如果跳脱出目前的视角，站在更高的角度去看，它其实是森林的一部分。你的主管并不是因为他是你的主管所以他就应该你比更高瞻远瞩，而是因为他看问题的高度比你更高、想得更远、做得更深，所以才成为了你的主管。 把这个问题说得实际点： 假设今天你负责的是一个系统，那么你仅仅是把这个系统的基本原理搞懂了？还是可以把上下游有几个系统、每个系统之间如何调用、依赖方式都理顺？ 假设今天你负责的是一块业务，那么你仅仅把自己负责的功能点弄清楚了？还是你可以从最上游开始，到你负责的系统，再到最下游，都思考得非常透彻？ 今天与其在抱怨没有机会、抱怨公司对自己能力没有提升，为什么不去思考机会为什么降临在别人头上不降临在你头上？为什么别人可以从小公司写着一样的增删改查走向BAT而你年复一年还在小公司写着增删改查？当你真正能转变自己的思维模式，跳脱出现在的圈子往更高一个层次去看问题、去提升自己，我相信总会有发光发热的一天的。 同样在阿里巴巴，马老师思考自然、思考环保、思考人类的发展，你的主管思考团队未来的方向和打法，我们在思考如何把某个客户需求完整落地，这就是高度，你未必能想到马老师想的，但是你对标层级高一点的人，一步一步尝试往他们的高度去靠。 总而言之：眼界决定高度，多看、多想、多保持好奇心、多问几个为什么，久而久之自然就迈上了一个新的台阶。 学会总结需求、项目的复盘是非常重要的一部分内容，然而我之前见过的太多团队、太多Leader，只顾着一个迭代接着一个迭代，一个版本接着一个版本，只满足于把需求做好，而忽略了总结的重要性。 我认为大到项目、小到需求，如果在完成之后缺乏总结那么某种程度上来说是失败的，可以总结的点非常多： 通过这个项目/需求，是否吃透了某一块业务，搞懂了来龙去脉 通过这个项目/需求，是否充分理解了公司某个技术框架/基础组件的用法 在整个项目的设计上，有哪些做的不好的地方 在整个项目的开发（针对程序员而言），是否踩了坑，犯了低级的错误 在整个项目的进度把控上、人员安排上、上下游协调上，是否存在不足之处 经历了某次大促的值班，是否对可以熟练使用公司的监控工具，遇到突发事件，是否快速有效地进行了解决 任何工作一定对个人都是有提升的，但是不会总结的人，在每个项目/需求中成长的东西都是散的，久而久之就忘了。通过充分的总结之后，犯过的错误我们不会二次再犯，理清楚的业务的来龙去脉铭记在心，对自己是一种提升，分享给别人对别人也是很大的帮助。 失败者失败的原因各有不同，成功者的做事方式总是相似的，从宏观角度去看，我认为总结就是成功者之所以能成功，很重要一个原因。 参考资料","link":"/2019/06/13/阿里一年的成长经历.html"},{"title":"mysql数据库索引解析","text":"摘要看了很多关于索引的博客，讲的大同小异。但是始终没有让我明白关于索引的一些概念，如B-Tree索引，Hash索引，唯一索引….或许有很多人和我一样，没搞清楚概念就开始研究B-Tree，B+Tree等结构，导致在面试的时候答非所问！ 索引是什么?索引是帮助MySQL高效获取数据的数据结构。 索引能干什么?索引非常关键，尤其是当表中的数据量越来越大时，索引对于性能的影响愈发重要。 索引能够轻易将查询性能提高好几个数量级，总的来说就是可以明显的提高查询效率。 索引的分类? 从存储结构上来划分：BTree索引（B-Tree或B+Tree索引），Hash索引，full-index全文索引，R-Tree索引。这里所描述的是索引存储时保存的形式， 从应用层次来分：普通索引，唯一索引，复合索引 根据中数据的物理顺序与键值的逻辑（索引）顺序关系：聚集索引，非聚集索引。 平时讲的索引类型一般是指在应用层次的划分。就像手机分类：安卓手机，IOS手机 与 华为手机，苹果手机，OPPO手机一样。 普通索引**：**即一个索引只包含单个列，一个表可以有多个单列索引 唯一索引：索引列的值必须唯一，但允许有空值 复合索引：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并 聚簇索引(聚集索引)：并不是一种单独的索引类型，而是一种数据存储方式。具体细节取决于不同的实现，InnoDB的聚簇索引其实就是在同一个结构中保存了B-Tree索引(技术上来说是B+Tree)和数据行。 非聚簇索引：不是聚簇索引，就是非聚簇索引 索引的底层实现mysql默认存储引擎innodb只显式支持B-Tree( 从技术上来说是B+Tree)索引，对于频繁访问的表，innodb会透明建立自适应hash索引，即在B树索引基础上建立hash索引，可以显著提高查找效率，对于客户端是透明的，不可控制的，隐式的。 不谈存储引擎，只讨论实现(抽象) Hash索引基于哈希表实现，只有精确匹配索引所有列的查询才有效，对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码（hash code），并且Hash索引将所有的哈希码存储在索引中，同时在索引表中保存指向每个数据行的指针。 B-Tree索引（MySQL使用B+Tree）B-Tree能加快数据的访问速度，因为存储引擎不再需要进行全表扫描来获取数据，数据分布在各个节点之中。 B+Tree索引 是B-Tree的改进版本，同时也是数据库索引索引所采用的存储结构。数据都在叶子节点上，并且增加了顺序访问指针，每个叶子节点都指向相邻的叶子节点的地址。相比B-Tree来说，进行范围查找时只需要查找两个节点，进行遍历即可。而B-Tree需要获取所有节点，相比之下B+Tree效率更高。 结合存储引擎来讨论（一般默认使用B+Tree） 案例：假设有一张学生表，id为主键 id name birthday 1 Tom 1996-01-01 2 Jann 1996-01-04 3 Ray 1996-01-08 4 Michael 1996-01-10 5 Jack 1996-01-13 6 Steven 1996-01-23 7 Lily 1996-01-25 在MyISAM引擎中的实现（二级索引也是这样实现的） 在InnoDB中的实现 为什么索引结构默认使用B+Tree，而不是Hash，二叉树，红黑树？B+tree：因为B树不管叶子节点还是非叶子节点，都会保存数据，这样导致在非叶子节点中能保存的指针数量变少（有些资料也称为扇出），指针少的情况下要保存大量数据，只能增加树的高度，导致IO操作变多，查询性能变低； Hash：虽然可以快速定位，但是没有顺序，IO复杂度高。 二叉树：树的高度不均匀，不能自平衡，查找效率跟数据有关（树的高度），并且IO代价高。 红黑树：树的高度随着数据量增加而增加，IO代价高。 红黑树: 每个节点或者是黑色，或者是红色。 根节点是黑色。 每个叶子节点是黑色。 [注意：这里叶子节点，是指为空的叶子节点！] 如果一个节点是红色的，则它的子节点必须是黑色的。 从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。 为什么官方建议使用自增长主键作为索引？结合B+Tree的特点，自增主键是连续的，在插入过程中尽量减少页分裂，即使要进行页分裂，也只会分裂很少一部分。并且能减少数据的移动，每次插入都是插入到最后。总之就是减少分裂和移动的频率。 插入连续的数据： 插入非连续的数据 简单总结下 MySQL使用B+Tree作为索引数据结构。 B+Tree在新增数据时，会根据索引指定列的值对旧的B+Tree做调整。 从物理存储结构上说，B-Tree和B+Tree都以页(4K)来划分节点的大小，但是由于B+Tree中中间节点不存储数据，因此B+Tree能够在同样大小的节点中，存储更多的key，提高查找效率。 影响MySQL查找性能的主要还是磁盘IO次数，大部分是磁头移动到指定磁道的时间花费。 MyISAM存储引擎下索引和数据存储是分离的，InnoDB索引和数据存储在一起。 InnoDB存储引擎下索引的实现，(辅助索引)全部是依赖于主索引建立的(辅助索引中叶子结点存储的并不是数据的地址，还是主索引的值，因此，所有依赖于辅助索引的都是先根据辅助索引查到主索引，再根据主索引查数据的地址)。 由于InnoDB索引的特性，因此如果主索引不是自增的(id作主键)，那么每次插入新的数据，都很可能对B+Tree的主索引进行重整，影响性能。因此，尽量以自增id作为InnoDB的主索引。 InnoDB一棵B+树能存多少行数据？为什么要用B+树？而不是其他树？InnoDB一棵B+树可以存放多少行数据？这个问题的简单回答是：约2千万。为什么是这么多呢？因为这是可以算出来的，要搞清楚这个问题，我们先从InnoDB索引数据结构、数据组织方式说起。 我们都知道计算机在存储数据的时候，有最小存储单元，这就好比我们今天进行现金的流通最小单位是一毛。在计算机中磁盘存储数据最小单元是扇区，一个扇区的大小是512字节，而文件系统（例如XFS/EXT4）他的最小单元是块，一个块的大小是4k，而对于我们的InnoDB存储引擎也有自己的最小储存单元——页（Page），一个页的大小是16K。 innodb的所有数据文件（后缀为ibd的文件），他的大小始终都是16384（16k）的整数倍。 磁盘扇区、文件系统、InnoDB存储引擎都有各自的最小存储单元。 在MySQL中我们的InnoDB页的大小默认是16k，当然也可以通过参数设置： 12345678mysql&gt; show variables like &apos;innodb_page_size&apos;;+------------------+-------+| Variable_name| Value|+------------------+-------+| innodb_page_size | 16384|+------------------+-------+1 row in set(0.00sec) 数据表中的数据都是存储在页中的，所以一个页中能存储多少行数据呢？假设一行数据的大小是1k，那么一个页可以存放16行这样的数据。 如果数据库只按这样的方式存储，那么如何查找数据就成为一个问题，因为我们不知道要查找的数据存在哪个页中，也不可能把所有的页遍历一遍，那样太慢了。所以人们想了一个办法，用B+树的方式组织这些数据。如图所示： 我们先将数据记录按主键进行排序，分别存放在不同的页中（为了便于理解我们这里一个页中只存放3条记录，实际情况可以存放很多），除了存放数据的页以外，还有存放键值+指针的页，如图中page number=3的页，该页存放键值和指向数据页的指针，这样的页由N个键值+指针组成。当然它也是排好序的。这样的数据组织形式，我们称为索引组织表。现在来看下，要查找一条数据，怎么查？ 如select * from user where id=5; 这里id是主键,我们通过这棵B+树来查找，首先找到根页，你怎么知道user表的根页在哪呢？其实每张表的根页位置在表空间文件中是固定的，即page number=3的页（这点我们下文还会进一步证明），找到根页后通过二分查找法，定位到id=5的数据应该在指针P5指向的页中，那么进一步去page number=5的页中查找，同样通过二分查询法即可找到id=5的记录： | 5 | zhao2 | 27 | 现在我们清楚了InnoDB中主键索引B+树是如何组织数据、查询数据的，我们总结一下： 1、InnoDB存储引擎的最小存储单元是页，页可以用于存放数据也可以用于存放键值+指针，在B+树中叶子节点存放数据，非叶子节点存放键值+指针。 2、索引组织表通过非叶子节点的二分查找法以及指针确定数据在哪个页中，进而在去数据页中查找到需要的数据； 那么回到我们开始的问题，通常一棵B+树可以存放多少行数据？这里我们先假设B+树高为2，即存在一个根节点和若干个叶子节点，那么这棵B+树的存放总记录数为：根节点指针数*单个叶子节点记录行数。 上文我们已经说明单个叶子节点（页）中的记录数=16K/1K=16。（这里假设一行记录的数据大小为1k，实际上现在很多互联网业务数据记录大小通常就是1K左右）。 那么现在我们需要计算出非叶子节点能存放多少指针？ 其实这也很好算，我们假设主键ID为bigint类型，长度为8字节，而指针大小在InnoDB源码中设置为6字节，这样一共14字节，我们一个页中能存放多少这样的单元，其实就代表有多少指针，即16384/14=1170。那么可以算出一棵高度为2的B+树，能存放1170*16=18720条这样的数据记录。 根据同样的原理我们可以算出一个高度为3的B+树可以存放：1170*1170*16=21902400条这样的记录。 所以在InnoDB中B+树高度一般为1-3层，它就能满足千万级的数据存储。在查找数据时一次页的查找代表一次IO，所以通过主键索引查询通常只需要1-3次IO操作即可查找到数据。 怎么得到InnoDB主键索引B+树的高度？上面我们通过推断得出B+树的高度通常是1-3，下面我们从另外一个侧面证明这个结论。在InnoDB的表空间文件中，约定page number为3的代表主键索引的根页，而在根页偏移量为64的地方存放了该B+树的page level。如果page level为1，树高为2，page level为2，则树高为3。即B+树的高度=page level+1；下面我们将从实际环境中尝试找到这个page level。 在实际操作之前，你可以通过InnoDB元数据表确认主键索引根页的page number为3，你也可以从《InnoDB存储引擎》这本书中得到确认。 1234567SELECTb.name, a.name, index_id, type, a.space, a.PAGE_NOFROMinformation_schema.INNODB_SYS_INDEXES a,information_schema.INNODB_SYS_TABLES bWHEREa.table_id = b.table_id AND a.space &lt;&gt; 0; 执行结果： 可以看出数据库dbt3下的customer表、lineitem表主键索引根页的page number均为3，而其他的二级索引page number为4。关于二级索引与主键索引的区别请参考MySQL相关书籍，本文不在此介绍。 下面我们对数据库表空间文件做想相关的解析： 因为主键索引B+树的根页在整个表空间文件中的第3个页开始，所以可以算出它在文件中的偏移量：16384*3=49152（16384为页大小）。 另外根据《InnoDB存储引擎》中描述在根页的64偏移量位置前2个字节，保存了page level的值，因此我们想要的page level的值在整个文件中的偏移量为：16384*3+64=49152+64=49216，前2个字节中。 接下来我们用hexdump工具，查看表空间文件指定偏移量上的数据： linetem表的page level为2，B+树高度为page level+1=3；**region表的page level为0，B+树高度为page level+1=1；**customer表的page level为2，B+树高度为page level+1=3； 这三张表的数据量如下： 小结lineitem表的数据行数为600多万，B+树高度为3，customer表数据行数只有15万，B+树高度也为3。可以看出尽管数据量差异较大，这两个表树的高度都是3，换句话说这两个表通过索引查询效率并没有太大差异，因为都只需要做3次IO。那么如果有一张表行数是一千万，那么他的B+树高度依旧是3，查询效率仍然不会相差太大。 region表只有5行数据，当然他的B+树高度为1。 最后回顾一道面试题有一道MySQL的面试题，为什么MySQL的索引要使用B+树而不是其它树形结构？比如B树？ 现在这个问题的复杂版本可以参考本文； 他的简单版本回答是： 因为B树不管叶子节点还是非叶子节点，都会保存数据，这样导致在非叶子节点中能保存的指针数量变少（有些资料也称为扇出），指针少的情况下要保存大量数据，只能增加树的高度，导致IO操作变多，查询性能变低； 总结本文从一个问题出发，逐步介绍了InnoDB索引组织表的原理、查询方式，并结合已有知识，回答该问题，结合实践来证明。当然为了表述简单易懂，文中忽略了一些细枝末节，比如一个页中不可能所有空间都用于存放数据，它还会存放一些少量的其他字段比如page level，index number等等，另外还有页的填充因子也导致一个页不可能全部用于保存数据。关于二级索引数据存取方式可以参考MySQL相关书籍，他的要点是结合主键索引进行回表查询。参考参考","link":"/2019/06/04/mysql数据库索引解析.html"},{"title":"java注解Annotation说明实例","text":"摘要Java 注解是附加在代码中的一些元信息，用于一些工具在编译、运行时进行解析和使用，起到说明、配置的功能。注解不会也不能影响代码的实际逻辑，仅仅起到辅助性的作用。 什么是注解？对于很多初次接触的开发者来说应该都有这个疑问？Annontation是Java5开始引入的新特征，中文名称叫注解。它提供了一种安全的类似注释的机制，用来将任何的信息或元数据（metadata）与程序元素（类、方法、成员变量等）进行关联。为程序的元素（类、方法、成员变量）加上更直观更明了的说明，这些说明信息是与程序的业务逻辑无关，并且供指定的工具或框架使用。Annontation像一种修饰符一样，应用于包、类型、构造方法、方法、成员变量、参数及本地变量的声明语句中。 Java注解是附加在代码中的一些元信息，用于一些工具在编译、运行时进行解析和使用，起到说明、配置的功能。注解不会也不能影响代码的实际逻辑，仅仅起到辅助性的作用。包含在 java.lang.annotation 包中。 注解的用处 生成文档。这是最常见的，也是java 最早提供的注解。常用的有@param @return 等 跟踪代码依赖性，实现替代配置文件功能。比如Dagger 2 依赖注入，未来java 开发，将大量注解配置，具有很大用处; 在编译时进行格式检查。如@override 放在方法前，如果你这个方法并不是覆盖了超类方法，则编译时就能检查出。 注解原理注解本质是一个继承了Annotation 的特殊接口，其具体实现类是Java 运行时生成的动态代理类。而我们通过反射获取注解时，返回的是Java 运行时生成的动态代理对象$Proxy1。通过代理对象调用自定义注解（接口）的方法，会最终调用AnnotationInvocationHandler 的invoke 方法。该方法会从memberValues 这个Map 中索引出对应的值。而memberValues 的来源是Java 常量池。 元注解java.lang.annotation 提供了四种元注解，专门注解其他的注解（在自定义注解的时候，需要使用到元注解）： @Documented – 注解是否将包含在JavaDoc中 @Retention – 什么时候使用该注解 @Target – 注解用于什么地方 @Inherited – 是否允许子类继承该注解 @Retention 定义该注解的生命周期 ● RetentionPolicy.SOURCE : 在编译阶段丢弃。这些注解在编译结束之后就不再有任何意义，所以它们不会写入字节码。@Override, @SuppressWarnings都属于这类注解。 ● RetentionPolicy.CLASS : 在类加载的时候丢弃。在字节码文件的处理中有用。注解默认使用这种方式 ● RetentionPolicy.RUNTIME : 始终不会丢弃，运行期也保留该注解，因此可以使用反射机制读取该注解的信息。我们自定义的注解通常使用这种方式。 @Target 表示该注解用于什么地方。默认值为任何元素，表示该注解用于什么地方。可用的ElementType 参数包括 ● ElementType.CONSTRUCTOR: 用于描述构造器 ● ElementType.FIELD: 成员变量、对象、属性（包括enum实例） ● ElementType.LOCAL_VARIABLE: 用于描述局部变量 ● ElementType.METHOD: 用于描述方法 ● ElementType.PACKAGE: 用于描述包 ● ElementType.PARAMETER: 用于描述参数 ● ElementType.TYPE: 用于描述类、接口(包括注解类型) 或enum声明 @Documented 一个简单的Annotations 标记注解，表示是否将注解信息添加在java 文档中。 @Inherited 定义该注释和子类的关系@Inherited 元注解是一个标记注解，@Inherited 阐述了某个被标注的类型是被继承的。如果一个使用了@Inherited 修饰的annotation 类型被用于一个class，则这个annotation 将被用于该class 的子类。 常见标准的Annotation Overridejava.lang.Override 是一个标记类型注解，它被用作标注方法。它说明了被标注的方法重载了父类的方法，起到了断言的作用。如果我们使用了这种注解在一个没有覆盖父类方法的方法时，java 编译器将以一个编译错误来警示 DeprecatedDeprecated 也是一种标记类型注解。当一个类型或者类型成员使用@Deprecated 修饰的话，编译器将不鼓励使用这个被标注的程序元素。所以使用这种修饰具有一定的“延续性”：如果我们在代码中通过继承或者覆盖的方式使用了这个过时的类型或者成员，虽然继承或者覆盖后的类型或者成员并不是被声明为@Deprecated，但编译器仍然要报警。 SuppressWarningsSuppressWarning 不是一个标记类型注解。它有一个类型为String[] 的成员，这个成员的值为被禁止的警告名。对于javac 编译器来讲，被-Xlint 选项有效的警告名也同样对@SuppressWarings 有效，同时编译器忽略掉无法识别的警告名。 @SuppressWarnings(“unchecked”) 自定义注解自定义注解类编写的一些规则: Annotation 型定义为@interface, 所有的Annotation 会自动继承java.lang.Annotation这一接口,并且不能再去继承别的类或是接口. 参数成员只能用public 或默认(default) 这两个访问权修饰 参数成员只能用基本类型byte、short、char、int、long、float、double、boolean八种基本数据类型和String、Enum、Class、annotations等数据类型，以及这一些类型的数组. 要获取类方法和字段的注解信息，必须通过Java的反射技术来获取 Annotation 对象，因为你除此之外没有别的获取注解对象的方法 注解也可以没有定义成员,，不过这样注解就没啥用了PS:自定义注解需要使用到元注解 实例FruitName.java 123456789101112131415import java.lang.annotation.Documented;import java.lang.annotation.Retention;import java.lang.annotation.Target;import static java.lang.annotation.ElementType.FIELD;import static java.lang.annotation.RetentionPolicy.RUNTIME;/** * 水果名称注解 */@Target(FIELD)@Retention(RUNTIME)@Documentedpublic @interface FruitName { String value() default \"\";} FruitColor.java 123456789101112131415161718192021222324import java.lang.annotation.Documented;import java.lang.annotation.Retention;import java.lang.annotation.Target;import static java.lang.annotation.ElementType.FIELD;import static java.lang.annotation.RetentionPolicy.RUNTIME;/** * 水果颜色注解 */@Target(FIELD)@Retention(RUNTIME)@Documentedpublic @interface FruitColor { /** * 颜色枚举 */ public enum Color{ BLUE,RED,GREEN}; /** * 颜色属性 */ Color fruitColor() default Color.GREEN;} FruitProvider.java 12345678910111213141516171819202122232425262728import java.lang.annotation.Documented;import java.lang.annotation.Retention;import java.lang.annotation.Target;import static java.lang.annotation.ElementType.FIELD;import static java.lang.annotation.RetentionPolicy.RUNTIME;/** * 水果供应者注解 */@Target(FIELD)@Retention(RUNTIME)@Documentedpublic @interface FruitProvider { /** * 供应商编号 */ public int id() default -1; /** * 供应商名称 */ public String name() default \"\"; /** * 供应商地址 */ public String address() default \"\";} FruitInfoUtil.java 1234567891011121314151617181920212223242526272829303132import java.lang.reflect.Field;/** * 注解处理器 */public class FruitInfoUtil { public static void getFruitInfo(Class&lt;?&gt; clazz){ String strFruitName=\" 水果名称：\"; String strFruitColor=\" 水果颜色：\"; String strFruitProvicer=\"供应商信息：\"; Field[] fields = clazz.getDeclaredFields(); for(Field field :fields){ if(field.isAnnotationPresent(FruitName.class)){ FruitName fruitName = (FruitName) field.getAnnotation(FruitName.class); strFruitName=strFruitName+fruitName.value(); System.out.println(strFruitName); } else if(field.isAnnotationPresent(FruitColor.class)){ FruitColor fruitColor= (FruitColor) field.getAnnotation(FruitColor.class); strFruitColor=strFruitColor+fruitColor.fruitColor().toString(); System.out.println(strFruitColor); } else if(field.isAnnotationPresent(FruitProvider.class)){ FruitProvider fruitProvider= (FruitProvider) field.getAnnotation(FruitProvider.class); strFruitProvicer=\" 供应商编号：\"+fruitProvider.id()+\" 供应商名称：\"+fruitProvider.name()+\" 供应商地址：\"+fruitProvider.address(); System.out.println(strFruitProvicer); } } }} Apple.java 1234567891011121314151617181920212223242526272829303132333435363738394041import test.FruitColor.Color;/** * 注解使用 */public class Apple { @FruitName(\"Apple\") private String appleName; @FruitColor(fruitColor=Color.RED) private String appleColor; @FruitProvider(id=1,name=\"陕西红富士集团\",address=\"陕西省西安市延安路89号红富士大厦\") private String appleProvider; public void setAppleColor(String appleColor) { this.appleColor = appleColor; } public String getAppleColor() { return appleColor; } public void setAppleName(String appleName) { this.appleName = appleName; } public String getAppleName() { return appleName; } public void setAppleProvider(String appleProvider) { this.appleProvider = appleProvider; } public String getAppleProvider() { return appleProvider; } public void displayName(){ System.out.println(\"水果的名字是：苹果\"); }} FruitRun.java 12345678/** * 输出结果 */public class FruitRun { public static void main(String[] args) { FruitInfoUtil.getFruitInfo(Apple.class); }} 运行结果： 123水果名称：Apple水果颜色：RED供应商编号：1 供应商名称：陕西红富士集团 供应商地址：陕西省西安市延安路89号红富士大厦 参考资料","link":"/2019/05/31/java注解Annotation说明实例.html"},{"title":"elasticsearch6.x倒排索引和分词","text":"摘要倒排索引（Inverted Index）也叫反向索引，有反向索引必有正向索引。通俗地来讲，正向索引是通过key找value，反向索引则是通过value找key。 倒排索引（Inverted Index）也叫反向索引，有反向索引必有正向索引。通俗地来讲，正向索引是通过key找value，反向索引则是通过value找key。 倒排索引 正排索引：文档id到单词的关联关系 倒排索引：单词到文档id的关联关系 示例：对以下三个文档去除停用词后构造倒排索引 倒排索引-查询过程查询包含“搜索引擎”的文档 通过倒排索引获得“搜索引擎”对应的文档id列表，有1，3 通过正排索引查询1和3的完整内容 返回最终结果 倒排索引-组成 单词词典（Term Dictionary） 倒排列表（Posting List） 单词词典（Term Dictionary）单词词典的实现一般用B+树，B+树构造的可视化过程网址: B+ Tree Visualization 关于B树和B+树 维基百科-B树 维基百科-B+树 B树和B+树的插入、删除图文详解 倒排列表（Posting List） 倒排列表记录了单词对应的文档集合，有倒排索引项（Posting）组成 倒排索引项主要包含如下信息： 文档id用于获取原始信息 单词频率（TF，Term Frequency），记录该单词在该文档中出现的次数，用于后续相关性算分 位置（Posting），记录单词在文档中的分词位置（多个），用于做词语搜索（Phrase Query） 偏移（Offset），记录单词在文档的开始和结束位置，用于高亮显示 B+树内部结点存索引，叶子结点存数据，这里的 单词词典就是B+树索引，倒排列表就是数据，整合在一起后如下所示 ES存储的是一个JSON格式的文档，其中包含多个字段，每个字段会有自己的倒排索引 分词分词是将文本转换成一系列单词（Term or Token）的过程，也可以叫文本分析，在ES里面称为Analysis 分词器分词器是ES中专门处理分词的组件，英文为Analyzer，它的组成如下： Character Filters：针对原始文本进行处理，比如去除html标签 Tokenizer：将原始文本按照一定规则切分为单词 Token Filters：针对Tokenizer处理的单词进行再加工，比如转小写、删除或增新等处理 分词器调用顺序 Analyze APIES提供了一个可以测试分词的API接口，方便验证分词效果，endpoint是_analyze 可以直接指定analyzer进行测试 可以直接指定索引中的字段进行测试 1234567891011POST test_index/doc{ \"username\": \"whirly\", \"age\":22}POST test_index/_analyze{ \"field\": \"username\", \"text\": [\"hello world\"]} 可以自定义分词器进行测试 123456POST _analyze{ &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [&quot;lowercase&quot;], &quot;text&quot;: [&quot;Hello World&quot;]} 预定义的分词器ES自带的分词器有如下： Standard Analyzer 默认分词器 按词切分，支持多语言 小写处理 Simple Analyzer 按照非字母切分 小写处理 Whitespace Analyzer 空白字符作为分隔符 Stop Analyzer 相比Simple Analyzer多了去除请用词处理 停用词指语气助词等修饰性词语，如the, an, 的， 这等 Keyword Analyzer 不分词，直接将输入作为一个单词输出 Pattern Analyzer 通过正则表达式自定义分隔符 默认是\\W+，即非字词的符号作为分隔符 Language Analyzer 提供了30+种常见语言的分词器 示例：停用词分词器 12345POST _analyze{ \"analyzer\": \"stop\", \"text\": [\"The 2 QUICK Brown Foxes jumped over the lazy dog's bone.\"]} 结果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667{ \"tokens\": [ { \"token\": \"quick\", \"start_offset\": 6, \"end_offset\": 11, \"type\": \"word\", \"position\": 1 }, { \"token\": \"brown\", \"start_offset\": 12, \"end_offset\": 17, \"type\": \"word\", \"position\": 2 }, { \"token\": \"foxes\", \"start_offset\": 18, \"end_offset\": 23, \"type\": \"word\", \"position\": 3 }, { \"token\": \"jumped\", \"start_offset\": 24, \"end_offset\": 30, \"type\": \"word\", \"position\": 4 }, { \"token\": \"over\", \"start_offset\": 31, \"end_offset\": 35, \"type\": \"word\", \"position\": 5 }, { \"token\": \"lazy\", \"start_offset\": 40, \"end_offset\": 44, \"type\": \"word\", \"position\": 7 }, { \"token\": \"dog\", \"start_offset\": 45, \"end_offset\": 48, \"type\": \"word\", \"position\": 8 }, { \"token\": \"s\", \"start_offset\": 49, \"end_offset\": 50, \"type\": \"word\", \"position\": 9 }, { \"token\": \"bone\", \"start_offset\": 51, \"end_offset\": 55, \"type\": \"word\", \"position\": 10 } ]} 中文分词 难点 中文分词指的是将一个汉字序列切分为一个一个的单独的词。在英文中，单词之间以空格作为自然分界词，汉语中词没有一个形式上的分界符 上下文不同，分词结果迥异，比如交叉歧义问题 常见分词系统 IK：实现中英文单词的切分，可自定义词库，支持热更新分词词典 jieba：支持分词和词性标注，支持繁体分词，自定义词典，并行分词等 Hanlp：由一系列模型与算法组成的Java工具包，目标是普及自然语言处理在生产环境中的应用 THUAC：中文分词和词性标注 安装ik中文分词插件12345# 在Elasticsearch安装目录下执行命令，然后重启esbin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.0/elasticsearch-analysis-ik-6.3.0.zip# 如果由于网络慢，安装失败，可以先下载好zip压缩包，将下面命令改为实际的路径，执行，然后重启esbin/elasticsearch-plugin install file:///path/to/elasticsearch-analysis-ik-6.3.0.zip ik测试 - ik_smart 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667POST _analyze{ \"analyzer\": \"ik_smart\", \"text\": [\"公安部：各地校车将享最高路权\"]}# 结果{ \"tokens\": [ { \"token\": \"公安部\", \"start_offset\": 0, \"end_offset\": 3, \"type\": \"CN_WORD\", \"position\": 0 }, { \"token\": \"各地\", \"start_offset\": 4, \"end_offset\": 6, \"type\": \"CN_WORD\", \"position\": 1 }, { \"token\": \"校车\", \"start_offset\": 6, \"end_offset\": 8, \"type\": \"CN_WORD\", \"position\": 2 }, { \"token\": \"将\", \"start_offset\": 8, \"end_offset\": 9, \"type\": \"CN_CHAR\", \"position\": 3 }, { \"token\": \"享\", \"start_offset\": 9, \"end_offset\": 10, \"type\": \"CN_CHAR\", \"position\": 4 }, { \"token\": \"最高\", \"start_offset\": 10, \"end_offset\": 12, \"type\": \"CN_WORD\", \"position\": 5 }, { \"token\": \"路\", \"start_offset\": 12, \"end_offset\": 13, \"type\": \"CN_CHAR\", \"position\": 6 }, { \"token\": \"权\", \"start_offset\": 13, \"end_offset\": 14, \"type\": \"CN_CHAR\", \"position\": 7 } ]} ik测试 - ik_max_word 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081POST _analyze{ \"analyzer\": \"ik_max_word\", \"text\": [\"公安部：各地校车将享最高路权\"]}# 结果{ \"tokens\": [ { \"token\": \"公安部\", \"start_offset\": 0, \"end_offset\": 3, \"type\": \"CN_WORD\", \"position\": 0 }, { \"token\": \"公安\", \"start_offset\": 0, \"end_offset\": 2, \"type\": \"CN_WORD\", \"position\": 1 }, { \"token\": \"部\", \"start_offset\": 2, \"end_offset\": 3, \"type\": \"CN_CHAR\", \"position\": 2 }, { \"token\": \"各地\", \"start_offset\": 4, \"end_offset\": 6, \"type\": \"CN_WORD\", \"position\": 3 }, { \"token\": \"校车\", \"start_offset\": 6, \"end_offset\": 8, \"type\": \"CN_WORD\", \"position\": 4 }, { \"token\": \"将\", \"start_offset\": 8, \"end_offset\": 9, \"type\": \"CN_CHAR\", \"position\": 5 }, { \"token\": \"享\", \"start_offset\": 9, \"end_offset\": 10, \"type\": \"CN_CHAR\", \"position\": 6 }, { \"token\": \"最高\", \"start_offset\": 10, \"end_offset\": 12, \"type\": \"CN_WORD\", \"position\": 7 }, { \"token\": \"路\", \"start_offset\": 12, \"end_offset\": 13, \"type\": \"CN_CHAR\", \"position\": 8 }, { \"token\": \"权\", \"start_offset\": 13, \"end_offset\": 14, \"type\": \"CN_CHAR\", \"position\": 9 } ]} ik两种分词模式ik_max_word 和 ik_smart 什么区别? ik_max_word: 会将文本做最细粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”，会穷尽各种可能的组合； ik_smart: 会做最粗粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,国歌”。 自定义分词当自带的分词无法满足需求时，可以自定义分词，通过定义Character Filters、Tokenizer和Token Filters实现 Character Filters 在Tokenizer之前对原始文本进行处理，比如增加、删除或替换字符等 自带的如下: HTML Strip Character Filter：去除HTML标签和转换HTML实体 Mapping Character Filter：进行字符替换操作 Pattern Replace Character Filter：进行正则匹配替换 会影响后续tokenizer解析的position和offset信息 Character Filters测试1234567891011121314151617181920212223POST _analyze{ \"tokenizer\": \"keyword\", \"char_filter\": [\"html_strip\"], \"text\": [\"&lt;p&gt;I&amp;apos;m so &lt;b&gt;happy&lt;/b&gt;!&lt;/p&gt;\"]}# 结果{ \"tokens\": [ { \"token\": \"\"\"I'm so happy!\"\"\", \"start_offset\": 0, \"end_offset\": 32, \"type\": \"word\", \"position\": 0 } ]} Tokenizers 将原始文本按照一定规则切分为单词（term or token） 自带的如下： standard 按照单词进行分割 letter 按照非字符类进行分割 whitespace 按照空格进行分割 UAX URL Email 按照standard进行分割，但不会分割邮箱和URL Ngram 和 Edge NGram 连词分割 Path Hierarchy 按照文件路径进行分割 Tokenizers 测试1234567891011121314151617181920212223242526272829303132POST _analyze{ \"tokenizer\": \"path_hierarchy\", \"text\": [\"/path/to/file\"]}# 结果{ \"tokens\": [ { \"token\": \"/path\", \"start_offset\": 0, \"end_offset\": 5, \"type\": \"word\", \"position\": 0 }, { \"token\": \"/path/to\", \"start_offset\": 0, \"end_offset\": 8, \"type\": \"word\", \"position\": 0 }, { \"token\": \"/path/to/file\", \"start_offset\": 0, \"end_offset\": 13, \"type\": \"word\", \"position\": 0 } ]} Token Filters 对于tokenizer输出的单词（term）进行增加、删除、修改等操作 自带的如下： lowercase 将所有term转为小写 stop 删除停用词 Ngram 和 Edge NGram 连词分割 Synonym 添加近义词的term Token Filters测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950POST _analyze{ \"text\": [ \"a Hello World!\" ], \"tokenizer\": \"standard\", \"filter\": [ \"stop\", \"lowercase\", { \"type\": \"ngram\", \"min_gram\": 4, \"max_gram\": 4 } ]}# 结果{ \"tokens\": [ { \"token\": \"hell\", \"start_offset\": 2, \"end_offset\": 7, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 1 }, { \"token\": \"ello\", \"start_offset\": 2, \"end_offset\": 7, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 1 }, { \"token\": \"worl\", \"start_offset\": 8, \"end_offset\": 13, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 2 }, { \"token\": \"orld\", \"start_offset\": 8, \"end_offset\": 13, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 2 } ]} 自定义分词自定义分词需要在索引配置中设定 char_filter、tokenizer、filter、analyzer等 自定义分词示例: 分词器名称：my_custom\\ 过滤器将token转为大写 1234567891011121314151617181920PUT test_index_1{ \"settings\": { \"analysis\": { \"analyzer\": { \"my_custom_analyzer\": { \"type\": \"custom\", \"tokenizer\": \"standard\", \"char_filter\": [ \"html_strip\" ], \"filter\": [ \"uppercase\", \"asciifolding\" ] } } } }} 12345678910111213141516171819202122232425262728293031323334353637383940// javaXContentFactory.jsonBuilder() .startObject().startObject(\"analysis\") .startObject(\"normalizer\").startObject(Normalizers.CASE_INSENSITIVE) .field(\"type\", \"custom\") .field(\"filter\", \"lowercase\") .endObject().endObject() // 动态同义词 .startObject(\"filter\") .startObject(\"dynamic_synonym\").field(\"type\", \"dynamic_synonym\").field(\"tokenizer\", \"ik_smart\").endObject().endObject() .startObject(\"analyzer\") // 自定义分词器 .startObject(\"ik\") .field(\"filter\", \"\") .field(\"char_filter\", Arrays.asList(\"char_mapper\")) .field(\"type\", \"custom\") .field(\"tokenizer\", \"ik_max_word\") .endObject() // 搜索时处理同义词 .startObject(\"ikt\") .field(\"filter\", Arrays.asList(\"dynamic_synonym\")) .field(\"char_filter\", Arrays.asList(\"char_mapper\")) .field(\"type\", \"custom\") .field(\"tokenizer\", \"ik_smart\") .endObject() .endObject() // 自定义字符过滤 .startObject(\"char_filter\") .startObject(\"char_mapper\") .field(\"type\", \"mapping\") .field(\"mappings_path\", \"analysis/mapping.txt\") .endObject() .endObject() .endObject().endObject(); 自定义分词器测试1234567891011121314151617181920212223242526272829303132POST test_index_1/_analyze{ \"analyzer\": \"my_custom_analyzer\", \"text\": [\"&lt;p&gt;I&amp;apos;m so &lt;b&gt;happy&lt;/b&gt;!&lt;/p&gt;\"]}# 结果{ \"tokens\": [ { \"token\": \"I'M\", \"start_offset\": 3, \"end_offset\": 11, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 0 }, { \"token\": \"SO\", \"start_offset\": 12, \"end_offset\": 14, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 1 }, { \"token\": \"HAPPY\", \"start_offset\": 18, \"end_offset\": 27, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 2 } ]} 分词使用说明分词会在如下两个时机使用： 创建或更新文档时(Index Time)，会对相应的文档进行分词处理 查询时（Search Time），会对查询语句进行分词 查询时通过analyzer指定分词器 通过index mapping设置search_analyzer实现 一般不需要特别指定查询时分词器，直接使用索引分词器即可，否则会出现无法匹配的情况 分词使用建议 明确字段是否需要分词，不需要分词的字段就将type设置为keyword，可以节省空间和提高写性能 善用_analyze API，查看文档的分词结果 参考自","link":"/2019/05/06/elasticsearch6-x倒排索引和分词.html"},{"title":"Java版常用正则表达式说明","text":"摘要正则表达式在几乎所有语言中都可以使用，无论是前端的JavaScript、还是后端的Java、c#。他们都提供相应的接口/函数支持正则表达式。 元字符元字符说明：匹配除换行符以外的任意字符 w：匹配字母或数字或下划线或汉字 s：匹配任意的空白符 d：匹配数字匹配单词的开始或结束 ^：匹配字符串的开始 $：匹配字符串的结束 eg:匹配有abc开头的字符串：abc或者^abc 匹配8位数字的QQ号码：^dddddddd$ 匹配1开头11位数字的手机号码：^1dddddddddd$ 重复限定符为了处理这些重复问题，正则表达式中一些重复限定符，把重复部分用合适的限定符替代 语法说明： *重复零次或更多次 +重复一次或更多次 ?重复零次或一次 {n}重复n次 {n,}重复n次或更多次 {n,m}重复n到m次 匹配8位数字的QQ号码：^d{8}$ 匹配1开头11位数字的手机号码：^1d{10}$ 匹配银行卡号是14~18位的数字：^d{14,18}$ 匹配以a开头的，0个或多个b结尾的字符串^ab*$ 分组正则表达式中用小括号()来做分组，也就是括号中的内容作为一个整体。 因此当我们要匹配多个ab时，我们可以这样。 如匹配字符串中包含0到多个ab开头：^(ab)* 转义我们看到正则表达式用小括号来做分组，那么问题来了： 如果要匹配的字符串中本身就包含小括号，那是不是冲突？应该怎么办？ 针对这种情况，正则提供了转义的方式，也就是要把这些元字符、限定符或者关键字转义成普通的字符，做法很简答，就是在要转义的字符前面加个斜杠，也就是\\即可。 如要匹配以(ab)开头：^\\(ab\\)* 条件或回到我们刚才的手机号匹配，我们都知道：国内号码都来自三大网，它们都有属于自己的号段，比如联通有130/131/132/155/156/185/186/145/176等号段，假如让我们匹配一个联通的号码，那按照我们目前所学到的正则，应该无从下手的，因为这里包含了一些并列的条件，也就是“或”，那么在正则中是如何表示“或”的呢？ 正则用符号 | 来表示或，也叫做分支条件，当满足正则里的分支条件的任何一种条件时，都会当成是匹配成功。 那么我们就可以用“或”条件来处理这个问题：^(130|131|132|155|156|185|186|145|176)d{8}$ 区间正则提供一个元字符中括号 [] 来表示区间条件。 限定0到9 可以写成[0-9] 限定A-Z 写成[A-Z] 限定某些数字 [165] 那上面的正则我们还改成这样： ^((13[0-2])|(15[56])|(18[5-6])|145|176)d{8}$ 好了，正则表达式的基本用法就讲到这里了，其实它还有非常多的知识点以及元字符，我们在此只列举了部分元字符和语法来讲，旨在给那些不懂正则或者想学正则但有看不下去文档的人做一个快速入门级的教程，看完本教程，即使你不能写出高大上的正则，至少也能写一些简单的正则或者看得懂别人写的正则了。 正则进阶知识点1.零宽断言断言：俗话的断言就是“我断定什么什么”，而正则中的断言，就是说正则可以指明在指定的内容的前面或后面会出现满足指定规则的内容，意思正则也可以像人类那样断定什么什么，比如”ss1aa2bb3”,正则可以用断言找出aa2前面有bb3，也可以找出aa2后面有ss1. 零宽：就是没有宽度，在正则中，断言只是匹配位置，不占字符，也就是说，匹配结果里是不会返回断言本身。 eg:假设我们要用爬虫抓取csdn里的文章阅读量。通过查看源代码可以看到文章阅读量这个内容是这样的结构 “阅读数：641“ 其中也就‘641’这个是变量，也就是说不同文章不同的值，当我们拿到这个字符串时，需要获得这里边的‘641’有很多种办法，但如果正则应该怎么匹配呢？ 正向先行断言（正前瞻）语法：（?=pattern） 作用：匹配pattern表达式的前面内容，不返回本身。 这样子说，还是一脸懵逼，好吧，回归刚才那个栗子，要取到阅读量，在正则表达式中就意味着要能匹配到‘’前面的数字内容。 按照上所说的正向先行断言可以匹配表达式前面的内容，那意思就是:(?=) 就可以匹配到前面的内容了。 匹配什么内容呢？如果要所有内容那就是： 12345678910 String reg=\".+(?=&lt;/span&gt;)\";String test = \"&lt;span class=\"read-count\"&gt;阅读数：641&lt;/span&gt;\"; Pattern pattern = Pattern.compile(reg); Matcher mc= pattern.matcher(test); while(mc.find()){ System.out.println(\"匹配结果：\") System.out.println(mc.group()); } //匹配结果：//&lt;span class=\"read-count\"&gt;阅读数：641 可是老哥我们要的只是前面的数字呀，那也简单咯，匹配数字 d,那可以改成： 123456789String reg=\"\\d+(?=&lt;/span&gt;)\";String test = \"&lt;span class=\\\"read-count\\\"&gt;阅读数：641&lt;/span&gt;\";Pattern pattern = Pattern.compile(reg);Matcher mc= pattern.matcher(test);while(mc.find()){ System.out.println(mc.group());}//匹配结果：//641 大功告成！ 正向后行断言（正后顾）语法：（?&lt;=pattern） 作用：匹配pattern表达式的后面的内容，不返回本身。 有先行就有后行，先行是匹配前面的内容，那后行就是匹配后面的内容啦。 上面的栗子，我们也可以用后行断言来处理。 12345678910//(?&lt;=&lt;span class=\"read-count\"&gt;阅读数：)d+String reg=\"(?&lt;=&lt;span class=\\\"read-count\\\"&gt;阅读数：)\\d+\";String test = \"&lt;span class=\\\"read-count\\\"&gt;阅读数：641&lt;/span&gt;\"; Pattern pattern = Pattern.compile(reg); Matcher mc= pattern.matcher(test); while(mc.find()){ System.out.println(mc.group()); }//匹配结果：//641 就这么简单。 负向先行断言（负前瞻）语法：(?!pattern) 作用：匹配非pattern表达式的前面内容，不返回本身。 有正向也有负向，负向在这里其实就是非的意思。 举个栗子：比如有一句 “我爱祖国，我是祖国的花朵” 现在要找到不是’的花朵’前面的祖国 用正则就可以这样写：祖国(?!的花朵)。 负向后行断言（负后顾）语法：(?&lt;!pattern) 作用：匹配非pattern表达式的后面内容，不返回本身。 2.捕获和非捕获单纯说到捕获，他的意思是匹配表达式，但捕获通常和分组联系在一起，也就是“捕获组”。 捕获组：匹配子表达式的内容，把匹配结果保存到内存中中数字编号或显示命名的组里，以深度优先进行编号，之后可以通过序号或名称来使用这些匹配结果。 而根据命名方式的不同，又可以分为两种组。 数字编号捕获组 语法：(exp) 解释：从表达式左侧开始，每出现一个左括号和它对应的右括号之间的内容为一个分组，在分组中，第0组为整个表达式，第一组开始为分组。 比如固定电话的：020-85653333 他的正则表达式为：(0d{2})-(d{8}) 按照左括号的顺序，这个表达式有如下分组： 序号编号分组内容00(0d{2})-(d{8})020-8565333311(0d{2})02022(d{8})85653333 我们用Java来验证一下： 12345678910String test = \"020-85653333\"; String reg=\"(0\\d{2})-(\\d{8})\"; Pattern pattern = Pattern.compile(reg); Matcher mc= pattern.matcher(test); if(mc.find()){ System.out.println(\"分组的个数有：\"+mc.groupCount()); for(int i=0;i&lt;=mc.groupCount();i++){ System.out.println(\"第\"+i+\"个分组为：\"+mc.group(i)); } } 输出结果： 1234分组的个数有：2第0个分组为：020-85653333第1个分组为：020第2个分组为：85653333 可见，分组个数是2，但是因为第0个为整个表达式本身，因此也一起输出了。 命名编号捕获组 语法：(?exp) 解释：分组的命名由表达式中的name指定 比如区号也可以这样写:(?d{2})-(?d{8}) 按照左括号的顺序，这个表达式有如下分组：序号名称分组内容00(0d{2})-(d{8})020-856533331quhao(0d{2})0202haoma(d{8})85653333 用代码来验证一下： 123456789String test = \"020-85653333\";String reg=\"(?&lt;quhao&gt;0\\d{2})-(?&lt;haoma&gt;\\d{8})\";Pattern pattern = Pattern.compile(reg);Matcher mc= pattern.matcher(test);if(mc.find()){ System.out.println(\"分组的个数有：\"+mc.groupCount()); System.out.println(mc.group(\"quhao\")); System.out.println(mc.group(\"haoma\"));} 输出结果： 123分组的个数有：2分组名称为:quhao,匹配内容为：020分组名称为:haoma,匹配内容为：85653333 非捕获组 语法：(?:exp) 解释：和捕获组刚好相反，它用来标识那些不需要捕获的分组，说的通俗一点，就是你可以根据需要去保存你的分组。 比如上面的正则表达式，程序不需要用到第一个分组，那就可以这样写：(?:d{2})-(d{8}) 序号编号分组内容00(0d{2})-(d{8})020-8565333311(d{8})85653333 验证一下： 12345678910String test = \"020-85653333\";String reg=\"(?:0\\d{2})-(\\d{8})\";Pattern pattern = Pattern.compile(reg);Matcher mc= pattern.matcher(test);if(mc.find()){ System.out.println(\"分组的个数有：\"+mc.groupCount()); for(inti=0;i&lt;=mc.groupCount();i++){ System.out.println(\"第\"+i+\"个分组为：\"+mc.group(i)); }} 输出结果： 123分组的个数有：1第0个分组为：020-85653333第1个分组为：85653333 3.反向引用上面讲到捕获，我们知道：捕获会返回一个捕获组，这个分组是保存在内存中，不仅可以在正则表达式外部通过程序进行引用，也可以在正则表达式内部进行引用，这种引用方式就是反向引用。 根据捕获组的命名规则，反向引用可分为： 数字编号组反向引用：k或 umber 命名编号组反向引用：k或者’name’ 好了 讲完了，懂吗？不懂！！！ 可能连前面讲的捕获有什么用都还不懂吧？ 其实只是看完捕获不懂不会用是很正常的！ 因为捕获组通常是和反向引用一起使用的。 上面说到捕获组是匹配子表达式的内容按序号或者命名保存起来以便使用。 注意两个字眼：“内容” 和 “使用”。 这里所说的“内容”，是匹配结果，而不是子表达式本身，强调这个有什么用？嗯，先记住。 那这里所说的“使用”是怎样使用呢？ 因为它的作用主要是用来查找一些重复的内容或者做替换指定字符。 还是举栗子吧。 比如要查找一串字母”aabbbbgbddesddfiid”里成对的字母 如果按照我们之前学到的正则，什么区间啊限定啊断言啊可能是办不到的， 现在我们先用程序思维理一下思路： 1）匹配到一个字母 2）匹配第下一个字母，检查是否和上一个字母是否一样 3）如果一样，则匹配成功，否则失败 这里的思路2中匹配下一个字母时，需要用到上一个字母，那怎么记住上一个字母呢？？？ 这下子捕获就有用处啦，我们可以利用捕获把上一个匹配成功的内容用来作为本次匹配的条件 好了，有思路就要实践 首先匹配一个字母：w 我们需要做成分组才能捕获，因此写成这样：(w) 那这个表达式就有一个捕获组：（w） 然后我们要用这个捕获组作为条件，那就可以：(w) 这样就大功告成了 可能有人不明白了，是什么意思呢？ 还记得捕获组有两种命名方式吗，一种是是根据捕获分组顺序命名，一种是自定义命名来作为捕获组的命名 在默认情况下都是以数字来命名，而且数字命名的顺序是从1开始的 因此要引用第一个捕获组，根据反向引用的数字命名规则 就需要 k或者 当然，通常都是是后者。 我们来测试一下： 1234567String test = \"aabbbbgbddesddfiid\";Pattern pattern = Pattern.compile(\"(\\w)\\1\");Matcher mc= pattern.matcher(test);while(mc.find()){ System.out.println(mc.group());} 输出结果： 1aabbbbddddii 嗯，这就是我们想要的了。 在举个替换的例子，假如想要把字符串中abc换成a。 123String test = \"abcbbabcbcgbddesddfiid\";String reg=\"(a)(b)c\";System.out.println(test.replaceAll(reg, \"$1\")); 输出结果： 1abbabcgbddesddfiid 4.贪婪和非贪婪 贪婪 我们都知道，贪婪就是不满足，尽可能多的要。 在正则中，贪婪也是差不多的意思: 贪婪匹配：当正则表达式中包含能接受重复的限定符时，通常的行为是（在使整个表达式能得到匹配的前提下）匹配尽可能多的字符，这匹配方式叫做贪婪匹配。 特性：一次性读入整个字符串进行匹配，每当不匹配就舍弃最右边一个字符，继续匹配，依次匹配和舍弃（这种匹配-舍弃的方式也叫做回溯），直到匹配成功或者把整个字符串舍弃完为止，因此它是一种最大化的数据返回，能多不会少。 前面我们讲过重复限定符，其实这些限定符就是贪婪量词，比如表达式：d{3,6}。 用来匹配3到6位数字，在这种情况下，它是一种贪婪模式的匹配，也就是假如字符串里有6个个数字可以匹配，那它就是全部匹配到。 如下面的代码。 123456789String reg=\"\\d{3,6}\";String test=\"61762828 176 2991 871\";System.out.println(\"文本：\"+test);System.out.println(\"贪婪模式：\"+reg);Pattern p1 =Pattern.compile(reg);Matcher m1 = p1.matcher(test);while(m1.find()){ System.out.println(\"匹配结果：\"+m1.group(0));} 输出结果： 123456文本：61762828 176 2991 44 871贪婪模式：d{3,6}匹配结果：617628匹配结果：176匹配结果：2991匹配结果：871 由结果可见：本来字符串中的“61762828”这一段，其实只需要出现3个（617）就已经匹配成功了的，但是他并不满足，而是匹配到了最大能匹配的字符，也就是6个。 一个量词就如此贪婪了， 那有人会问，如果多个贪婪量词凑在一起，那他们是如何支配自己的匹配权的呢？ 是这样的，多个贪婪在一起时，如果字符串能满足他们各自最大程度的匹配时，就互不干扰，但如果不能满足时，会根据深度优先原则，也就是从左到右的每一个贪婪量词，优先最大数量的满足，剩余再分配下一个量词匹配。 123456789String reg=\"(\\d{1,2})(\\d{3,4})\";String test=\"61762828 176 2991 87321\";System.out.println(\"文本：\"+test);System.out.println(\"贪婪模式：\"+reg);Pattern p1 =Pattern.compile(reg);Matcher m1 = p1.matcher(test);while(m1.find()){ System.out.println(\"匹配结果：\"+m1.group(0));} 输出结果： 12345文本：61762828 176 2991 87321贪婪模式：(d{1,2})(d{3,4})匹配结果：617628匹配结果：2991匹配结果：87321 “617628” 是前面的d{1,2}匹配出了61，后面的匹配出了7628 “2991” 是前面的d{1,2}匹配出了29 ，后面的匹配出了91 “87321”是前面的d{1,2}匹配出了87，后面的匹配出了321 懒惰（非贪婪） 懒惰匹配：当正则表达式中包含能接受重复的限定符时，通常的行为是（在使整个表达式能得到匹配的前提下）匹配尽可能少的字符，这匹配方式叫做懒惰匹配。 特性：从左到右，从字符串的最左边开始匹配，每次试图不读入字符匹配，匹配成功，则完成匹配，否则读入一个字符再匹配，依此循环（读入字符、匹配）直到匹配成功或者把字符串的字符匹配完为止。 懒惰量词是在贪婪量词后面加个“？” 代码说明 *?重复任意次，但尽可能少重复 +?重复1次或更多次，但尽可能少重复 ??重复0次或1次，但尽可能少重复 {n,m}?重复n到m次，但尽可能少重复 {n,}?重复n次以上，但尽可能少重复。 123456789String reg=\"(\\d{1,2}?)(\\d{3,4})\";String test=\"61762828 176 2991 87321\";System.out.println(\"文本：\"+test);System.out.println(\"贪婪模式：\"+reg);Pattern p1 =Pattern.compile(reg);Matcher m1 = p1.matcher(test);while(m1.find()){ System.out.println(\"匹配结果：\"+m1.group(0));} 输出结果： 12345文本：61762828 176 2991 87321贪婪模式：(d{1,2}?)(d{3,4})匹配结果：61762匹配结果：2991匹配结果：87321 “61762” 是左边的懒惰匹配出6，右边的贪婪匹配出1762 “2991” 是左边的懒惰匹配出2，右边的贪婪匹配出991 “87321” 左边的懒惰匹配出8，右边的贪婪匹配出7321 5.反义前面说到元字符的都是要匹配什么什么，当然如果你想反着来，不想匹配某些字符，正则也提供了一些常用的反义元字符。 元字符解释: W匹配任意不是字母，数字，下划线，汉字的字符 S匹配任意不是空白符的字符 D匹配任意非数字的字符 B匹配不是单词开头或结束的位置 [x]匹配除了x以外的任意字符 [aeiou]匹配除了aeiou这几个字母以外的任意字符 正则进阶知识就讲到这里，正则是一门博大精深的语言，其实学会它的一些语法和知识点还算不太难，但想要做到真正学以致用能写出非常6的正则，还有很远的距离，只有真正对它感兴趣的，并且经常研究和使用它，才会渐渐的理解它的博大精深之处，我就带你们走到这，剩下的，靠自己啦。 参考资料","link":"/2019/04/28/Java版常用正则表达式说明.html"},{"title":"Elasticsearch常用工具api","text":"摘要Elasticsearch 是一个高度可扩展且开源的全文检索和分析引擎。它可以让您快速且近实时地存储，检索以及分析海量数据。它通常用作那些具有复杂搜索功能和需求的应用的底层引擎或者技术。 下面是 Elasticsearch 一些简单的使用案例 : 您运行一个可以让您顾客来搜索您所售产品的在线的网络商店。在这种情况下，您可以使用 Elasticsearch 来存储您的整个产品的目录和库存，并且为他们提供搜索和自动完成的建议。 您想要去收集日志或交易数据，并且您还想要去分析和挖掘这些数据以来找出趋势，统计，概述，或者异常现。在这种情况下，您可以使用 Logstash（Elasticsearch/Logstash/Kibana 技术栈中的一部分）来收集，聚合，以及解析数据，然后让 Logstash 发送这些数据到 Elasticsearch。如果这些数据存在于 Elasticsearch 中，那么您就可以执行搜索和聚合以挖掘出任何您感兴趣的信息。 您运行一个价格警告平台，它允许客户指定精确的价格，如“我感兴趣的是购买指定的电子产品，如果任何供应商该产品的价格在未来一个月内低于 $X 这个价钱的话我应该被通知到”。在这种情况下，您可以收集供应商的价格，推送它们到 Elasticsearch 中去，然后使用 reverse-search（Percolator）（反向搜索（过滤器））功能以匹配客户查询价格的变动，最后如果发现匹配成功就给客户发出通知。 您必须分析/商业智能的需求，并希望快速的研究，分析，可视化，并且需要 ad-hoc（即席查询）海量数据（像数百万或者数十亿条记录）上的质疑。在这种情况下，您可以使用 Elasticsearch 来存储数据，然后使用 Kibana（Elasticsearch/Logstash/Kibana 技术栈中的一部分）以建立一个能够可视化的对您很重要的数据方面的定制的 dashboards（面板）。此外，您还可以使用 Elasticsearch 的聚合功能对您的数据执行复杂的商业智能查询 对于本教程的其余部分，我将引导您完成 Elasticsearch 的启动和运行的过程，同时了解其原理，并执行像 indexing（索引），searching（查询）和 modifing（修改）数据的基础操作。在本教程的最后一部分，您应该可以清楚的了解到 Elasticsearch 是什么，它是如何工作的，并有希望获得启发。看您如何使用它来构建复杂的搜索应用程序或者从数据中挖掘出想要的信息。 常用实例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238# 常用kibana脚本(可以自动填充，比较好用)PUT /test_business/data/1 #更改数据{ \"name\":\"zhe\"}}POST _analyze #实时分词{ \"text\": \"张月超\", \"analyzer\": \"ik_max_word\"}DELETE /analyze_index #删除索引GET analyze_index/_settings #查看索引_settingsGET analyze_index/_mapping #查看索引_mappingPOST /analyze_index/data/_mapping #设置mapping{ \"dynamic\": \"false\", \"properties\": { \"name\": { \"type\": \"text\", \"analyzer\": \"ik\" } }}POST /analyze_index #设置settings{ \"settings\": { \"index\": { \"analysis\": { \"normalizer\": { \"case_insensitive_normalizer\": { \"type\": \"custom\", \"filter\": \"lowercase\" } }, \"filter\": { \"my_synonym\": { \"type\": \"synonym\", \"synonyms_path\": \"analysis-ik/extra/synonyms.dic\" } }, \"analyzer\": { \"ik\": { \"filter\": [ \"my_synonym\" ], \"char_filter\": [ \"my_char_mapper\" ], \"type\": \"custom\", \"tokenizer\": \"ik_max_word\" } }, \"char_filter\": { \"my_char_mapper\": { \"type\": \"mapping\", \"mappings_path\": \"analysis-ik/extra/pre_filter_mapping.dic\" } } } } }, \"mapping\": { \"data\": { \"properties\": { \"name\": { \"type\": \"text\", \"analyzer\": \"ik\" } } } }}GET /es_test_analyzer/_settingsGET /es_test_analyzer/_mappingGET /analyze_index/data/1POST /es_test_analyzer/_analyze{ \"text\": \"超越\", \"analyzer\": \"ik_max_word\"}POST /es_test_analyzer/data/1{ \"name\": \"张月超666哈哈张月超\", \"nick_name\":\"张月超666哈哈张月超\"}POST /es_test_analyzer/data/2{ \"name\": \"王月\", \"nick_name\":\"王月\"}POST /es_test_analyzer/data/3{ \"name\": \"超越\", \"nick_name\":\"超越\"}GET /es_test_analyzer/data/1GET /es_test_analyzer/data/3/_termvectors?fields=nick_name # 查看索引中分词情况GET /es_test_analyzer/data/_search{ \"query\": { \"multi_match\": { \"query\": \"超越\", \"fields\": [ \"nick_name\" ], \"analyzer\": \"ik\", \"operator\": \"or\", \"type\": \"best_fields\" } }, \"highlight\": { \"fields\": { \"name\": { } } }}DELETE /es_test_analyzer/GET /analyze_index/data/_mappingGET /test/data/_search{ \"query\": { \"bool\": { \"should\": [ { \"multi_match\": { \"query\": \"餐饮\", \"fields\": [\"title^2\",\"tags\"] } },{ \"wildcard\": { \"name\": { \"value\": \"*中国*\" } } } ] } }}GET /user/data/9969/_explain # 查看详细id：9969 得分情况{ \"query\": { \"bool\": { \"must\": [ { \"multi_match\": { \"query\": \"tacos n frankies\", \"fields\": [ \"name^1.0\", \"info^1.0\" ], \"type\": \"best_fields\", \"operator\": \"or\", \"analyzer\": \"ik_max_word\", \"slop\": 0, \"prefix_length\": 0, \"max_expansions\": 50, \"zero_terms_query\": \"NONE\", \"auto_generate_synonyms_phrase_query\": true, \"fuzzy_transpositions\": true, \"boost\": 1 } } \"adjust_pure_negative\": true, \"boost\": 1 } } , \"highlight\": { \"fields\": { \"name\": {}, \"info\": {} } }}GET /user/data/_search{ \"query\": { \"bool\": { \"must\": [ { \"multi_match\": { \"query\": \"tacos n frankies\", \"fields\": [ \"name^1.0\", \"info^1.0\" ], \"type\": \"best_fields\", \"operator\": \"or\", \"analyzer\": \"ik_max_word\", \"slop\": 0, \"prefix_length\": 0, \"max_expansions\": 50, \"zero_terms_query\": \"NONE\", \"auto_generate_synonyms_phrase_query\": true, \"fuzzy_transpositions\": true, \"boost\": 1 } } \"adjust_pure_negative\": true, \"boost\": 1 } } , \"highlight\": { \"fields\": { \"name\": {}, \"info\": {} } }} 查看健康信息 curl -XGET ‘localhost:9200/_cat/health?v&amp;pretty’ 我们可以获得 green，yellow，或者 red 的 status。Green 表示一切正常（集群功能齐全）， yellow 表示所有数据可用，但是有些副本尚未分配（集群功能齐全），red 意味着由于某些原因有些数据不可用。注意，集群是 red，它仍然具有部分功能（例如，它将继续从可用的分片中服务搜索请求），但是您可能需要尽快去修复它，因为您已经丢失数据了。 查看所有的索引 curl -XGET ‘localhost:9200/_cat/indices?v&amp;pretty’ Document APISindex api 索引 API 在特定索引中 add ( 添加 ) 或 update *( 更新 ) *a typed JSON document ( 类型化的 JSON 文档 )，使其可搜索。以下示例将 JSON 文档插入到 “twitter” 索引中，ID 为1 ： 123456789101112131415161718192021curl -XPUT 'localhost:9200/twitter/tweet/1?pretty' -H 'Content-Type: application/json' -d'{ \"user\" : \"kimchy\", \"post_date\" : \"2009-11-15T14:12:12\", \"message\" : \"trying out Elasticsearch\"}'结果：{ \"_shards\" : { \"total\" : 2, \"failed\" : 0, \"successful\" : 2 }, \"_index\" : \"twitter\", \"_type\" : \"tweet\", \"_id\" : \"1\", \"_version\" : 1, \"created\" : true, \"result\" : created} total - 指示应对多少 shard copies ( 分片副本 )（ \\primary ( 主 )分片和 replica ( 副本 ) 分片）执行索引操作。 successful - 表示索引操作成功的分片副本的数量。 failed - 在索引操作在副本碎片上失败的情况下包含与复制相关的错误的数组。 当索引操作 successful 返回时，可能不会全部启动副本碎片（默认情况下，只需要主索引，但可以更改此行为）。在这种情况下， total 将等于基于 number_of_replicas 设置的总分片，并且 successful 将等于已启动的分片数（主副本和副本）。如果没有失败， failed 将是 0 。 get apiget api 允许从一个基于其id的 index 中获取一个 JSON格式的 document，下面的示例是从一个在名称为tweet的 type \\下的id为1，名称为twitter的 \\index \\中获取一个JSON格式的 \\document。 1curl -XGET 'http://localhost:9200/twitter/tweet/1' update api12345678910111213141516171819202122232425262728293031323334353637383940# 更新api PUT test/type1/1{ \"counter\" : 1, \"tags\" : [\"red\"]}# 脚本更新POST test/type1/1/_update{ \"script\" : { \"inline\": \"ctx._source.counter += params.count\", \"lang\": \"painless\", \"params\" : { \"count\" : 4 } }}# 将新字段添加到文档：POST test/type1/1/_update{ \"script\" : \"ctx._source.new_field = \\\"value_of_new_field\\\"\"}# 删除字段POST test/type1/1/_update{ \"script\" : \"ctx._source.remove(\\\"new_field\\\")\"}# 甚至可以改变已执行的操作。这个例子就是删除文档，如果 tags包含 green，否则就什么也不做（noop）：POST test/type1/1/_update{ \"script\" : { \"inline\": \"if (ctx._source.tags.contains(params.tag)) { ctx.op = \\\"delete\\\" } else { ctx.op = \\\"none\\\" }\", \"lang\": \"painless\", \"params\" : { \"tag\" : \"green\" } }} 通过查询api更新123456789101112POST twitter/_update_by_query{ \"script\": { \"inline\": \"ctx._source.likes++\", \"lang\": \"painless\" }, \"query\": { \"term\": { \"user\": \"kimchy\" } }} bulk apiBulk API，能够在一个单一的API调用执行多项索引/删除操作。这可以大大提高索引速度。 该 REST API 端点/_bulk，它遵循JSON结构： 1234567action_and_meta_data\\noptional_source\\naction_and_meta_data\\noptional_source\\n....action_and_meta_data\\noptional_source\\n 注意：数据的最终行必须以换行符结束\\n。 可能的操作有 index，create，delete和 update， index 和 `create期望在下一行的作为源，并与索引 API 有相同的语义。（如果文件具有相同的索引和类型的文件已经存在，就会创建失败，必要时候而索引回添加或替换文件）。delete不会作为下一行的源，并与 delete API 中具有相同的语义。update 是希望`部分文档，upsert 和脚本及其选项能够在下一行指定。 delete apidelete API允许基于指定的ID来从索引库中删除一个JSON文件。下面演示了从一个叫twitter的索引库的tweettype下删除文档，id是1: 1$ curl -XDELETE 'http://localhost:9200/twitter/tweet/1' 索引的每个文档都被标记了版本。当删除文档时， 可以通过指定version来确保我们试图删除一个实际上已被删除的文档时，它在此期间并没有改变。在文档中执行的每个写入操作，包括删除，都会使其版本递增。 delete by query api最简单的用法是使用_delete_by_query对每个查询匹配的文档执行删除。这是API: 12345678POST twitter/_delete_by_query{ \"query\": { //① \"match\": { \"message\": \"some message\" } }} term vectors返回有关特定文档字段中的词条的信息和统计信息。文档可以存储在索引中或由用户人工提供。词条向量默认为实时，不是近实时。这可以通过将realtime参数设置为false来更改。 1GET /twitter/tweet/1/_termvectors 可选的，您可以使用url中的参数指定检索信息的字段： 1GET /twitter/tweet/1/_termvectors?fields=message searchquery api结果的分页可以通过使用 from 和 size 参数来完成。 from 参数定义了您要提取的第一个结果的偏移量。 size 参数允许您配置要返回的最大匹配数。 虽然 from 和 size 可以设置为请求参数，但它们也可以在搜索正文中设置。from 默认值为 0，size 默认为 10。 1234567GET /_search{ &quot;from&quot; : 0, &quot;size&quot; : 10, &quot;query&quot; : { &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; } }} 注意 from + size 不能超过 index.max_result_window 索引设置，默认为 10,000。 有关深入滚动的更有效方法，请参阅 Scroll 或 Search After API。 sort排序选项可以有以下值： | asc | 按升序排序 | | desc | 按倒序排序 | 在对 _score 进行排序时，该顺序默认为 desc，在对其他事物进行排序时默认为 asc。 Sort mode Option Elasticsearch支持按数组或多值字段排序。 mode 选项控制选择用于对其所属文档进行排序的数组值。 mode 选项可以具有以下值： | min | 选择最低值。 | | max | 选择最高值。 | | sum | 使用所有值的和作为排序值。 仅适用于基于数字的数组字段。 | | avg | 使用所有值的平均值作为排序值。 仅适用于基于数字的数组字段。 | | median | 使用所有值的中值作为排序值。 仅适用于基于数字的数组字段。 | Elasticsearch 还支持根据一个或多个嵌套对象内的字段进行排序。 通过嵌套字段支持进行的排序在已经存在的排序选项之上具有以下参数： nested_path 定义要排序的嵌套对象。 实际排序字段必须是此嵌套对象内的直接字段。 当通过嵌套字段排序时，此字段是必需的。 nested_filter 嵌套路径中的内部对象应与其匹配的过滤器，以便通过排序考虑其字段值。 常见的情况是在嵌套的过滤器或查询中重复查询/过滤。 默认情况下，没有 nested_filter 是激活的。 Nested sorting example 在下面的示例中，offer是一个类型为嵌套的字段。 需要指定nested_path; 否则，elasticsearch不知道需要捕获哪个嵌套级排序值。 123456789101112131415161718POST /_search{ \"query\" : { \"term\" : { \"product\" : \"chocolate\" } }, \"sort\" : [ { \"offer.price\" : { \"mode\" : \"avg\", \"order\" : \"asc\", \"nested_path\" : \"offer\", \"nested_filter\" : { \"term\" : { \"offer.color\" : \"blue\" } } } } ]} 当通过脚本排序和按地理距离排序时，也支持嵌套排序。 Missing Values 缺少的参数指定应如何处理缺少字段的文档：缺少的值可以设置为 last，first 或自定义值（将用于缺少文档作为排序值）。 123456789GET /_search{ &quot;sort&quot; : [ { &quot;price&quot; : {&quot;missing&quot; : &quot;_last&quot;} } ], &quot;query&quot; : { &quot;term&quot; : { &quot;product&quot; : &quot;chocolate&quot; } }} Note：如果嵌套的内部对象与 nested_filter 不匹配，则使用缺少的值。 Geo Distance Sorting 允许按 geodistance 排序。 下面是一个例子，假设 pin.location 是一个类型为 geo_point 的字段： 1234567891011121314151617GET /_search{ &quot;sort&quot; : [ { &quot;_geo_distance&quot; : { &quot;pin.location&quot; : [-70, 40], &quot;order&quot; : &quot;asc&quot;, &quot;unit&quot; : &quot;km&quot;, &quot;mode&quot; : &quot;min&quot;, &quot;distance_type&quot; : &quot;sloppy_arc&quot; } } ], &quot;query&quot; : { &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; } }} distance_type 如何计算距离。 可以是 sloppy_arc（默认），弧（稍微更精确但显着更慢）或平面（更快，但不准确在长距离和接近极点）。 mode 如果字段有多个地理点，该怎么办。 默认情况下，按升序排序时考虑最短距离，按降序排序时最长距离。 支持的值为 min，max，median 和 avg。 unit 计算排序值时使用的单位。 默认值为 m（米）。 geo distance sorting 不支持可配置的缺失值：当文档没有用于距离计算的字段的值时，距离将始终被视为等于 Infinity。 在提供坐标时支持以下格式： highlighting允许突出显示一个或多个字段的搜索结果。 实现使用 lucene 普通荧光笔，快速向量荧光笔（fvh）或 postings 荧光笔。 以下是一个搜索请求正文的示例： 1234567891011GET /_search{ &quot;query&quot; : { &quot;match&quot;: { &quot;user&quot;: &quot;kimchy&quot; } }, &quot;highlight&quot; : { &quot;fields&quot; : { &quot;content&quot; : {} } }} 在上述情况下，内容字段将为每个搜索命中突出显示（每个搜索命中内将有另一个元素，称为突出显示，其中包括突出显示的字段和突出显示的片段）。 Note： 为了执行突出显示，需要字段的实际内容。 如果有问题的字段被存储（在映射中存储设置为 true），它将被使用，否则，实际的 _source 将被加载，并且相关字段将从中提取。 _all 字段不能从 _source 中提取，因此它只能用于突出显示，如果它映射到将 store 设置为 true。 字段名称支持通配符符号。 例如，使用 comment_ * 将导致所有与表达式匹配的文本和关键字字段（以及 5.0 之前的字符串）被突出显示。 请注意，所有其他字段将不会突出显示。 如果您使用自定义映射器并要在字段上突出显示，则必须显式提供字段名称。 Plain highlighte(有多种类型选择、根据实际情况使用) 荧光笔的默认选择是普通类型，并使用Lucene荧光笔。 它试图在理解词重要性和短语查询中的任何词定位标准方面反映查询匹配逻辑。 warning： 如果你想突出很多文档中的大量字段与复杂的查询，这个荧光笔不会快。 在努力准确地反映查询逻辑，它创建一个微小的内存索引，并通过 Lucene 的查询执行计划程序重新运行原始查询条件，以获取当前文档的低级别匹配信息。 这对于每个字段和需要突出显示的每个文档重复。 如果这在您的系统中出现性能问题，请考虑使用替代荧光笔。 search typeQuery Then Fetch 参数值： query_then_fetch。 请求分两个阶段处理。 在第一阶段，查询被转发到所有涉及的分片。 每个分片执行搜索并生成对该分片本地的结果的排序列表。 每个分片只向协调节点返回足够的信息，以允许其合并并将分片级结果重新排序为全局排序的最大长度大小的结果集。 在第二阶段期间，协调节点仅从相关分片请求文档内容（以及高亮显示的片段，如果有的话）。 Note： 如果您未在请求中指定 search_type，那么这是默认设置。 Dfs, Query Then Fetch 参数值：dfs_query_then_fetch 与 “Query Then Fetch” 相同，除了初始分散阶段，其计算分布项频率用于更准确的计分。 scroll 游标(多数据深度分页问题解决)从滚动请求返回的结果反映了进行初始搜索请求时索引的状态，如时间快照。 对文档（索引，更新或删除）的后续更改只会影响以后的搜索请求。 为了使用滚动，初始搜索请求应该在查询字符串中指定滚动参数，它告诉 Elasticsearch 应保持“搜索上下文”活动的时间（见保持搜索上下文），例如 ?scroll=1m。 123456789POST /twitter/tweet/_search?scroll=1m{ \"size\": 100, \"query\": { \"match\" : { \"title\" : \"elasticsearch\" } }} 上述请求的结果包括一个 scrollid，它应该被传递给滚动 API，以便检索下一批结果。 12345POST ①/_search/scroll ②{ \"scroll\" : \"1m\", ③ \"scroll_id\" : \"DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAD4WYm9laVYtZndUQlNsdDcwakFMNjU1QQ==\" ④} | ① | 可以使用GET或POST。 | | ② | 网址不应包含索引或类型名称 - 而是在原始搜索请求中指定的。 | | ③ | scroll 参数告诉 Elasticsearch 将搜索上下文打开另一个1m。 | | ④ | scroll_id参数 | size 参数允许您配置每批结果返回的最大命中数。 每次调用 scroll API 都会返回下一批结果，直到没有更多结果要返回，即 hits 数组为空。 explan启用每次匹配对其评分计算方式的说明。 1234567GET /_search{ \"explain\": true, \"query\" : { \"term\" : { \"user\" : \"kimchy\" } }} suggesters (共有四种方式) Completion Suggester Context Suggester Phrase Suggester Term suggester completion suggester完全（completion）suggester 提供自动完成/按需搜索功能。 这是一种导航功能，可在用户输入时引导用户查看相关结果，从而提高搜索精度。 它不是用于拼写校正或平均值功能，如术语或短语 suggesters 。 理想地，自动完成功能应当与用户键入的速度一样快，以提供与用户已经键入的内容相关的即时反馈。因此，完成 suggester 针对速度进行优化。 suggester 使用允许快速查找的数据结构，但是构建成本高并且存储在存储器中。 要使用此功能，请为此字段指定一个特殊映射，为快速完成的字段值编制索引。 123456789101112131415PUT music{ \"mappings\": { \"song\" : { \"properties\" : { \"suggest\" : { \"type\" : \"completion\" }, \"title\" : { \"type\": \"keyword\" } } } }} 映射支持以下参数： analyzer 使用索引分析器，默认为简单。 如果你想知道为什么我们没有选择标准分析器：我们尝试在这里很容易理解的行为，如果你索引字段内容在Drive-in，你不会得到任何建议， （第一个非停用词） search_analyzer 要使用的搜索分析器，默认为分析器的值。 preserve_separators 保留分隔符，默认为true。 如果禁用，你可以找到一个以Foo Fighters开头的字段，如果你推荐foof。 preserve_position_increments 启用位置增量，默认为true。 如果禁用和使用停用分析器，您可以得到一个字段从披头士开始，如果你 suggest b。 注意：你也可以通过索引两个输入，Beatles和披头士，不需要改变一个简单的分析器，如果你能够丰富你的数据。 max_input_length 限制单个输入的长度，默认为50个UTF-16代码点。 此限制仅在索引时使用，以减少每个输入字符串的字符总数，以防止大量输入膨胀底层数据结构。 大多数用例不会受默认值的影响，因为前缀完成很少超过前缀长度超过少数几个字符。 索引 您像任何其他字段一样索引 suggestion 。 suggestion 由输入和可选的权重属性组成。 输入是要由 suggestion 查询匹配的期望文本，并且权重确定如何对 suggestion 进行评分。 索引 suggestion 如下： 1234567PUT music/song/1?refresh{ &quot;suggest&quot; : { &quot;input&quot;: [ &quot;Nevermind&quot;, &quot;Nirvana&quot; ], &quot;weight&quot; : 34 }} 以下参数被支持： input 输入存储，这可以是字符串数组或只是一个字符串。 此字段是必填字段。 weight 正整数或包含正整数的字符串，用于定义权重并允许对 suggestions 进行排名。 此字段是可选的。 查询 suggest 像往常一样工作，除了您必须指定 suggest 类型为完成。 suggestions 接近实时，这意味着可以通过刷新显示新 suggestions ，并且一旦删除就不会显示文档。 此请求： 123456789POST music/_suggest?pretty{ &quot;song-suggest&quot; : { &quot;prefix&quot; : &quot;nir&quot;, &quot;completion&quot; : { &quot;field&quot; : &quot;suggest&quot; } }} 返回这个响应： 12345678910111213141516171819202122{ &quot;_shards&quot; : { &quot;total&quot; : 5, &quot;successful&quot; : 5, &quot;failed&quot; : 0 }, &quot;song-suggest&quot; : [ { &quot;text&quot; : &quot;nir&quot;, &quot;offset&quot; : 0, &quot;length&quot; : 3, &quot;options&quot; : [ { &quot;text&quot; : &quot;Nirvana&quot;, &quot;_index&quot;: &quot;music&quot;, &quot;_type&quot;: &quot;song&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1.0, &quot;_source&quot;: { &quot;suggest&quot;: [&quot;Nevermind&quot;, &quot;Nirvana&quot;] } } ] } ]} 模糊查询 完成 suggester 还支持模糊查询 - 这意味着，您可以在搜索中输入错误，并仍然返回结果。 123456789101112POST music/_suggest?pretty{ &quot;song-suggest&quot; : { &quot;prefix&quot; : &quot;nor&quot;, &quot;completion&quot; : { &quot;field&quot; : &quot;suggest&quot;, &quot;fuzzy&quot; : { &quot;fuzziness&quot; : 2 } } }} 与查询前缀共享最长前缀的 suggestion 将得分更高。 模糊查询可以采用特定的模糊参数。 支持以下参数： | fuzziness | 模糊系数，默认为AUTO。 有关允许的设置，请参阅 “Fuzzinessedit”一节。 | | transpositions | 如果设置为true，则换位计数为一个更改而不是两个，默认为true | | min_length | 返回模糊 suggestions 前的输入的最小长度，默认值3 | | prefix_length | 输入的最小长度（未针对模糊替代项进行检查）默认为1 | | unicode_aware | 如果为true，则所有度量（如模糊编辑距离，置换和长度）都以Unicode代码点而不是字节为单位。 这比原始字节稍慢，因此默认情况下设置为false。 | 如果你想坚持使用默认值，但仍然使用模糊，你可以使用 fuzzy：{}或fuzzy：true。 Explan apiExplain API 计算查询和特定文档的分数说明。 这可以提供有用的反馈，无论文档是否匹配特定查询。 index 和 type 参数分别期望单个索引和单个类型。 用法完整查询示例： 123456GET /twitter/tweet/0/_explain{ &quot;query&quot; : { &quot;match&quot; : { &quot;message&quot; : &quot;elasticsearch&quot; } }} 这将产生以下结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748{ &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;tweet&quot;, &quot;_id&quot; : &quot;0&quot;, &quot;matched&quot; : true, &quot;explanation&quot; : { &quot;value&quot; : 1.55077, &quot;description&quot; : &quot;sum of:&quot;, &quot;details&quot; : [ { &quot;value&quot; : 1.55077, &quot;description&quot; : &quot;weight(message:elasticsearch in 0) [PerFieldSimilarity], result of:&quot;, &quot;details&quot; : [ { &quot;value&quot; : 1.55077, &quot;description&quot; : &quot;score(doc=0,freq=1.0 = termFreq=1.0\\n), product of:&quot;, &quot;details&quot; : [ { &quot;value&quot; : 1.3862944, &quot;description&quot; : &quot;idf(docFreq=1, docCount=5)&quot;, &quot;details&quot; : [ ] }, { &quot;value&quot; : 1.1186441, &quot;description&quot; : &quot;tfNorm, computed from:&quot;, &quot;details&quot; : [ { &quot;value&quot; : 1.0, &quot;description&quot; : &quot;termFreq=1.0&quot;, &quot;details&quot; : [ ] }, { &quot;value&quot; : 1.2, &quot;description&quot; : &quot;parameter k1&quot;, &quot;details&quot; : [ ] }, { &quot;value&quot; : 0.75, &quot;description&quot; : &quot;parameter b&quot;, &quot;details&quot; : [ ] }, { &quot;value&quot; : 5.4, &quot;description&quot; : &quot;avgFieldLength&quot;, &quot;details&quot; : [ ] }, { &quot;value&quot; : 4.0, &quot;description&quot; : &quot;fieldLength&quot;, &quot;details&quot; : [ ] } ] } ] } ] }, { &quot;value&quot; : 0.0, &quot;description&quot; : &quot;match on required clause, product of:&quot;, &quot;details&quot; : [ { &quot;value&quot; : 0.0, &quot;description&quot; : &quot;# clause&quot;, &quot;details&quot; : [ ] }, { &quot;value&quot; : 1.0, &quot;description&quot; : &quot;_type:tweet, product of:&quot;, &quot;details&quot; : [ { &quot;value&quot; : 1.0, &quot;description&quot; : &quot;boost&quot;, &quot;details&quot; : [ ] }, { &quot;value&quot; : 1.0, &quot;description&quot; : &quot;queryNorm&quot;, &quot;details&quot; : [ ] } ] } ] } ] }} 还有一种更简单的通过 q 参数指定查询的方法。 然后解析指定的 q 参数值，就像使用 query_string 查询一样。 在api中的 q 参数的用法示例： 1GET /twitter/tweet/0/_explain?q=message:search 这将产生与先前请求相同的结果。 参考资料","link":"/2019/04/26/Elasticsearch常用工具api.html"},{"title":"restful api 设计以及幂等性相关设计","text":"摘要针对项目中部分使用restful-api接口，总结文档如下，没有规矩不成方圆。写代码亦是，设计restful-api接口亦是。 RESTful 的核心思想就是，客户端发出的数据操作指令都是”动词 + 宾语”的结构。比如，GET /articles这个命令，GET是动词，/articles是宾语。 动词通常就是五种 HTTP 方法，对应 CRUD 操作。 GET：读取（Read） POST：新建（Create） PUT：更新（Update） PATCH：更新（Update），通常是部分更新 DELETE：删除（Delete） 根据 HTTP 规范，动词一律大写 话不多说，先上代码，一份完整的restful-api示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445@RestController@RequestMapping(\"admin/biz-name/v1/user\") //@RequestMapping(\"api/biz-name/v1/city\")public class AdminUserController { @Autowired private UserService service; @GetMapping(\"{id}/county/list\") public List&lt;UserView&gt; getUsersByCategoryId(@PathVariable(\"id\") Integer id) { return service.getUsersByCategoryId(id); } @GetMapping(\"{id}\") public UserView userInfoById(@PathVariable(\"id\") Integer id) { return service.getUserInfoById(id); } @PostMapping public Object create(@RequestBody UserEntityForm form) { // 相关验证valid 设置默认值 service.valid(form); if (StringUtil.isEmpty(form.getCsEmail())) { form.setCsEmail(\"980099577@qq.com\"); } Integer id = service.ceate(form); return Collections.singletonMap(\"id\", id); } @PutMapping public Result update(@RequestBody UserEntityForm form) { // 相关验证 valid service.valid(form); service.update(form); return Result.SUCCEED; } @DeleteMapping(\"{id}\") public Result delete(@PathVariable(\"id\") Integer id) { // 相关验证 valid service.valid(form); service.delete(id); return Result.SUCCEED; }} 针对以上restful-api 代码需要注意一下几点：@RestController 注解此注解加在类的上面，返回的结果以json格式，标注此为RestController。不同于Controller注解。Controller非json格式返回。 @RequestMapping此注解标识api请求的路劲，可指定相应的请求方法，例如@RequestMapping(“admin/biz-name/v1/user”)：http://localhost:8080/admin/biz-name/v1/user 有如下定义参数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146@Target({ElementType.METHOD, ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Mappingpublic @interface RequestMapping { /** * Assign a name to this mapping. * &lt;p&gt;&lt;b&gt;Supported at the type level as well as at the method level!&lt;/b&gt; * When used on both levels, a combined name is derived by concatenation * with \"#\" as separator. * @see org.springframework.web.servlet.mvc.method.annotation.MvcUriComponentsBuilder * @see org.springframework.web.servlet.handler.HandlerMethodMappingNamingStrategy */ String name() default \"\"; /** * The primary mapping expressed by this annotation. * &lt;p&gt;In a Servlet environment this is an alias for {@link #path}. * For example {@code @RequestMapping(\"/foo\")} is equivalent to * {@code @RequestMapping(path=\"/foo\")}. * &lt;p&gt;In a Portlet environment this is the mapped portlet modes * (i.e. \"EDIT\", \"VIEW\", \"HELP\" or any custom modes). * &lt;p&gt;&lt;b&gt;Supported at the type level as well as at the method level!&lt;/b&gt; * When used at the type level, all method-level mappings inherit * this primary mapping, narrowing it for a specific handler method. */ @AliasFor(\"path\") String[] value() default {}; /** * In a Servlet environment only: the path mapping URIs (e.g. \"/myPath.do\"). * Ant-style path patterns are also supported (e.g. \"/myPath/*.do\"). * At the method level, relative paths (e.g. \"edit.do\") are supported within * the primary mapping expressed at the type level. Path mapping URIs may * contain placeholders (e.g. \"/${connect}\") * &lt;p&gt;&lt;b&gt;Supported at the type level as well as at the method level!&lt;/b&gt; * When used at the type level, all method-level mappings inherit * this primary mapping, narrowing it for a specific handler method. * @see org.springframework.web.bind.annotation.ValueConstants#DEFAULT_NONE * @since 4.2 */ @AliasFor(\"value\") String[] path() default {}; /** * The HTTP request methods to map to, narrowing the primary mapping: * GET, POST, HEAD, OPTIONS, PUT, PATCH, DELETE, TRACE. * &lt;p&gt;&lt;b&gt;Supported at the type level as well as at the method level!&lt;/b&gt; * When used at the type level, all method-level mappings inherit * this HTTP method restriction (i.e. the type-level restriction * gets checked before the handler method is even resolved). * &lt;p&gt;Supported for Servlet environments as well as Portlet 2.0 environments. */ RequestMethod[] method() default {}; /** * The parameters of the mapped request, narrowing the primary mapping. * &lt;p&gt;Same format for any environment: a sequence of \"myParam=myValue\" style * expressions, with a request only mapped if each such parameter is found * to have the given value. Expressions can be negated by using the \"!=\" operator, * as in \"myParam!=myValue\". \"myParam\" style expressions are also supported, * with such parameters having to be present in the request (allowed to have * any value). Finally, \"!myParam\" style expressions indicate that the * specified parameter is &lt;i&gt;not&lt;/i&gt; supposed to be present in the request. * &lt;p&gt;&lt;b&gt;Supported at the type level as well as at the method level!&lt;/b&gt; * When used at the type level, all method-level mappings inherit * this parameter restriction (i.e. the type-level restriction * gets checked before the handler method is even resolved). * &lt;p&gt;In a Servlet environment, parameter mappings are considered as restrictions * that are enforced at the type level. The primary path mapping (i.e. the * specified URI value) still has to uniquely identify the target handler, with * parameter mappings simply expressing preconditions for invoking the handler. * &lt;p&gt;In a Portlet environment, parameters are taken into account as mapping * differentiators, i.e. the primary portlet mode mapping plus the parameter * conditions uniquely identify the target handler. Different handlers may be * mapped onto the same portlet mode, as long as their parameter mappings differ. */ String[] params() default {}; /** * The headers of the mapped request, narrowing the primary mapping. * &lt;p&gt;Same format for any environment: a sequence of \"My-Header=myValue\" style * expressions, with a request only mapped if each such header is found * to have the given value. Expressions can be negated by using the \"!=\" operator, * as in \"My-Header!=myValue\". \"My-Header\" style expressions are also supported, * with such headers having to be present in the request (allowed to have * any value). Finally, \"!My-Header\" style expressions indicate that the * specified header is &lt;i&gt;not&lt;/i&gt; supposed to be present in the request. * &lt;p&gt;Also supports media type wildcards (*), for headers such as Accept * and Content-Type. For instance, * &lt;pre class=\"code\"&gt; * &amp;#064;RequestMapping(value = \"/something\", headers = \"content-type=text/*\") * &lt;/pre&gt; * will match requests with a Content-Type of \"text/html\", \"text/plain\", etc. * &lt;p&gt;&lt;b&gt;Supported at the type level as well as at the method level!&lt;/b&gt; * When used at the type level, all method-level mappings inherit * this header restriction (i.e. the type-level restriction * gets checked before the handler method is even resolved). * &lt;p&gt;Maps against HttpServletRequest headers in a Servlet environment, * and against PortletRequest properties in a Portlet 2.0 environment. * @see org.springframework.http.MediaType */ String[] headers() default {}; /** * The consumable media types of the mapped request, narrowing the primary mapping. * &lt;p&gt;The format is a single media type or a sequence of media types, * with a request only mapped if the {@code Content-Type} matches one of these media types. * Examples: * &lt;pre class=\"code\"&gt; * consumes = \"text/plain\" * consumes = {\"text/plain\", \"application/*\"} * &lt;/pre&gt; * Expressions can be negated by using the \"!\" operator, as in \"!text/plain\", which matches * all requests with a {@code Content-Type} other than \"text/plain\". * &lt;p&gt;&lt;b&gt;Supported at the type level as well as at the method level!&lt;/b&gt; * When used at the type level, all method-level mappings override * this consumes restriction. * @see org.springframework.http.MediaType * @see javax.servlet.http.HttpServletRequest#getContentType() */ String[] consumes() default {}; /** * The producible media types of the mapped request, narrowing the primary mapping. * &lt;p&gt;The format is a single media type or a sequence of media types, * with a request only mapped if the {@code Accept} matches one of these media types. * Examples: * &lt;pre class=\"code\"&gt; * produces = \"text/plain\" * produces = {\"text/plain\", \"application/*\"} * produces = \"application/json; charset=UTF-8\" * &lt;/pre&gt; * &lt;p&gt;It affects the actual content type written, for example to produce a JSON response * with UTF-8 encoding, {@code \"application/json; charset=UTF-8\"} should be used. * &lt;p&gt;Expressions can be negated by using the \"!\" operator, as in \"!text/plain\", which matches * all requests with a {@code Accept} other than \"text/plain\". * &lt;p&gt;&lt;b&gt;Supported at the type level as well as at the method level!&lt;/b&gt; * When used at the type level, all method-level mappings override * this produces restriction. * @see org.springframework.http.MediaType */ String[] produces() default {};} @GetMapping此注解，接收前端为get请求的相关方法,接口返回天生具有幂等性，每次请求参数一致返回的结果也一致 此注解请求一般为获取相关信息的方法 有如下参数， 12345678910111213141516171819202122232425262728293031323334353637383940414243@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)@Documented@RequestMapping(method = RequestMethod.GET)public @interface GetMapping { /** * Alias for {@link RequestMapping#name}. */ @AliasFor(annotation = RequestMapping.class) String name() default \"\"; /** * Alias for {@link RequestMapping#value}. */ @AliasFor(annotation = RequestMapping.class) String[] value() default {}; /** * Alias for {@link RequestMapping#path}. */ @AliasFor(annotation = RequestMapping.class) String[] path() default {}; /** * Alias for {@link RequestMapping#params}. */ @AliasFor(annotation = RequestMapping.class) String[] params() default {}; /** * Alias for {@link RequestMapping#headers}. */ @AliasFor(annotation = RequestMapping.class) String[] headers() default {}; /** * Alias for {@link RequestMapping#produces}. */ @AliasFor(annotation = RequestMapping.class) String[] produces() default {};} @PostMapping此注解，接收前端为post请求的相关方法 一般为保存，创建信息的方法，save…,create… @putMapping此注解，接收前端为put请求的相关方法 一般为更新数据的接口方法，update… @deleteMapping此注解，接收前端为delete请求的相关方法 一般为删除数据的方法 @RequestBody此注解放入接口方法的参数前面，对应请求中的body里的内容，要求body内容为json格式 例如： 1public Object create(@RequestBody UserEntityForm form) {}; @PathVariable此注解放入接口方法的参数前面，要求里面的值出现在Mapping中，以{}包裹，如下 接受参数required，设置判断此字段是否必须 12@GetMapping(\"{id}\")public UserView userInfoById(@PathVariable(\"id\",required = true) Integer id) {} @RequestParam此注解放入接口方法的参数前面，前端放入请求参数中，非body里面 接受参数required，设置判断此字段是否必须 defaultValue 设置此字段的默认值 1public UserView userInfoById(@RequestParam(value = \"id\", required = false, defaultValue = \"1\") Integer id）{} 所有的接口方法都应该设计为幂等性，即接口请求多次或一次都应该达到同样的效果。此特性在高并发下面非常适用。 高并发的核心技术 - 幂等的实现方案一、背景我们实际系统中有很多操作，是不管做多少次，都应该产生一样的效果或返回一样的结果。 例如： 前端重复提交选中的数据，应该后台只产生对应这个数据的一个反应结果。 2. 我们发起一笔付款请求，应该只扣用户账户一次钱，当遇到网络重发或系统bug重发，也应该只扣一次钱； 3. 发送消息，也应该只发一次，同样的短信发给用户，用户会哭的； 4. 创建业务订单，一次业务请求只能创建一个，创建多个就会出大问题。 等等很多重要的情况，这些逻辑都需要幂等的特性来支持。 二、幂等性概念幂等（idempotent、idempotence）是一个数学与计算机学概念，常见于抽象代数中。 在编程中.一个幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。幂等函数，或幂等方法，是指可以使用相同参数重复执行，并能获得相同结果的函数。这些函数不会影响系统状态，也不用担心重复执行会对系统造成改变。例如，“getUsername()和setTrue()”函数就是一个幂等函数. 更复杂的操作幂等保证是利用唯一交易号(流水号)实现. 我的理解：幂等就是一个操作，不论执行多少次，产生的效果和返回的结果都是一样的 三、技术方案 查询操作 查询一次和查询多次，在数据不变的情况下，查询结果是一样的。select是天然的幂等操作 删除操作 删除操作也是幂等的，删除一次和多次删除都是把数据删除。(注意可能返回结果不一样，删除的数据不存在，返回0，删除的数据多条，返回结果多个) 3.唯一索引，防止新增脏数据 比如：支付宝的资金账户，支付宝也有用户账户，每个用户只能有一个资金账户，怎么防止给用户创建资金账户多个，那么给资金账户表中的用户ID加唯一索引，所以一个用户新增成功一个资金账户记录 要点： 唯一索引或唯一组合索引来防止新增数据存在脏数据 （当表存在唯一索引，并发时新增报错时，再查询一次就可以了，数据应该已经存在了，返回结果即可） token机制，防止页面重复提交 业务要求： 页面的数据只能被点击提交一次 发生原因： 由于重复点击或者网络重发，或者nginx重发等情况会导致数据被重复提交 解决办法： 集群环境：采用token加redis（redis单线程的，处理需要排队） 单JVM环境：采用token加redis或token加jvm内存 处理流程： 1. 数据提交前要向服务的申请token，token放到redis或jvm内存，token有效时间 2. 提交后后台校验token，同时删除token，生成新的token返回 token特点： 要申请，一次有效性，可以限流 注意：redis要用删除操作来判断token，删除成功代表token校验通过，如果用select+delete来校验token，存在并发问题，不建议使用 悲观锁 获取数据的时候加锁获取 select * from table_xxx where id=’xxx’ for update; 注意：id字段一定是主键或者唯一索引，不然是锁表，会死人的 悲观锁使用时一般伴随事务一起使用，数据锁定时间可能会很长，根据实际情况选用 乐观锁 乐观锁只是在更新数据那一刻锁表，其他时间不锁表，所以相对于悲观锁，效率更高。 乐观锁的实现方式多种多样可以通过version或者其他状态条件： 1. 通过版本号实现 update table_xxx set name=#name#,version=version+1 where version=#version# 如下图(来自网上)： 通过条件限制 update tablexxx set avaiamount=avaiamount-#subAmount# where avaiamount-#subAmount# &gt;= 0 要求：quality-#subQuality# &gt;= ，这个情景适合不用版本号，只更新是做数据安全校验，适合库存模型，扣份额和回滚份额，性能更高 注意：乐观锁的更新操作，最好用主键或者唯一索引来更新,这样是行锁，否则更新时会锁表，上面两个sql改成下面的两个更好 update tablexxx set name=#name#,version=version+1 where id=#id# and version=#version# update tablexxx set avaiamount=avaiamount-#subAmount# where id=#id# and avai_amount-#subAmount# &gt;= 0 分布式锁 还是拿插入数据的例子，如果是分布是系统，构建全局唯一索引比较困难，例如唯一性的字段没法确定，这时候可以引入分布式锁，通过第三方的系统(redis或zookeeper)，在业务系统插入数据或者更新数据，获取分布式锁，然后做操作，之后释放锁，这样其实是把多线程并发的锁的思路，引入多多个系统，也就是分布式系统中得解决思路。 要点：某个长流程处理过程要求不能并发执行，可以在流程执行之前根据某个标志(用户ID+后缀等)获取分布式锁，其他流程执行时获取锁就会失败，也就是同一时间该流程只能有一个能执行成功，执行完成后，释放分布式锁(分布式锁要第三方系统提供) select + insert 并发不高的后台系统，或者一些任务JOB，为了支持幂等，支持重复执行，简单的处理方法是，先查询下一些关键数据，判断是否已经执行过，在进行业务处理，就可以了 注意：核心高并发流程不要用这种方法 状态机幂等 在设计单据相关的业务，或者是任务相关的业务，肯定会涉及到状态机(状态变更图)，就是业务单据上面有个状态，状态在不同的情况下会发生变更，一般情况下存在有限状态机，这时候，如果状态机已经处于下一个状态，这时候来了一个上一个状态的变更，理论上是不能够变更的，这样的话，保证了有限状态机的幂等。 注意：订单等单据类业务，存在很长的状态流转，一定要深刻理解状态机，对业务系统设计能力提高有很大帮助 对外提供接口的api如何保证幂等 如银联提供的付款接口：需要接入商户提交付款请求时附带：source来源，seq序列号 source+seq在数据库里面做唯一索引，防止多次付款，(并发时，只能处理一个请求) 重点对外提供接口为了支持幂等调用，接口有两个字段必须传，一个是来源source，一个是来源方序列号seq，这个两个字段在提供方系统里面做联合唯一索引，这样当第三方调用时，先在本方系统里面查询一下，是否已经处理过，返回相应处理结果；没有处理过，进行相应处理，返回结果。注意，为了幂等友好，一定要先查询一下，是否处理过该笔业务，不查询直接插入业务系统，会报错，但实际已经处理了。 总结幂等性应该是合格程序员的一个基因，在设计系统时，是首要考虑的问题，尤其是在像支付宝，银行，互联网金融公司等涉及的都是钱的系统，既要高效，数据也要准确，所以不能出现多扣款，多打款等问题，这样会很难处理，用户体验也不好 参考自","link":"/2019/04/18/restful-api-设计以及幂等性相关设计.html"},{"title":"一千行 MySQL 学习笔记(转载)","text":"摘要以下为本人初学 MySQL 时做的笔记，也从那时起没再更新过，但还是囊括了基本的知识点，有时还翻出来查查。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989990991992993994995996997998999100010011002100310041005100610071008100910101011101210131014101510161017101810191020102110221023102410251026102710281029103010311032103310341035103610371038103910401041104210431044/* Windows服务 */-- 启动MySQL net start mysql-- 创建Windows服务 sc create mysql binPath= mysqld_bin_path(注意：等号与值之间有空格)/* 连接与断开服务器 */mysql -h 地址 -P 端口 -u 用户名 -p 密码SHOW PROCESSLIST -- 显示哪些线程正在运行SHOW VARIABLES -- 显示系统变量信息/* 数据库操作 */ -------------------- 查看当前数据库 SELECT DATABASE();-- 显示当前时间、用户名、数据库版本 SELECT now(), user(), version();-- 创建库 CREATE DATABASE[ IF NOT EXISTS] 数据库名 数据库选项 数据库选项： CHARACTER SET charset_name COLLATE collation_name-- 查看已有库 SHOW DATABASES[ LIKE 'PATTERN']-- 查看当前库信息 SHOW CREATE DATABASE 数据库名-- 修改库的选项信息 ALTER DATABASE 库名 选项信息-- 删除库 DROP DATABASE[ IF EXISTS] 数据库名 同时删除该数据库相关的目录及其目录内容/* 表的操作 */ -------------------- 创建表 CREATE [TEMPORARY] TABLE[ IF NOT EXISTS] [库名.]表名 ( 表的结构定义 )[ 表选项] 每个字段必须有数据类型 最后一个字段后不能有逗号 TEMPORARY 临时表，会话结束时表自动消失 对于字段的定义： 字段名 数据类型 [NOT NULL | NULL] [DEFAULT default_value] [AUTO_INCREMENT] [UNIQUE [KEY] | [PRIMARY] KEY] [COMMENT 'string']-- 表选项 -- 字符集 CHARSET = charset_name 如果表没有设定，则使用数据库字符集 -- 存储引擎 ENGINE = engine_name 表在管理数据时采用的不同的数据结构，结构不同会导致处理方式、提供的特性操作等不同 常见的引擎：InnoDB MyISAM Memory/Heap BDB Merge Example CSV MaxDB Archive 不同的引擎在保存表的结构和数据时采用不同的方式 MyISAM表文件含义：.frm表定义，.MYD表数据，.MYI表索引 InnoDB表文件含义：.frm表定义，表空间数据和日志文件 SHOW ENGINES -- 显示存储引擎的状态信息 SHOW ENGINE 引擎名 {LOGS|STATUS} -- 显示存储引擎的日志或状态信息 -- 自增起始数 AUTO_INCREMENT = 行数 -- 数据文件目录 DATA DIRECTORY = '目录' -- 索引文件目录 INDEX DIRECTORY = '目录' -- 表注释 COMMENT = 'string' -- 分区选项 PARTITION BY ... (详细见手册)-- 查看所有表 SHOW TABLES[ LIKE 'pattern'] SHOW TABLES FROM 表名-- 查看表机构 SHOW CREATE TABLE 表名 （信息更详细） DESC 表名 / DESCRIBE 表名 / EXPLAIN 表名 / SHOW COLUMNS FROM 表名 [LIKE 'PATTERN'] SHOW TABLE STATUS [FROM db_name] [LIKE 'pattern']-- 修改表 -- 修改表本身的选项 ALTER TABLE 表名 表的选项 eg: ALTER TABLE 表名 ENGINE=MYISAM; -- 对表进行重命名 RENAME TABLE 原表名 TO 新表名 RENAME TABLE 原表名 TO 库名.表名 （可将表移动到另一个数据库） -- RENAME可以交换两个表名 -- 修改表的字段机构（13.1.2. ALTER TABLE语法） ALTER TABLE 表名 操作名 -- 操作名 ADD[ COLUMN] 字段定义 -- 增加字段 AFTER 字段名 -- 表示增加在该字段名后面 FIRST -- 表示增加在第一个 ADD PRIMARY KEY(字段名) -- 创建主键 ADD UNIQUE [索引名] (字段名)-- 创建唯一索引 ADD INDEX [索引名] (字段名) -- 创建普通索引 DROP[ COLUMN] 字段名 -- 删除字段 MODIFY[ COLUMN] 字段名 字段属性 -- 支持对字段属性进行修改，不能修改字段名(所有原有属性也需写上) CHANGE[ COLUMN] 原字段名 新字段名 字段属性 -- 支持对字段名修改 DROP PRIMARY KEY -- 删除主键(删除主键前需删除其AUTO_INCREMENT属性) DROP INDEX 索引名 -- 删除索引 DROP FOREIGN KEY 外键 -- 删除外键-- 删除表 DROP TABLE[ IF EXISTS] 表名 ...-- 清空表数据 TRUNCATE [TABLE] 表名-- 复制表结构 CREATE TABLE 表名 LIKE 要复制的表名-- 复制表结构和数据 CREATE TABLE 表名 [AS] SELECT * FROM 要复制的表名-- 检查表是否有错误 CHECK TABLE tbl_name [, tbl_name] ... [option] ...-- 优化表 OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ...-- 修复表 REPAIR [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... [QUICK] [EXTENDED] [USE_FRM]-- 分析表 ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] .../* 数据操作 */ -------------------- 增 INSERT [INTO] 表名 [(字段列表)] VALUES (值列表)[, (值列表), ...] -- 如果要插入的值列表包含所有字段并且顺序一致，则可以省略字段列表。 -- 可同时插入多条数据记录！ REPLACE 与 INSERT 完全一样，可互换。 INSERT [INTO] 表名 SET 字段名=值[, 字段名=值, ...]-- 查 SELECT 字段列表 FROM 表名[ 其他子句] -- 可来自多个表的多个字段 -- 其他子句可以不使用 -- 字段列表可以用*代替，表示所有字段-- 删 DELETE FROM 表名[ 删除条件子句] 没有条件子句，则会删除全部-- 改 UPDATE 表名 SET 字段名=新值[, 字段名=新值] [更新条件]/* 字符集编码 */ -------------------- MySQL、数据库、表、字段均可设置编码-- 数据编码与客户端编码不需一致SHOW VARIABLES LIKE 'character_set_%' -- 查看所有字符集编码项 character_set_client 客户端向服务器发送数据时使用的编码 character_set_results 服务器端将结果返回给客户端所使用的编码 character_set_connection 连接层编码SET 变量名 = 变量值 SET character_set_client = gbk; SET character_set_results = gbk; SET character_set_connection = gbk;SET NAMES GBK; -- 相当于完成以上三个设置-- 校对集 校对集用以排序 SHOW CHARACTER SET [LIKE 'pattern']/SHOW CHARSET [LIKE 'pattern'] 查看所有字符集 SHOW COLLATION [LIKE 'pattern'] 查看所有校对集 CHARSET 字符集编码 设置字符集编码 COLLATE 校对集编码 设置校对集编码/* 数据类型（列类型） */ ------------------1. 数值类型-- a. 整型 ---------- 类型 字节 范围（有符号位） tinyint 1字节 -128 ~ 127 无符号位：0 ~ 255 smallint 2字节 -32768 ~ 32767 mediumint 3字节 -8388608 ~ 8388607 int 4字节 bigint 8字节 int(M) M表示总位数 - 默认存在符号位，unsigned 属性修改 - 显示宽度，如果某个数不够定义字段时设置的位数，则前面以0补填，zerofill 属性修改 例：int(5) 插入一个数'123'，补填后为'00123' - 在满足要求的情况下，越小越好。 - 1表示bool值真，0表示bool值假。MySQL没有布尔类型，通过整型0和1表示。常用tinyint(1)表示布尔型。-- b. 浮点型 ---------- 类型 字节 范围 float(单精度) 4字节 double(双精度) 8字节 浮点型既支持符号位 unsigned 属性，也支持显示宽度 zerofill 属性。 不同于整型，前后均会补填0. 定义浮点型时，需指定总位数和小数位数。 float(M, D) double(M, D) M表示总位数，D表示小数位数。 M和D的大小会决定浮点数的范围。不同于整型的固定范围。 M既表示总位数（不包括小数点和正负号），也表示显示宽度（所有显示符号均包括）。 支持科学计数法表示。 浮点数表示近似值。-- c. 定点数 ---------- decimal -- 可变长度 decimal(M, D) M也表示总位数，D表示小数位数。 保存一个精确的数值，不会发生数据的改变，不同于浮点数的四舍五入。 将浮点数转换为字符串来保存，每9位数字保存为4个字节。2. 字符串类型-- a. char, varchar ---------- char 定长字符串，速度快，但浪费空间 varchar 变长字符串，速度慢，但节省空间 M表示能存储的最大长度，此长度是字符数，非字节数。 不同的编码，所占用的空间不同。 char,最多255个字符，与编码无关。 varchar,最多65535字符，与编码有关。 一条有效记录最大不能超过65535个字节。 utf8 最大为21844个字符，gbk 最大为32766个字符，latin1 最大为65532个字符 varchar 是变长的，需要利用存储空间保存 varchar 的长度，如果数据小于255个字节，则采用一个字节来保存长度，反之需要两个字节来保存。 varchar 的最大有效长度由最大行大小和使用的字符集确定。 最大有效长度是65532字节，因为在varchar存字符串时，第一个字节是空的，不存在任何数据，然后还需两个字节来存放字符串的长度，所以有效长度是64432-1-2=65532字节。 例：若一个表定义为 CREATE TABLE tb(c1 int, c2 char(30), c3 varchar(N)) charset=utf8; 问N的最大值是多少？ 答：(65535-1-2-4-30*3)/3-- b. blob, text ---------- blob 二进制字符串（字节字符串） tinyblob, blob, mediumblob, longblob text 非二进制字符串（字符字符串） tinytext, text, mediumtext, longtext text 在定义时，不需要定义长度，也不会计算总长度。 text 类型在定义时，不可给default值-- c. binary, varbinary ---------- 类似于char和varchar，用于保存二进制字符串，也就是保存字节字符串而非字符字符串。 char, varchar, text 对应 binary, varbinary, blob.3. 日期时间类型 一般用整型保存时间戳，因为PHP可以很方便的将时间戳进行格式化。 datetime 8字节 日期及时间 1000-01-01 00:00:00 到 9999-12-31 23:59:59 date 3字节 日期 1000-01-01 到 9999-12-31 timestamp 4字节 时间戳 19700101000000 到 2038-01-19 03:14:07 time 3字节 时间 -838:59:59 到 838:59:59 year 1字节 年份 1901 - 2155datetime YYYY-MM-DD hh:mm:sstimestamp YY-MM-DD hh:mm:ss YYYYMMDDhhmmss YYMMDDhhmmss YYYYMMDDhhmmss YYMMDDhhmmssdate YYYY-MM-DD YY-MM-DD YYYYMMDD YYMMDD YYYYMMDD YYMMDDtime hh:mm:ss hhmmss hhmmssyear YYYY YY YYYY YY4. 枚举和集合-- 枚举(enum) ----------enum(val1, val2, val3...) 在已知的值中进行单选。最大数量为65535. 枚举值在保存时，以2个字节的整型(smallint)保存。每个枚举值，按保存的位置顺序，从1开始逐一递增。 表现为字符串类型，存储却是整型。 NULL值的索引是NULL。 空字符串错误值的索引值是0。-- 集合（set） ----------set(val1, val2, val3...) create table tab ( gender set('男', '女', '无') ); insert into tab values ('男, 女'); 最多可以有64个不同的成员。以bigint存储，共8个字节。采取位运算的形式。 当创建表时，SET成员值的尾部空格将自动被删除。/* 选择类型 */-- PHP角度1. 功能满足2. 存储空间尽量小，处理效率更高3. 考虑兼容问题-- IP存储 ----------1. 只需存储，可用字符串2. 如果需计算，查找等，可存储为4个字节的无符号int，即unsigned 1) PHP函数转换 ip2long可转换为整型，但会出现携带符号问题。需格式化为无符号的整型。 利用sprintf函数格式化字符串 sprintf(\"%u\", ip2long('192.168.3.134')); 然后用long2ip将整型转回IP字符串 2) MySQL函数转换(无符号整型，UNSIGNED) INET_ATON('127.0.0.1') 将IP转为整型 INET_NTOA(2130706433) 将整型转为IP/* 列属性（列约束） */ ------------------1. PRIMARY 主键 - 能唯一标识记录的字段，可以作为主键。 - 一个表只能有一个主键。 - 主键具有唯一性。 - 声明字段时，用 primary key 标识。 也可以在字段列表之后声明 例：create table tab ( id int, stu varchar(10), primary key (id)); - 主键字段的值不能为null。 - 主键可以由多个字段共同组成。此时需要在字段列表后声明的方法。 例：create table tab ( id int, stu varchar(10), age int, primary key (stu, age));2. UNIQUE 唯一索引（唯一约束） 使得某字段的值也不能重复。3. NULL 约束 null不是数据类型，是列的一个属性。 表示当前列是否可以为null，表示什么都没有。 null, 允许为空。默认。 not null, 不允许为空。 insert into tab values (null, 'val'); -- 此时表示将第一个字段的值设为null, 取决于该字段是否允许为null4. DEFAULT 默认值属性 当前字段的默认值。 insert into tab values (default, 'val'); -- 此时表示强制使用默认值。 create table tab ( add_time timestamp default current_timestamp ); -- 表示将当前时间的时间戳设为默认值。 current_date, current_time5. AUTO_INCREMENT 自动增长约束 自动增长必须为索引（主键或unique） 只能存在一个字段为自动增长。 默认为1开始自动增长。可以通过表属性 auto_increment = x进行设置，或 alter table tbl auto_increment = x;6. COMMENT 注释 例：create table tab ( id int ) comment '注释内容';7. FOREIGN KEY 外键约束 用于限制主表与从表数据完整性。 alter table t1 add constraint `t1_t2_fk` foreign key (t1_id) references t2(id); -- 将表t1的t1_id外键关联到表t2的id字段。 -- 每个外键都有一个名字，可以通过 constraint 指定 存在外键的表，称之为从表（子表），外键指向的表，称之为主表（父表）。 作用：保持数据一致性，完整性，主要目的是控制存储在外键表（从表）中的数据。 MySQL中，可以对InnoDB引擎使用外键约束： 语法： foreign key (外键字段） references 主表名 (关联字段) [主表记录删除时的动作] [主表记录更新时的动作] 此时需要检测一个从表的外键需要约束为主表的已存在的值。外键在没有关联的情况下，可以设置为null.前提是该外键列，没有not null。 可以不指定主表记录更改或更新时的动作，那么此时主表的操作被拒绝。 如果指定了 on update 或 on delete：在删除或更新时，有如下几个操作可以选择： 1. cascade，级联操作。主表数据被更新（主键值更新），从表也被更新（外键值更新）。主表记录被删除，从表相关记录也被删除。 2. set null，设置为null。主表数据被更新（主键值更新），从表的外键被设置为null。主表记录被删除，从表相关记录外键被设置成null。但注意，要求该外键列，没有not null属性约束。 3. restrict，拒绝父表删除和更新。 注意，外键只被InnoDB存储引擎所支持。其他引擎是不支持的。/* 建表规范 */ ------------------ -- Normal Format, NF - 每个表保存一个实体信息 - 每个具有一个ID字段作为主键 - ID主键 + 原子表 -- 1NF, 第一范式 字段不能再分，就满足第一范式。 -- 2NF, 第二范式 满足第一范式的前提下，不能出现部分依赖。 消除符合主键就可以避免部分依赖。增加单列关键字。 -- 3NF, 第三范式 满足第二范式的前提下，不能出现传递依赖。 某个字段依赖于主键，而有其他字段依赖于该字段。这就是传递依赖。 将一个实体信息的数据放在一个表内实现。/* SELECT */ ------------------SELECT [ALL|DISTINCT] select_expr FROM -&gt; WHERE -&gt; GROUP BY [合计函数] -&gt; HAVING -&gt; ORDER BY -&gt; LIMITa. select_expr -- 可以用 * 表示所有字段。 select * from tb; -- 可以使用表达式（计算公式、函数调用、字段也是个表达式） select stu, 29+25, now() from tb; -- 可以为每个列使用别名。适用于简化列标识，避免多个列标识符重复。 - 使用 as 关键字，也可省略 as. select stu+10 as add10 from tb;b. FROM 子句 用于标识查询来源。 -- 可以为表起别名。使用as关键字。 SELECT * FROM tb1 AS tt, tb2 AS bb; -- from子句后，可以同时出现多个表。 -- 多个表会横向叠加到一起，而数据会形成一个笛卡尔积。 SELECT * FROM tb1, tb2; -- 向优化符提示如何选择索引 USE INDEX、IGNORE INDEX、FORCE INDEX SELECT * FROM table1 USE INDEX (key1,key2) WHERE key1=1 AND key2=2 AND key3=3; SELECT * FROM table1 IGNORE INDEX (key3) WHERE key1=1 AND key2=2 AND key3=3;c. WHERE 子句 -- 从from获得的数据源中进行筛选。 -- 整型1表示真，0表示假。 -- 表达式由运算符和运算数组成。 -- 运算数：变量（字段）、值、函数返回值 -- 运算符： =, &lt;=&gt;, &lt;&gt;, !=, &lt;=, &lt;, &gt;=, &gt;, !, &amp;&amp;, ||, in (not) null, (not) like, (not) in, (not) between and, is (not), and, or, not, xor is/is not 加上ture/false/unknown，检验某个值的真假 &lt;=&gt;与&lt;&gt;功能相同，&lt;=&gt;可用于null比较d. GROUP BY 子句, 分组子句 GROUP BY 字段/别名 [排序方式] 分组后会进行排序。升序：ASC，降序：DESC 以下[合计函数]需配合 GROUP BY 使用： count 返回不同的非NULL值数目 count(*)、count(字段) sum 求和 max 求最大值 min 求最小值 avg 求平均值 group_concat 返回带有来自一个组的连接的非NULL值的字符串结果。组内字符串连接。e. HAVING 子句，条件子句 与 where 功能、用法相同，执行时机不同。 where 在开始时执行检测数据，对原数据进行过滤。 having 对筛选出的结果再次进行过滤。 having 字段必须是查询出来的，where 字段必须是数据表存在的。 where 不可以使用字段的别名，having 可以。因为执行WHERE代码时，可能尚未确定列值。 where 不可以使用合计函数。一般需用合计函数才会用 having SQL标准要求HAVING必须引用GROUP BY子句中的列或用于合计函数中的列。f. ORDER BY 子句，排序子句 order by 排序字段/别名 排序方式 [,排序字段/别名 排序方式]... 升序：ASC，降序：DESC 支持多个字段的排序。g. LIMIT 子句，限制结果数量子句 仅对处理好的结果进行数量限制。将处理好的结果的看作是一个集合，按照记录出现的顺序，索引从0开始。 limit 起始位置, 获取条数 省略第一个参数，表示从索引0开始。limit 获取条数h. DISTINCT, ALL 选项 distinct 去除重复记录 默认为 all, 全部记录/* UNION */ ------------------ 将多个select查询的结果组合成一个结果集合。 SELECT ... UNION [ALL|DISTINCT] SELECT ... 默认 DISTINCT 方式，即所有返回的行都是唯一的 建议，对每个SELECT查询加上小括号包裹。 ORDER BY 排序时，需加上 LIMIT 进行结合。 需要各select查询的字段数量一样。 每个select查询的字段列表(数量、类型)应一致，因为结果中的字段名以第一条select语句为准。/* 子查询 */ ------------------ - 子查询需用括号包裹。-- from型 from后要求是一个表，必须给子查询结果取个别名。 - 简化每个查询内的条件。 - from型需将结果生成一个临时表格，可用以原表的锁定的释放。 - 子查询返回一个表，表型子查询。 select * from (select * from tb where id&gt;0) as subfrom where id&gt;1;-- where型 - 子查询返回一个值，标量子查询。 - 不需要给子查询取别名。 - where子查询内的表，不能直接用以更新。 select * from tb where money = (select max(money) from tb); -- 列子查询 如果子查询结果返回的是一列。 使用 in 或 not in 完成查询 exists 和 not exists 条件 如果子查询返回数据，则返回1或0。常用于判断条件。 select column1 from t1 where exists (select * from t2); -- 行子查询 查询条件是一个行。 select * from t1 where (id, gender) in (select id, gender from t2); 行构造符：(col1, col2, ...) 或 ROW(col1, col2, ...) 行构造符通常用于与对能返回两个或两个以上列的子查询进行比较。 -- 特殊运算符 != all() 相当于 not in = some() 相当于 in。any 是 some 的别名 != some() 不等同于 not in，不等于其中某一个。 all, some 可以配合其他运算符一起使用。/* 连接查询(join) */ ------------------ 将多个表的字段进行连接，可以指定连接条件。-- 内连接(inner join) - 默认就是内连接，可省略inner。 - 只有数据存在时才能发送连接。即连接结果不能出现空行。 on 表示连接条件。其条件表达式与where类似。也可以省略条件（表示条件永远为真） 也可用where表示连接条件。 还有 using, 但需字段名相同。 using(字段名) -- 交叉连接 cross join 即，没有条件的内连接。 select * from tb1 cross join tb2;-- 外连接(outer join) - 如果数据不存在，也会出现在连接结果中。 -- 左外连接 left join 如果数据不存在，左表记录会出现，而右表为null填充 -- 右外连接 right join 如果数据不存在，右表记录会出现，而左表为null填充-- 自然连接(natural join) 自动判断连接条件完成连接。 相当于省略了using，会自动查找相同字段名。 natural join natural left join natural right joinselect info.id, info.name, info.stu_num, extra_info.hobby, extra_info.sex from info, extra_info where info.stu_num = extra_info.stu_id;/* 导入导出 */ ------------------select * into outfile 文件地址 [控制格式] from 表名; -- 导出表数据load data [local] infile 文件地址 [replace|ignore] into table 表名 [控制格式]; -- 导入数据 生成的数据默认的分隔符是制表符 local未指定，则数据文件必须在服务器上 replace 和 ignore 关键词控制对现有的唯一键记录的重复的处理-- 控制格式fields 控制字段格式默认：fields terminated by '\\t' enclosed by '' escaped by '\\\\' terminated by 'string' -- 终止 enclosed by 'char' -- 包裹 escaped by 'char' -- 转义 -- 示例： SELECT a,b,a+b INTO OUTFILE '/tmp/result.text' FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"' LINES TERMINATED BY '\\n' FROM test_table;lines 控制行格式默认：lines terminated by '\\n' terminated by 'string' -- 终止/* INSERT */ ------------------select语句获得的数据可以用insert插入。可以省略对列的指定，要求 values () 括号内，提供给了按照列顺序出现的所有字段的值。 或者使用set语法。 INSERT INTO tbl_name SET field=value,...；可以一次性使用多个值，采用(), (), ();的形式。 INSERT INTO tbl_name VALUES (), (), ();可以在列值指定时，使用表达式。 INSERT INTO tbl_name VALUES (field_value, 10+10, now());可以使用一个特殊值 DEFAULT，表示该列使用默认值。 INSERT INTO tbl_name VALUES (field_value, DEFAULT);可以通过一个查询的结果，作为需要插入的值。 INSERT INTO tbl_name SELECT ...;可以指定在插入的值出现主键（或唯一索引）冲突时，更新其他非主键列的信息。 INSERT INTO tbl_name VALUES/SET/SELECT ON DUPLICATE KEY UPDATE 字段=值, …;/* DELETE */ ------------------DELETE FROM tbl_name [WHERE where_definition] [ORDER BY ...] [LIMIT row_count]按照条件删除。where指定删除的最多记录数。limit可以通过排序条件删除。order by + limit支持多表删除，使用类似连接语法。delete from 需要删除数据多表1，表2 using 表连接操作 条件。/* TRUNCATE */ ------------------TRUNCATE [TABLE] tbl_name清空数据删除重建表区别：1，truncate 是删除表再创建，delete 是逐条删除2，truncate 重置auto_increment的值。而delete不会3，truncate 不知道删除了几条，而delete知道。4，当被用于带分区的表时，truncate 会保留分区/* 备份与还原 */ ------------------备份，将数据的结构与表内数据保存起来。利用 mysqldump 指令完成。-- 导出mysqldump [options] db_name [tables]mysqldump [options] ---database DB1 [DB2 DB3...]mysqldump [options] --all--database1. 导出一张表 mysqldump -u用户名 -p密码 库名 表名 &gt; 文件名(D:/a.sql)2. 导出多张表 mysqldump -u用户名 -p密码 库名 表1 表2 表3 &gt; 文件名(D:/a.sql)3. 导出所有表 mysqldump -u用户名 -p密码 库名 &gt; 文件名(D:/a.sql)4. 导出一个库 mysqldump -u用户名 -p密码 --lock-all-tables --database 库名 &gt; 文件名(D:/a.sql)可以-w携带WHERE条件-- 导入1. 在登录mysql的情况下： source 备份文件2. 在不登录的情况下 mysql -u用户名 -p密码 库名 &lt; 备份文件/* 视图 */ ------------------什么是视图： 视图是一个虚拟表，其内容由查询定义。同真实的表一样，视图包含一系列带有名称的列和行数据。但是，视图并不在数据库中以存储的数据值集形式存在。行和列数据来自由定义视图的查询所引用的表，并且在引用视图时动态生成。 视图具有表结构文件，但不存在数据文件。 对其中所引用的基础表来说，视图的作用类似于筛选。定义视图的筛选可以来自当前或其它数据库的一个或多个表，或者其它视图。通过视图进行查询没有任何限制，通过它们进行数据修改时的限制也很少。 视图是存储在数据库中的查询的sql语句，它主要出于两种原因：安全原因，视图可以隐藏一些数据，如：社会保险基金表，可以用视图只显示姓名，地址，而不显示社会保险号和工资数等，另一原因是可使复杂的查询易于理解和使用。-- 创建视图CREATE [OR REPLACE] [ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}] VIEW view_name [(column_list)] AS select_statement - 视图名必须唯一，同时不能与表重名。 - 视图可以使用select语句查询到的列名，也可以自己指定相应的列名。 - 可以指定视图执行的算法，通过ALGORITHM指定。 - column_list如果存在，则数目必须等于SELECT语句检索的列数-- 查看结构 SHOW CREATE VIEW view_name-- 删除视图 - 删除视图后，数据依然存在。 - 可同时删除多个视图。 DROP VIEW [IF EXISTS] view_name ...-- 修改视图结构 - 一般不修改视图，因为不是所有的更新视图都会映射到表上。 ALTER VIEW view_name [(column_list)] AS select_statement-- 视图作用 1. 简化业务逻辑 2. 对客户端隐藏真实的表结构-- 视图算法(ALGORITHM) MERGE 合并 将视图的查询语句，与外部查询需要先合并再执行！ TEMPTABLE 临时表 将视图执行完毕后，形成临时表，再做外层查询！ UNDEFINED 未定义(默认)，指的是MySQL自主去选择相应的算法。/* 事务(transaction) */ ------------------事务是指逻辑上的一组操作，组成这组操作的各个单元，要不全成功要不全失败。 - 支持连续SQL的集体成功或集体撤销。 - 事务是数据库在数据晚自习方面的一个功能。 - 需要利用 InnoDB 或 BDB 存储引擎，对自动提交的特性支持完成。 - InnoDB被称为事务安全型引擎。-- 事务开启 START TRANSACTION; 或者 BEGIN; 开启事务后，所有被执行的SQL语句均被认作当前事务内的SQL语句。-- 事务提交 COMMIT;-- 事务回滚 ROLLBACK; 如果部分操作发生问题，映射到事务开启前。-- 事务的特性 1. 原子性（Atomicity） 事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 2. 一致性（Consistency） 事务前后数据的完整性必须保持一致。 - 事务开始和结束时，外部数据一致 - 在整个事务过程中，操作是连续的 3. 隔离性（Isolation） 多个用户并发访问数据库时，一个用户的事务不能被其它用户的事物所干扰，多个并发事务之间的数据要相互隔离。 4. 持久性（Durability） 一个事务一旦被提交，它对数据库中的数据改变就是永久性的。-- 事务的实现 1. 要求是事务支持的表类型 2. 执行一组相关的操作前开启事务 3. 整组操作完成后，都成功，则提交；如果存在失败，选择回滚，则会回到事务开始的备份点。-- 事务的原理 利用InnoDB的自动提交(autocommit)特性完成。 普通的MySQL执行语句后，当前的数据提交操作均可被其他客户端可见。 而事务是暂时关闭“自动提交”机制，需要commit提交持久化数据操作。-- 注意 1. 数据定义语言（DDL）语句不能被回滚，比如创建或取消数据库的语句，和创建、取消或更改表或存储的子程序的语句。 2. 事务不能被嵌套-- 保存点 SAVEPOINT 保存点名称 -- 设置一个事务保存点 ROLLBACK TO SAVEPOINT 保存点名称 -- 回滚到保存点 RELEASE SAVEPOINT 保存点名称 -- 删除保存点-- InnoDB自动提交特性设置 SET autocommit = 0|1; 0表示关闭自动提交，1表示开启自动提交。 - 如果关闭了，那普通操作的结果对其他客户端也不可见，需要commit提交后才能持久化数据操作。 - 也可以关闭自动提交来开启事务。但与START TRANSACTION不同的是， SET autocommit是永久改变服务器的设置，直到下次再次修改该设置。(针对当前连接) 而START TRANSACTION记录开启前的状态，而一旦事务提交或回滚后就需要再次开启事务。(针对当前事务)/* 锁表 */表锁定只用于防止其它客户端进行不正当地读取和写入MyISAM 支持表锁，InnoDB 支持行锁-- 锁定 LOCK TABLES tbl_name [AS alias]-- 解锁 UNLOCK TABLES/* 触发器 */ ------------------ 触发程序是与表有关的命名数据库对象，当该表出现特定事件时，将激活该对象 监听：记录的增加、修改、删除。-- 创建触发器CREATE TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW trigger_stmt 参数： trigger_time是触发程序的动作时间。它可以是 before 或 after，以指明触发程序是在激活它的语句之前或之后触发。 trigger_event指明了激活触发程序的语句的类型 INSERT：将新行插入表时激活触发程序 UPDATE：更改某一行时激活触发程序 DELETE：从表中删除某一行时激活触发程序 tbl_name：监听的表，必须是永久性的表，不能将触发程序与TEMPORARY表或视图关联起来。 trigger_stmt：当触发程序激活时执行的语句。执行多个语句，可使用BEGIN...END复合语句结构-- 删除DROP TRIGGER [schema_name.]trigger_name可以使用old和new代替旧的和新的数据 更新操作，更新前是old，更新后是new. 删除操作，只有old. 增加操作，只有new.-- 注意 1. 对于具有相同触发程序动作时间和事件的给定表，不能有两个触发程序。-- 字符连接函数concat(str1,str2,...])concat_ws(separator,str1,str2,...)-- 分支语句if 条件 then 执行语句elseif 条件 then 执行语句else 执行语句end if;-- 修改最外层语句结束符delimiter 自定义结束符号 SQL语句自定义结束符号delimiter ; -- 修改回原来的分号-- 语句块包裹begin 语句块end-- 特殊的执行1. 只要添加记录，就会触发程序。2. Insert into on duplicate key update 语法会触发： 如果没有重复记录，会触发 before insert, after insert; 如果有重复记录并更新，会触发 before insert, before update, after update; 如果有重复记录但是没有发生更新，则触发 before insert, before update3. Replace 语法 如果有记录，则执行 before insert, before delete, after delete, after insert/* SQL编程 */ --------------------// 局部变量 ------------ 变量声明 declare var_name[,...] type [default value] 这个语句被用来声明局部变量。要给变量提供一个默认值，请包含一个default子句。值可以被指定为一个表达式，不需要为一个常数。如果没有default子句，初始值为null。-- 赋值 使用 set 和 select into 语句为变量赋值。 - 注意：在函数内是可以使用全局变量（用户自定义的变量）--// 全局变量 ------------ 定义、赋值set 语句可以定义并为变量赋值。set @var = value;也可以使用select into语句为变量初始化并赋值。这样要求select语句只能返回一行，但是可以是多个字段，就意味着同时为多个变量进行赋值，变量的数量需要与查询的列数一致。还可以把赋值语句看作一个表达式，通过select执行完成。此时为了避免=被当作关系运算符看待，使用:=代替。（set语句可以使用= 和 :=）。select @var:=20;select @v1:=id, @v2=name from t1 limit 1;select * from tbl_name where @var:=30;select into 可以将表中查询获得的数据赋给变量。 -| select max(height) into @max_height from tb;-- 自定义变量名为了避免select语句中，用户自定义的变量与系统标识符（通常是字段名）冲突，用户自定义变量在变量名前使用@作为开始符号。@var=10; - 变量被定义后，在整个会话周期都有效（登录到退出）--// 控制结构 ------------ if语句if search_condition then statement_list [elseif search_condition then statement_list]...[else statement_list]end if;-- case语句CASE value WHEN [compare-value] THEN result[WHEN [compare-value] THEN result ...][ELSE result]END-- while循环[begin_label:] while search_condition do statement_listend while [end_label];- 如果需要在循环内提前终止 while循环，则需要使用标签；标签需要成对出现。 -- 退出循环 退出整个循环 leave 退出当前循环 iterate 通过退出的标签决定退出哪个循环--// 内置函数 ------------ 数值函数abs(x) -- 绝对值 abs(-10.9) = 10format(x, d) -- 格式化千分位数值 format(1234567.456, 2) = 1,234,567.46ceil(x) -- 向上取整 ceil(10.1) = 11floor(x) -- 向下取整 floor (10.1) = 10round(x) -- 四舍五入去整mod(m, n) -- m%n m mod n 求余 10%3=1pi() -- 获得圆周率pow(m, n) -- m^nsqrt(x) -- 算术平方根rand() -- 随机数truncate(x, d) -- 截取d位小数-- 时间日期函数now(), current_timestamp(); -- 当前日期时间current_date(); -- 当前日期current_time(); -- 当前时间date('yyyy-mm-dd hh:ii:ss'); -- 获取日期部分time('yyyy-mm-dd hh:ii:ss'); -- 获取时间部分date_format('yyyy-mm-dd hh:ii:ss', '%d %y %a %d %m %b %j'); -- 格式化时间unix_timestamp(); -- 获得unix时间戳from_unixtime(); -- 从时间戳获得时间-- 字符串函数length(string) -- string长度，字节char_length(string) -- string的字符个数substring(str, position [,length]) -- 从str的position开始,取length个字符replace(str ,search_str ,replace_str) -- 在str中用replace_str替换search_strinstr(string ,substring) -- 返回substring首次在string中出现的位置concat(string [,...]) -- 连接字串charset(str) -- 返回字串字符集lcase(string) -- 转换成小写left(string, length) -- 从string2中的左边起取length个字符load_file(file_name) -- 从文件读取内容locate(substring, string [,start_position]) -- 同instr,但可指定开始位置lpad(string, length, pad) -- 重复用pad加在string开头,直到字串长度为lengthltrim(string) -- 去除前端空格repeat(string, count) -- 重复count次rpad(string, length, pad) --在str后用pad补充,直到长度为lengthrtrim(string) -- 去除后端空格strcmp(string1 ,string2) -- 逐字符比较两字串大小-- 流程函数case when [condition] then result [when [condition] then result ...] [else result] end 多分支if(expr1,expr2,expr3) 双分支。-- 聚合函数count()sum();max();min();avg();group_concat()-- 其他常用函数md5();default();--// 存储函数，自定义函数 ------------ 新建 CREATE FUNCTION function_name (参数列表) RETURNS 返回值类型 函数体 - 函数名，应该合法的标识符，并且不应该与已有的关键字冲突。 - 一个函数应该属于某个数据库，可以使用db_name.funciton_name的形式执行当前函数所属数据库，否则为当前数据库。 - 参数部分，由\"参数名\"和\"参数类型\"组成。多个参数用逗号隔开。 - 函数体由多条可用的mysql语句，流程控制，变量声明等语句构成。 - 多条语句应该使用 begin...end 语句块包含。 - 一定要有 return 返回值语句。-- 删除 DROP FUNCTION [IF EXISTS] function_name;-- 查看 SHOW FUNCTION STATUS LIKE 'partten' SHOW CREATE FUNCTION function_name;-- 修改 ALTER FUNCTION function_name 函数选项--// 存储过程，自定义功能 ------------ 定义存储存储过程 是一段代码（过程），存储在数据库中的sql组成。一个存储过程通常用于完成一段业务逻辑，例如报名，交班费，订单入库等。而一个函数通常专注与某个功能，视为其他程序服务的，需要在其他语句中调用函数才可以，而存储过程不能被其他调用，是自己执行 通过call执行。-- 创建CREATE PROCEDURE sp_name (参数列表) 过程体参数列表：不同于函数的参数列表，需要指明参数类型IN，表示输入型OUT，表示输出型INOUT，表示混合型注意，没有返回值。/* 存储过程 */ ------------------存储过程是一段可执行性代码的集合。相比函数，更偏向于业务逻辑。调用：CALL 过程名-- 注意- 没有返回值。- 只能单独调用，不可夹杂在其他语句中-- 参数IN|OUT|INOUT 参数名 数据类型IN 输入：在调用过程中，将数据输入到过程体内部的参数OUT 输出：在调用过程中，将过程体处理完的结果返回到客户端INOUT 输入输出：既可输入，也可输出-- 语法CREATE PROCEDURE 过程名 (参数列表)BEGIN 过程体END/* 用户和权限管理 */ -------------------- root密码重置1. 停止MySQL服务2. [Linux] /usr/local/mysql/bin/safe_mysqld --skip-grant-tables &amp; [Windows] mysqld --skip-grant-tables3. use mysql;4. UPDATE `user` SET PASSWORD=PASSWORD(\"密码\") WHERE `user` = \"root\";5. FLUSH PRIVILEGES;用户信息表：mysql.user-- 刷新权限FLUSH PRIVILEGES;-- 增加用户CREATE USER 用户名 IDENTIFIED BY [PASSWORD] 密码(字符串) - 必须拥有mysql数据库的全局CREATE USER权限，或拥有INSERT权限。 - 只能创建用户，不能赋予权限。 - 用户名，注意引号：如 'user_name'@'192.168.1.1' - 密码也需引号，纯数字密码也要加引号 - 要在纯文本中指定密码，需忽略PASSWORD关键词。要把密码指定为由PASSWORD()函数返回的混编值，需包含关键字PASSWORD-- 重命名用户RENAME USER old_user TO new_user-- 设置密码SET PASSWORD = PASSWORD('密码') -- 为当前用户设置密码SET PASSWORD FOR 用户名 = PASSWORD('密码') -- 为指定用户设置密码-- 删除用户DROP USER 用户名-- 分配权限/添加用户GRANT 权限列表 ON 表名 TO 用户名 [IDENTIFIED BY [PASSWORD] 'password'] - all privileges 表示所有权限 - *.* 表示所有库的所有表 - 库名.表名 表示某库下面的某表 GRANT ALL PRIVILEGES ON `pms`.* TO 'pms'@'%' IDENTIFIED BY 'pms0817';-- 查看权限SHOW GRANTS FOR 用户名 -- 查看当前用户权限 SHOW GRANTS; 或 SHOW GRANTS FOR CURRENT_USER; 或 SHOW GRANTS FOR CURRENT_USER();-- 撤消权限REVOKE 权限列表 ON 表名 FROM 用户名REVOKE ALL PRIVILEGES, GRANT OPTION FROM 用户名 -- 撤销所有权限-- 权限层级-- 要使用GRANT或REVOKE，您必须拥有GRANT OPTION权限，并且您必须用于您正在授予或撤销的权限。全局层级：全局权限适用于一个给定服务器中的所有数据库，mysql.user GRANT ALL ON *.*和 REVOKE ALL ON *.*只授予和撤销全局权限。数据库层级：数据库权限适用于一个给定数据库中的所有目标，mysql.db, mysql.host GRANT ALL ON db_name.*和REVOKE ALL ON db_name.*只授予和撤销数据库权限。表层级：表权限适用于一个给定表中的所有列，mysql.talbes_priv GRANT ALL ON db_name.tbl_name和REVOKE ALL ON db_name.tbl_name只授予和撤销表权限。列层级：列权限适用于一个给定表中的单一列，mysql.columns_priv 当使用REVOKE时，您必须指定与被授权列相同的列。-- 权限列表ALL [PRIVILEGES] -- 设置除GRANT OPTION之外的所有简单权限ALTER -- 允许使用ALTER TABLEALTER ROUTINE -- 更改或取消已存储的子程序CREATE -- 允许使用CREATE TABLECREATE ROUTINE -- 创建已存储的子程序CREATE TEMPORARY TABLES -- 允许使用CREATE TEMPORARY TABLECREATE USER -- 允许使用CREATE USER, DROP USER, RENAME USER和REVOKE ALL PRIVILEGES。CREATE VIEW -- 允许使用CREATE VIEWDELETE -- 允许使用DELETEDROP -- 允许使用DROP TABLEEXECUTE -- 允许用户运行已存储的子程序FILE -- 允许使用SELECT...INTO OUTFILE和LOAD DATA INFILEINDEX -- 允许使用CREATE INDEX和DROP INDEXINSERT -- 允许使用INSERTLOCK TABLES -- 允许对您拥有SELECT权限的表使用LOCK TABLESPROCESS -- 允许使用SHOW FULL PROCESSLISTREFERENCES -- 未被实施RELOAD -- 允许使用FLUSHREPLICATION CLIENT -- 允许用户询问从属服务器或主服务器的地址REPLICATION SLAVE -- 用于复制型从属服务器（从主服务器中读取二进制日志事件）SELECT -- 允许使用SELECTSHOW DATABASES -- 显示所有数据库SHOW VIEW -- 允许使用SHOW CREATE VIEWSHUTDOWN -- 允许使用mysqladmin shutdownSUPER -- 允许使用CHANGE MASTER, KILL, PURGE MASTER LOGS和SET GLOBAL语句，mysqladmin debug命令；允许您连接（一次），即使已达到max_connections。UPDATE -- 允许使用UPDATEUSAGE -- “无权限”的同义词GRANT OPTION -- 允许授予权限/* 表维护 */-- 分析和存储表的关键字分布ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE 表名 ...-- 检查一个或多个表是否有错误CHECK TABLE tbl_name [, tbl_name] ... [option] ...option = {QUICK | FAST | MEDIUM | EXTENDED | CHANGED}-- 整理数据文件的碎片OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] .../* 杂项 */ ------------------1. 可用反引号（`）为标识符（库名、表名、字段名、索引、别名）包裹，以避免与关键字重名！中文也可以作为标识符！2. 每个库目录存在一个保存当前数据库的选项文件db.opt。3. 注释： 单行注释 # 注释内容 多行注释 /* 注释内容 */ 单行注释 -- 注释内容 (标准SQL注释风格，要求双破折号后加一空格符（空格、TAB、换行等）)4. 模式通配符： _ 任意单个字符 % 任意多个字符，甚至包括零字符 单引号需要进行转义 \\'5. CMD命令行内的语句结束符可以为 \";\", \"\\G\", \"\\g\"，仅影响显示结果。其他地方还是用分号结束。delimiter 可修改当前对话的语句结束符。6. SQL对大小写不敏感7. 清除已有语句：\\c","link":"/2019/04/18/一千行-MySQL-学习笔记-转载.html"},{"title":"安装、部分配置icarus主题中文版","text":"摘要发现icarus主题还不错，花了一两个小时研究了下安装、部分配置icarus主题中文版 安装icarus 直接下载主题模块放到blog项目 ,blog项目根目录执行 1git clone https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus 此时已经下载到项目中。 顶级_config.yml中选择icarus主题 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: icarus 此时主题已经安装好，清除、编译、部署可以看到效果了 配置icarus 完全参照官网配置，进行翻译解说 配置文章部分顶部图片添加icarus 主题中的配置_config.yml中开启图片开关 12article: thumbnail: true 文章.md文件头中添加图片绝对/相对地址 12345title: Getting Started with Icarusthumbnail: /gallery/thumbnails/desert.jpg// thumbnail:https://raw.githubusercontent.com/removeif/blog_image/master/20190620152744.png---Post content... 配置完成后部署显示效果如下(最新文章列表显示缩略图、文章开头显示一张设置图片) 左边文章导航栏开启icarus 主题中的配置_config.yml中开关 1234widgets: - type: toc position: left 同事文章顶部加入标签 1234title: Table of Contents Exampletoc: true---Post content... 配置效果 评论系统开启icarus 主题中的配置_config.yml中开启（部分评论系统需要翻墙才能使用，valine不用翻墙个人推荐，valine安装参考） 1234567comment: type: valine app_id: xxxxxxxx # (required) LeanCloud application id app_key: xxxxxxxx # (required) LeanCloud application key notify: false # (optional) receive email notification verify: false # (optional) show verification code placeholder: xxxxxxxx # (optional) comment box placeholder text 开启效果 捐赠收款开启icarus 主题中的配置_config.yml中开启 注意如果默认不配置，编译时有报错，可以# 把它注释掉，不启用功能 1234567891011donate: - # Donation entry name type: alipay # Qrcode image URL qrcode: 'https://wx2.sinaimg.cn/large/b5d1b710gy1g0lvxdcwm0j20p011i4bg.jpg' - # Donation entry name type: wechat # Qrcode image URL qrcode: 'https://wx2.sinaimg.cn/large/b5d1b710gy1g0lvwdcpb5j20u014qgy2.jpg' 开启配置效果如下 全局搜索开启icarus 主题中的配置_config.yml中开启,不同的搜索类型需要安装插件参考官网,type: insight此类型不需要安装，已经内置 12search: type: insight 效果如下 更多配置请参考官网配置目前配置基本已经够使用，还需要更多配置请参考连接 参考自","link":"/2019/02/28/安装、部分配置icarus主题中文版.html"},{"title":"java基础-static关键字","text":"static用法 static和final是两个我们必须掌握的关键字。不同于其他关键字，他们都有多种用法，而且在一定环境下使用，可以提高程序的运行性能，优化程序的结构。下面我们先来了解一下static关键字及其用法。 修饰成员变量在我们平时的使用当中，static最常用的功能就是修饰类的属性和方法，让他们成为类的成员属性和方法，我们通常将用static修饰的成员称为类成员或者静态成员。 1234567891011121314151617181920212223public class Person { String name; int age; public String toString() { return \"Name:\" + name + \", Age:\" + age; } public static void main(String[] args) { Person p1 = new Person(); p1.name = \"zhangsan\"; p1.age = 10; Person p2 = new Person(); p2.name = \"lisi\"; p2.age = 12; System.out.println(p1); System.out.println(p2); } /**Output * Name:zhangsan, Age:10 * Name:lisi, Age:12 *///~} 根据Person构造出的每一个对象都是独立存在的，保存有自己独立的成员变量，相互不会影响，他们在内存中的示意如下: 从上图中可以看出，p1和p2两个变量引用的对象分别存储在内存中堆区域的不同地址中，所以他们之间相互不会干扰。但其实，在这当中，我们省略了一些重要信息，相信大家也都会想到，对象的成员属性都在这了，由每个对象自己保存，那么他们的方法呢？实际上，不论一个类创建了几个对象，他们的方法都是一样的： 从上面的图中我们可以看到，两个Person对象的方法实际上只是指向了同一个方法定义。这个方法定义是位于内存中的一块不变区域（由jvm划分），我们暂称它为静态存储区。这一块存储区不仅存放了方法的定义，实际上从更大的角度而言，它存放的是各种类的定义，当我们通过new来生成对象时，会根据这里定义的类的定义去创建对象。多个对象仅会对应同一个方法，这里有一个让我们充分信服的理由，那就是不管多少的对象，他们的方法总是相同的，尽管最后的输出会有所不同，但是方法总是会按照我们预想的结果去操作，即不同的对象去调用同一个方法，结果会不尽相同。 static关键字可以修饰成员变量和方法，来让它们变成类的所属，而不是对象的所属，比如我们将Person的age属性用static进行修饰，结果会是什么样呢? 1234567891011public class Person { String name; static int age; /* 其余代码不变... */ /**Output * Name:zhangsan, Age:12 * Name:lisi, Age:12 *///~} 我们发现，结果发生了一点变化，在给p2的age属性赋值时，干扰了p1的age属性，这是为什么呢？我们还是来看他们在内存中的示意： 我们发现，给age属性加了static关键字之后，Person对象就不再拥有age属性了，age属性会统一交给Person类去管理，即多个Person对象只会对应一个age属性，一个对象如果对age属性做了改变，其他的对象都会受到影响。我们看到此时的age和toString()方法一样，都是交由类去管理。 虽然我们看到static可以让对象共享属性，但是实际中我们很少这么用，也不推荐这么使用。因为这样会让该属性变得难以控制，因为它在任何地方都有可能被改变。如果我们想共享属性，一般我们会采用其他的办法： 1234567891011121314151617181920212223242526272829public class Person { private static int count = 0; int id; String name; int age; public Person() { id = ++count; } public String toString() { return \"Id:\" + id + \", Name:\" + name + \", Age:\" + age; } public static void main(String[] args) { Person p1 = new Person(); p1.name = \"zhangsan\"; p1.age = 10; Person p2 = new Person(); p2.name = \"lisi\"; p2.age = 12; System.out.println(p1); System.out.println(p2); } /**Output * Id:1, Name:zhangsan, Age:10 * Id:2, Name:lisi, Age:12 *///~} 上面的代码起到了给Person的对象创建一个唯一id以及记录总数的作用，其中count由static修饰，是Person类的成员属性，每次创建一个Person对象，就会使该属性自加1然后赋给对象的id属性，这样，count属性记录了创建Person对象的总数，由于count使用了private修饰，所以从类外面无法随意改变。 修饰成员方法static的另一个作用，就是修饰成员方法。相比于修饰成员属性，修饰成员方法对于数据的存储上面并没有多大的变化，因为我们从上面可以看出，方法本来就是存放在类的定义当中的。static修饰成员方法最大的作用，就是可以使用”类名.方法名“的方式操作方法，避免了先要new出对象的繁琐和资源消耗，我们可能会经常在帮助类中看到它的使用： 12345678910public class PrintHelper { public static void print(Object o){ System.out.println(o); } public static void main(String[] args) { PrintHelper.print(\"Hello world\"); }} 上面便是一个例子（现在还不太实用），但是我们可以看到它的作用，使得static修饰的方法成为类的方法，使用时通过“类名.方法名”的方式就可以方便的使用了，相当于定义了一个全局的函数（只要导入该类所在的包即可）。不过它也有使用的局限，一个static修饰的类中，不能使用非static修饰的成员变量和方法，这很好理解，因为static修饰的方法是属于类的，如果去直接使用对象的成员变量，它会不知所措（不知该使用哪一个对象的属性）。 静态块在说明static关键字的第三个用法时，我们有必要重新梳理一下一个对象的初始化过程。以下面的代码为例： 1234567891011121314151617181920212223242526272829class Book{ public Book(String msg) { System.out.println(msg); }}public class Person { Book book1 = new Book(\"book1成员变量初始化\"); static Book book2 = new Book(\"static成员book2成员变量初始化\"); public Person(String msg) { System.out.println(msg); } Book book3 = new Book(\"book3成员变量初始化\"); static Book book4 = new Book(\"static成员book4成员变量初始化\"); public static void main(String[] args) { Person p1 = new Person(\"p1初始化\"); } /**Output * static成员book2成员变量初始化 * static成员book4成员变量初始化 * book1成员变量初始化 * book3成员变量初始化 * p1初始化 *///~} 上面的例子中，Person类中组合了四个Book成员变量，两个是普通成员，两个是static修饰的类成员。我们可以看到，当我们new一个Person对象时，static修饰的成员变量首先被初始化，随后是普通成员，最后调用Person类的构造方法完成初始化。也就是说，在创建对象时，static修饰的成员会首先被初始化，而且我们还可以看到，如果有多个static修饰的成员，那么会按照他们的先后位置进行初始化。 实际上，static修饰的成员的初始化可以更早的进行，请看下面的例子： 12345678910111213141516171819202122232425262728293031323334353637class Book{ public Book(String msg) { System.out.println(msg); }}public class Person { Book book1 = new Book(\"book1成员变量初始化\"); static Book book2 = new Book(\"static成员book2成员变量初始化\"); public Person(String msg) { System.out.println(msg); } Book book3 = new Book(\"book3成员变量初始化\"); static Book book4 = new Book(\"static成员book4成员变量初始化\"); public static void funStatic() { System.out.println(\"static修饰的funStatic方法\"); } public static void main(String[] args) { Person.funStatic(); System.out.println(\"****************\"); Person p1 = new Person(\"p1初始化\"); } /**Output * static成员book2成员变量初始化 * static成员book4成员变量初始化 * static修饰的funStatic方法 * *************** * book1成员变量初始化 * book3成员变量初始化 * p1初始化 *///~} 在上面的例子中我们可以发现两个有意思的地方，第一个是当我们没有创建对象，而是通过类去调用类方法时，尽管该方法没有使用到任何的类成员，类成员还是在方法调用之前就初始化了，这说明，当我们第一次去使用一个类时，就会触发该类的成员初始化。第二个是当我们使用了类方法，完成类的成员的初始化后，再new该类的对象时，static修饰的类成员没有再次初始化，这说明，static修饰的类成员，在程序运行过程中，只需要初始化一次即可，不会进行多次的初始化。 回顾了对象的初始化以后，我们再来看static的第三个作用就非常简单了，那就是当我们初始化static修饰的成员时，可以将他们统一放在一个以static开始，用花括号包裹起来的块状语句中： 123456789101112131415161718192021222324252627282930313233343536373839404142class Book{ public Book(String msg) { System.out.println(msg); }}public class Person { Book book1 = new Book(\"book1成员变量初始化\"); static Book book2; static { book2 = new Book(\"static成员book2成员变量初始化\"); book4 = new Book(\"static成员book4成员变量初始化\"); } public Person(String msg) { System.out.println(msg); } Book book3 = new Book(\"book3成员变量初始化\"); static Book book4; public static void funStatic() { System.out.println(\"static修饰的funStatic方法\"); } public static void main(String[] args) { Person.funStatic(); System.out.println(\"****************\"); Person p1 = new Person(\"p1初始化\"); } /**Output * static成员book2成员变量初始化 * static成员book4成员变量初始化 * static修饰的funStatic方法 * *************** * book1成员变量初始化 * book3成员变量初始化 * p1初始化 *///~} 我们将上一个例子稍微做了一下修改，可以看到，结果没有二致。 静态导包相比于上面的三种用途，第四种用途可能了解的人就比较少了，但是实际上它很简单，而且在调用类方法时会更方便。以上面的“PrintHelper”的例子为例，做一下稍微的变化，即可使用静态导包带给我们的方便： 123456789/* PrintHelper.java文件 */package com.dotgua.study;public class PrintHelper { public static void print(Object o){ System.out.println(o); }} 123456789101112import static com.dotgua.study.PrintHelper.*; // 导入上面的包public class App { public static void main( String[] args ) { print(\"Hello World!\"); } /**Output * Hello World! *///~} 上面的代码来自于两个java文件，其中的PrintHelper很简单，包含了一个用于打印的static方法。而在App.java文件中，我们首先将PrintHelper类导入，这里在导入时，我们使用了static关键字，而且在引入类的最后还加上了“.*”，它的作用就是将PrintHelper类中的所有类方法直接导入。不同于非static导入，采用static导入包后，在不与当前类的方法名冲突的情况下，无需使用“类名.方法名”的方法去调用类方法了，直接可以采用”方法名“去调用类方法，就好像是该类自己的方法一样使用即可。 总结static是java中非常重要的一个关键字，而且它的用法也很丰富，主要有四种用法： 用来修饰成员变量，将其变为类的成员，从而实现所有对象对于该成员的共享； 用来修饰成员方法，将其变为类方法，可以直接使用“类名.方法名”的方式调用，常用于工具类； 静态块用法，将多个类成员放在一起初始化，使得程序更加规整，其中理解对象的初始化过程非常关键； 静态导包用法，将类的方法直接导入到当前类中，从而直接使用“方法名”即可调用类方法，更加方便。 参考自","link":"/2019/01/07/java基础-static关键字.html"},{"title":"java基础-final关键字","text":"final 关键字 在java的关键字中，static和final是两个我们必须掌握的关键字。不同于其他关键字，他们都有多种用法，而且在一定环境下使用，可以提高程序的运行性能，优化程序的结构。下面我们来了解一下final关键字及其用法。 修饰数据在编写程序时，我们经常需要说明一个数据是不可变的，我们成为常量。在java中，用final关键字修饰的变量，只能进行一次赋值操作，并且在生存期内不可以改变它的值。更重要的是，final会告诉编译器，这个数据是不会修改的，那么编译器就可能会在编译时期就对该数据进行替换甚至执行计算，这样可以对我们的程序起到一点优化。不过在针对基本类型和引用类型时，final关键字的效果存在细微差别。我们来看下面的例子： 123456789101112131415161718192021222324 class Value { int v; public Value(int v) { this.v = v; }}public class FinalTest { final int f1 = 1; final int f2; public FinalTest() { f2 = 2; } public static void main(String[] args) { final int value1 = 1; // value1 = 4; final double value2; value2 = 2.0; final Value value3 = new Value(1); value3.v = 4; }} 上面的例子中，我们先来看一下main方法中的几个final修饰的数据，在给value1赋初始值之后，我们无法再对value1的值进行修改，final关键字起到了常量的作用。从value2我们可以看到，final修饰的变量可以不在声明时赋值，即可以先声明，后赋值。value3时一个引用变量，这里我们可以看到final修饰引用变量时，只是限定了引用变量的引用不可改变，即不能将value3再次引用另一个Value对象，但是引用的对象的值是可以改变的，从内存模型中我们看的更加清晰： 上图中，final修饰的值用粗线条的边框表示它的值是不可改变的，我们知道引用变量的值实际上是它所引用的对象的地址，也就是说该地址的值是不可改变的，从而说明了为什么引用变量不可以改变引用对象。而实际引用的对象实际上是不受final关键字的影响的，所以它的值是可以改变的。 另一方面，我们看到了用final修饰成员变量时的细微差别，因为final修饰的数据的值是不可改变的，所以我们必须确保在使用前就已经对成员变量赋值了。因此对于final修饰的成员变量，我们有且只有两个地方可以给它赋值，一个是声明该成员时赋值，另一个是在构造方法中赋值，在这两个地方我们必须给它们赋初始值。 最后我们需要注意的一点是，同时使用static和final修饰的成员在内存中只占据一段不能改变的存储空间。 修饰方法参数前面我们可以看到，如果变量是我们自己创建的，那么使用final修饰表示我们只会给它赋值一次且不会改变变量的值。那么如果变量是作为参数传入的，我们怎么保证它的值不会改变呢？这就用到了final的第二种用法，即在我们编写方法时，可以在参数前面添加final关键字，它表示在整个方法中，我们不会（实际上是不能）改变参数的值： 12345678910public class FinalTest { /* ... */ public void finalFunc(final int i, final Value value) { // i = 5; 不能改变i的值 // v = new Value(); 不能改变v的值 value.v = 5; // 可以改变引用对象的值 }} 修饰方法第三种方式，即用final关键字修饰方法，它表示该方法不能被覆盖。这种使用方式主要是从设计的角度考虑，即明确告诉其他可能会继承该类的程序员，不希望他们去覆盖这个方法。这种方式我们很容易理解，然而，关于private和final关键字还有一点联系，这就是类中所有的private方法都隐式地指定为是final的，由于无法在类外使用private方法，所以也就无法覆盖它。 修饰类了解了final关键字的其他用法，我们很容易可以想到使用final关键字修饰类的作用，那就是用final修饰的类是无法被继承的。 上面我们讲解了final的四种用法，然而，对于第三种和第四种用法，我们却甚少使用。这不是没有道理的，从final的设计来讲，这两种用法甚至可以说是鸡肋，因为对于开发人员来讲，如果我们写的类被继承的越多，就说明我们写的类越有价值，越成功。即使是从设计的角度来讲，也没有必要将一个类设计为不可继承的。Java标准库就是一个很好的反例，特别是Java 1.0/1.1中Vector类被如此广泛的运用，如果所有的方法均未被指定为final的话，它可能会更加有用。如此有用的类，我们很容易想到去继承和重写他们，然而，由于final的作用，导致我们对Vector类的扩展受到了一些阻碍，导致了Vector并没有完全发挥它应有的全部价值。 总结final关键字是我们经常使用的关键字之一，它的用法有很多，但是并不是每一种用法都值得我们去广泛使用。它的主要用法有以下四种： 用来修饰数据，包括成员变量和局部变量，该变量只能被赋值一次且它的值无法被改变。对于成员变量来讲，我们必须在声明时或者构造方法中对它赋值； 用来修饰方法参数，表示在变量的生存期中它的值不能被改变； 修饰方法，表示该方法无法被重写； 修饰类，表示该类无法被继承。 上面的四种方法中，第三种和第四种方法需要谨慎使用，因为在大多数情况下，如果是仅仅为了一点设计上的考虑，我们并不需要使用final来修饰方法和类。 参考自","link":"/2019/01/07/java基础-final关键字.html"},{"title":"Java设计模式之代理模式","text":"代理模式代理(Proxy)是一种设计模式,提供了对目标对象另外的访问方式;即通过代理对象访问目标对象.这样做的好处是:可以在目标对象实现的基础上,增强额外的功能操作,即扩展目标对象的功能.这里使用到编程中的一个思想:不要随意去修改别人已经写好的代码或者方法,如果需改修改,可以通过代理的方式来扩展该方法 举个例子来说明代理的作用:假设我们想邀请一位明星,那么并不是直接连接明星,而是联系明星的经纪人,来达到同样的目的.明星就是一个目标对象,他只要负责活动中的节目,而其他琐碎的事情就交给他的代理人(经纪人)来解决.这就是代理思想在现实中的一个例子.图片表示如下： 代理模式的关键点是:代理对象与目标对象.代理对象是对目标对象的扩展,并会调用目标对象 静态代理静态代理在使用时,需要定义接口或者父类,被代理对象与代理对象一起实现相同的接口或者是继承相同父类. eg:模拟保存动作,定义一个保存动作的接口:IUserDao.java,然后目标对象实现这个接口的方法UserDao.java,此时如果使用静态代理方式,就需要在代理对象(UserDaoProxy.java)中也实现IUserDao接口.调用的时候通过调用代理对象的方法来调用目标对象.需要注意的是,代理对象与目标对象要实现相同的接口,然后通过调用相同的方法来调用目标对象的方法 示例代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * @desc 用户保存接口 */public interface IUserDao { // 保存方法 void save();}/** * @desc 用户保存 */public class UserDao implements IUserDao { @Override public void save() { System.out.println(\"OK，已保存数据!\"); }}/** * @desc 代理对象，静态代理 */public class UserDaoProxy implements IUserDao { // 目标对象 private IUserDao target; public UserDaoProxy(IUserDao user) { this.target = user; } @Override public void save() { System.out.println(\"开始事物.\"); target.save(); System.out.println(\"提交事物.\"); }}/** * @desc 静态代理测试方法 */public class MainTest { public static void main(String[] args) { // 目标对象 IUserDao userDao = new UserDao(); // 代理对象，把目标对象传给代理，建立代理关系 UserDaoProxy proxy = new UserDaoProxy(userDao); proxy.save(); }} 结果： 静态代理总结:1.可以做到在不修改目标对象的功能前提下,对目标功能扩展.2.缺点: 因为代理对象需要与目标对象实现一样的接口,所以会有很多代理类,类太多.同时,一旦接口增加方法,目标对象与代理对象都要维护. 如何解决静态代理中的缺点呢?答案是可以使用动态代理方式 动态代理动态代理有以下特点: 代理对象,不需要实现接口 .代理对象的生成,是利用JDK的API,动态的在内存中构建代理对象(需要我们指定创建代理对象/目标对象实现的接口的类型) 动态代理也叫做:JDK代理,接口代理 JDK中生成代理对象的API代理类所在包:java.lang.reflect.ProxyJDK实现代理只需要使用newProxyInstance方法,但是该方法需要接收三个参数,完整的写法是: 1static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces,InvocationHandler h ) 注意该方法是在Proxy类中是静态方法,且接收的三个参数依次为: ClassLoader loader,:指定当前目标对象使用类加载器,获取加载器的方法是固定的 Class&lt;?&gt;[] interfaces,:目标对象实现的接口的类型,使用泛型方式确认类型 InvocationHandler h:事件处理,执行目标对象的方法时,会触发事件处理器的方法,会把当前执行目标对象的方法作为参数传入 代码示例:接口类IUserDao.java以及接口实现类,目标对象UserDao是一样的,没有做修改.在这个基础上,增加一个代理工厂类(ProxyFactory.java),将代理类写在这个地方,然后在测试类(需要使用到代理的代码)中先建立目标对象和代理对象的联系,然后代用代理对象的中同名方法 示例代码代理工厂类:ProxyFactory.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;/** * @desc 动态代理工厂 */public class ProxyFactory { private Object target; /** * 维护一个目标对象 * * @param target */ public ProxyFactory(Object target) { this.target = target; } public Object getProxyInstance() { return Proxy.newProxyInstance(target.getClass().getClassLoader(), target.getClass().getInterfaces(), new InvocationHandler() { @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(\"开始事物2.\"); // 执行目标方法 Object returnValue = method.invoke(target, args); System.out.println(\"提交事物2.\"); return returnValue; } }); }}/** * @desc 动态代理测试方法 */public class MainTest { public static void main(String[] args) { // 目标对象 IUserDao userDao = new UserDao(); // 原始类型 System.out.println(userDao.getClass()); // 给目标对象，创建代理对象 IUserDao proxy = (IUserDao) new ProxyFactory(userDao).getProxyInstance(); // 内存中动态生成的代理对象 System.out.println(proxy.getClass()); // 执行方法 proxy.save(); }} 结果： 总结代理对象不需要实现接口,但是目标对象一定要实现接口,否则不能用动态代理 Cglib代理上面的静态代理和动态代理模式都是要求目标对象是实现一个接口的目标对象,但是有时候目标对象只是一个单独的对象,并没有实现任何的接口,这个时候就可以使用以目标对象子类的方式类实现代理,这种方法就叫做:Cglib代理 一. Cglib代理,也叫作子类代理,它是在内存中构建一个子类对象从而实现对目标对象功能的扩展. JDK的动态代理有一个限制,就是使用动态代理的对象必须实现一个或多个接口,如果想代理没有实现接口的类,就可以使用Cglib实现. Cglib是一个强大的高性能的代码生成包,它可以在运行期扩展java类与实现java接口.它广泛的被许多AOP的框架使用,例如Spring AOP和synaop为他们提供方法的interception(拦截) Cglib包的底层是通过使用一个小而块的字节码处理框架ASM来转换字节码并生成新的类.不鼓励直接使用ASM,因为它要求你必须对JVM内部结构包括class文件的格式和指令集都很熟悉. 二. Cglib子类代理实现方法: 需要引入cglib的jar文件,但是Spring的核心包中已经包括了Cglib功能,所以直接引入spring-core-3.2.5.jar即可. 引入功能包后,就可以在内存中动态构建子类 代理的类不能为final,否则报错 目标对象的方法如果为final/static,那么就不会被拦截,即不会执行目标对象额外的业务方法. 示例代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * 目标对象,没有实现任何接口 */public class UserDao { public void save() { System.out.println(\"----已经保存数据!----\"); }}/** * Cglib子类代理工厂 * 对UserDao在内存中动态构建一个子类对象 */public class ProxyFactory implements MethodInterceptor{ //维护目标对象 private Object target; public ProxyFactory(Object target) { this.target = target; } //给目标对象创建一个代理对象 public Object getProxyInstance(){ //1.工具类 Enhancer en = new Enhancer(); //2.设置父类 en.setSuperclass(target.getClass()); //3.设置回调函数 en.setCallback(this); //4.创建子类(代理对象) return en.create(); } @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable { System.out.println(\"开始事务...\"); //执行目标对象的方法 Object returnValue = method.invoke(target, args); System.out.println(\"提交事务...\"); return returnValue; }}/** * 测试类 */public class App { @Test public void test(){ //目标对象 UserDao target = new UserDao(); //代理对象 UserDao proxy = (UserDao)new ProxyFactory(target).getProxyInstance(); //执行代理对象的方法 proxy.save(); }} 总结动态代理与静态代理相比较，最大的好处是接口中声明的所有方法都被转移到调用处理器一个集中的方法中处理。在接口方法数量比较多的时候，我们可以进行灵活处理，而不需要像静态代理那样对每一个方法或方法组合进行处理。Proxy 很美很强大，但是仅支持 interface 代理。Java 的单继承机制注定了这些动态代理类们无法实现对 class 的动态代理。好在有cglib为Proxy提供了弥补。class与interface的区别本来就模糊，在java8中更是增加了一些新特性，使得interface越来越接近class，当有一日，java突破了单继承的限制，动态代理将会更加强大。 参考：http://www.cnblogs.com/cenyu/p/6289209.html http://blog.csdn.net/goskalrie/article/details/52458773","link":"/2019/01/06/Java设计模式之代理模式.html"},{"title":"Java设计模式之命令模式","text":"目的 将一个请求封装为一个对象，从而可用不同的请求对客户进行参数化；对请求排队或记录日志，以及支持可撤销的操作。 将”发出请求的对象”和”接收与执行这些请求的对象”分隔开来。 效果 command模式将调用操作的对象和实现该操作的对象解耦 可以将多个命令装配成一个复合命令，复合命令是Composite模式的一个实例 增加新的command很容易，无需改变已有的类 适用性 抽象出待执行的动作以参数化某对象 在不同的时刻指定、排列和执行请求。如请求队列 支持取消操作 支持修改日志 用构建在原语操作上的高层操作构造一个系统。支持事物 参与者 Command声明执行操作的接口 ConcreteCommand将一个接收者对象绑定于一个动作 调用接收者相应的操作，以实现execute Client创建一个具体命令对象并设定它的接收者 Invoker要求该命令执行这个请求 Receiver知道如何实施与执行一个请求相关的操作。任何类都可能作为一个接收者 结构图 协作： 1)、client创建一个ConcreteCommand对象并指定它的Receiver对象 2)、某Invoker对象存储该ConcreteCommand对象 3)、该Invoker通过调用Command对象的execute操作来提交一个请求。若该命令是可撤销的，ConcreteCommand在执行execute操作前存储当前状态以用于取消该命令 4)、ConcreteCommand对象调用它的Receiver的操作以执行该请求 命令对象将动作和接受者包进对象中，这个对象只暴露出一个execute()方法，当此方法被调用的时候，接收者就会进行这些动作。从外面来看，其他对象不知道究竟哪个接收者进行了哪些动作，只知道如果调用execute()方法，请求的目的就能达到。 java 实现源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102/** * @desc 命令接口 */public interface Command { /** * 执行命令 */ void excute(); /** * 撤销命令 */ void undo();}/** * @desc 真正命令执行者，接收者 */public class Receiver { public void action() { System.out.println(\"正在执行命令...\"); } public void unAction() { System.out.println(\"正在撤销命令...\"); }}/** * @desc 创建命令（可实现多条命令以及多个命令执行人） */public class CreateCommand implements Command { private Receiver receiver; /** * 初始化命令接收人 * * @param receiver */ public CreateCommand(Receiver receiver) { this.receiver = receiver; } @Override public void excute() { receiver.action(); } @Override public void undo() { receiver.unAction(); }}/** * @desc 命令调用者 */public class Invoker { private Command command; /** * 命令调用者持有该命令 * * @param command */ public Invoker(Command command) { this.command = command; } /** * 执行命令 */ public void runCommand() { command.excute(); } /** * 撤销命令 */ public void unRunCommand() { command.undo(); }}/** * @desc 命令模式测试 */public class TestMain { public static void main(String[] args) { // new 一个命令执行人 Receiver receiver = new Receiver(); // new 一条命令 Command command = new CreateCommand(receiver); // 开始调用命令 Invoker invoker = new Invoker(command); invoker.runCommand(); invoker.unRunCommand(); }} 执行结果 常见应用： 1、工作队列，线程池，日程安排 2、日志请求(系统恢复)要点： 1、命令模式将发出请求的对象和执行请求的对象解耦 2、在被解耦的两者之间是通过命令对象进行沟通的。命令对象封装了接收者和一个或一组动作 3、调用者通过调用命令对象的execute()发出请求，这会使得接收者的动作被调用 4、调用者可以接受命令当作参数，甚至在运行时动态的进行 5、命令可以支持撤销，做法是实现一个undo()方法来回到execute()被执行前的状态 6、宏命令是命令的一种简单的延伸，允许调用多个命令。宏方法也可以支持撤销 7、实际操作时，很常见使用”聪明”命令对象，也就是直接实现了请求，而不是将工作委托给接受者(弊端？) 8、命令也可以用来实现日志和事物系统 参考：http://www.cnblogs.com/ikuman/archive/2013/08/06/3233092.html","link":"/2019/01/04/Java设计模式之命令模式.html"},{"title":"java 8部分读书笔记","text":"Lambda 表达式 Lambda 表达式引用的是值，不是变量。 Lambda 表达式中的变量只能是final类型，只能给变量赋值一次。 123String name = getUserName();name = formatUesrName();button.addActionListener(event -&gt; System.out.println(\"Hi\" + name)) 如上代码将不会编译通过，name被赋值多次。 函数接口：只有一个抽象方法的接口，用作Lambda表达式的类型。 Java中重要的几个函数接口 名称 解释 返回值 eg 参数 Predicate 断言 boolean 这张唱片已经发行了吗 T Consumer 消费 void 输出一个值 T Function&lt;T,R&gt; 运行 R 获得Artist对象的名字 T Supplier 供应 T 工厂方法 None UnaryOperator 一元运算 T 逻辑非(!) T BinaryOperator 二元运算 T 求两个数的乘积 (T,T) 所有的都有泛型没有的话值代码编译不过 123Predicate&lt;Integer&gt; atLeast5 = x -&gt; x &gt; 5;// 编译通过Predicate atLeast5 = x -&gt; x &gt; 5;// 编译不通过BinaryOperator&lt;Long&gt; addLongs = (x , y) -&gt; x + y;// 编译通过 流 Stream(针对于集合) 惰性求值 和及早求值 1allArtists.stream().filter(artist -&gt; artist.isFrom(\"London\")); 这行代码并没有做什么实质性工作,filter只是刻画出了Stream，没有产生新的集合。像filter这种只描述Stream，不产生新集合的方法叫做惰性求值方法,而像count这样最终会从Stream产生值的方法叫做及早求值方法。 123456789allArtists.stream().filter(artist - &gt;{ System.out.println(artist.getName()); return artist.isFrom(\"London\");});// 此段代码并不会输出 艺术家名字allArtists.stream().filter(artist - &gt;{ System.out.println(artist.getName()); return artist.isFrom(\"London\");}).count();// 此段代码并会输出 艺术家名字 判断一个操作是惰性求值还是及早求值，只需看它的返回值，返回值是Stream,那么就是惰性求值，返回值是另一个值或者是空，则是及早求值。最终达到的效果：通过这些方法形成一个惰性求值的链，最终调用一个及早求值方法得到我们需要的最终结果。 常用的流操作 collect(toList()) :由Stream里的值生成一个列表 Stream的of 方法使用一组初始值生成新的Stream 12List&lt;String&gt; collected = Stream.of(\"a\",\"b\",\"c\").collect(Collectors.toList());assertEquals(Arrays.asList(\"a\",\"b\",\"c\"),collected);//判断结果和预期值是否一样 map 将一种类型转换为另一种类型，将一个流中的值转换为一个新的流。mapToInt/mapToDouble/mapToLong 123List&lt;String&gt; collected = Stream.of(\"a\",\"b\",\"hello\") .map(string -&gt; string.toUpperCase()) .collect(Collectors.toList());//将小写转换为大写 filter filter模式，保留Stream中的一些元素，过滤掉其他的。返回true保留，返回false过滤。 123List&lt;String&gt; beginWithNumbers = Stream.of(\"a\",\"1adf\",\"abc1\") .filter(value -&gt; isDigit(value.chartAt(0))) .collect(toList());//返回数据开头的字符串 flatMap :可用Stream替换值，将多个Stream连接成一个Stream 123List&lt;Integer&gt; together = Stream.of(asList(1,2), asList(3,4)) .flatMap(numbers -&gt; numbers.stream()) .collect(toList()); 它会把原流中的每一个元素经过指定函数处理之后，返回一个Stream对象，并将之展开到原父流中。 max和min 1234List&lt;Track&gt; tracks = asList(new Track(\"Bakai\",524), new Track(\"Violets\",378), new Track(\"Time\",451));Track shortestTrack = tracks.stream().min(Commparator.comparing(track -&gt; track.getLength())).get();// 查找距离最短的 reduce 聚合归纳：操作中可以实现从一组值生成一个值 使用reduce求和 12int count = Stream.of(1,2,3) .reduce(0, (acc, element) -&gt; acc + element); 展开reduce操作 123456BinaryOperator&lt;Integer&gt; accumulator = (acc, element) -&gt; acc + element;int count = accumulator.apply( accumulator.apply( accumulator.apply(0, 1), 2), 3); 1collections.stream().map(Entity::getNum).reduce(0, Integer::sum); // collections求和num 1234//根据typeId分组 entities[{typeId:1,name:\"火锅\"},{typeId:1,name:\"烧烤\"}，{typeId:2,name:\"律师\"}]Map&lt;Integer, List&lt;Entity&gt;&gt; groups = entities.stream() .collect(Collectors.groupingBy(Entity::getTypeId));List&lt;Entity&gt; list = groups.get(typeId); //拿到对应的分组数据 找出长度大于一分钟的曲目 1234567public Set&lt;String&gt; findLongTracks(List&lt;Album&gt; albums){ return albums.stream() .flatMap(album -&gt; album.getTracks()) .filter(track -&gt; track.getLength() &gt; 60) .map(track -&gt; track.getName) .collect(toSet());}","link":"/2018/12/19/java-8部分读书笔记.html"},{"title":"Java设计模式之适配器模式","text":"定义 适配器模式把一个类的接口变换成客户端所期待的另一种接口，从而使原本因接口不匹配而无法在一起工作的两个类能够在一起工作。 适配器模式的用途用电器做例子，笔记本电脑的插头一般都是三相的，即除了阳极、阴极外，还有一个地极。而有些地方的电源插座却只有两极，没有地极。电源插座与笔记本电脑的电源插头不匹配使得笔记本电脑无法使用。这时候一个三相到两相的转换器（适配器）就能解决此问题，而这正像是本模式所做的事情。 适配器模式的结构适配器模式有类的适配器模式和对象的适配器模式两种不同的形式。 类适配器模式类的适配器模式把适配的类的API转换成为目标类的API。 在上图中可以看出，Adaptee类并没有sampleOperation2()方法，而客户端则期待这个方法。为使客户端能够使用Adaptee类，提供一个中间环节，即类Adapter，把Adaptee的API与Target类的API衔接起来。Adapter与Adaptee是继承关系，这决定了这个适配器模式是类的： 模式所涉及的角色有： ● 目标(Target)角色：这就是所期待得到的接口。注意：由于这里讨论的是类适配器模式，因此目标不可以是类。 ● 源(Adapee)角色：现在需要适配的接口。 ● 适配器(Adapter)角色：适配器类是本模式的核心。适配器把源接口转换成目标接口。显然，这一角色不可以是接口，而必须是具体类。 java源码123456789101112131415161718192021222324252627282930313233343536public interface Target { /** * 这是源类Adaptee也有的方法 */ public void sampleOperation1(); /** * 这是源类Adapteee没有的方法 */ public void sampleOperation2(); }/**上面给出的是目标角色的源代码，这个角色是以一个JAVA接口的形式实现的。可以看出，这个接口声明了两个方法：*sampleOperation1()和sampleOperation2()。而源角色Adaptee是一个具体类，它有一个sampleOperation1()方法，但是没有sampleOperation2()方法。*/public class Adaptee { public void sampleOperation1(){}}/**适配器角色Adapter扩展了Adaptee,同时又实现了目标(Target)接口。由于Adaptee没有提供sampleOperation2()方法，而目标接口又要求这个方法，因此适配器角色Adapter实现了这个方法。*/public class Adapter extends Adaptee implements Target { /** * 由于源类Adaptee没有方法sampleOperation2() * 因此适配器补充上这个方法 */ @Override public void sampleOperation2() { //写相关的代码 }} 对象适配器模式与类的适配器模式一样，对象的适配器模式把被适配的类的API转换成为目标类的API，与类的适配器模式不同的是，对象的适配器模式不是使用继承关系连接到Adaptee类，而是使用委派关系连接到Adaptee类。 从上图可以看出，Adaptee类并没有sampleOperation2()方法，而客户端则期待这个方法。为使客户端能够使用Adaptee类，需要提供一个包装(Wrapper)类Adapter。这个包装类包装了一个Adaptee的实例，从而此包装类能够把Adaptee的API与Target类的API衔接起来。Adapter与Adaptee是委派关系，这决定了适配器模式是对象的。 java 源码实现1234567891011121314151617181920212223242526272829303132333435363738public interface Target { /** * 这是源类Adaptee也有的方法 */ public void sampleOperation1(); /** * 这是源类Adapteee没有的方法 */ public void sampleOperation2(); }public class Adaptee { public void sampleOperation1(){} }public class Adapter { private Adaptee adaptee; public Adapter(Adaptee adaptee){ this.adaptee = adaptee; } /** * 源类Adaptee有方法sampleOperation1 * 因此适配器类直接委派即可 */ public void sampleOperation1(){ this.adaptee.sampleOperation1(); } /** * 源类Adaptee没有方法sampleOperation2 * 因此由适配器类需要补充此方法 */ public void sampleOperation2(){ //写相关的代码 }} 类适配器模式和对象适配器模式的权衡 类适配器，使用对象继承的方式，是静态的定义方式；对象适配器，使用对象组合的方式，是动态组合的方式。 对于类适配器，由于适配器直接继承了Adaptee，使得适配器不能和Adaptee的子类一起工作，因为继承是静态的关系 对于对象适配器，一个适配器可以把多种不同的源适配到同一个目标。换言之，同一个适配器可以把源类和它的子类都适配到目标接口。因为对象适配器采用的是对象组合的关系，只要对象类型正确，是不是子类都无所谓。 对于类适配器，适配器可以重定义Adaptee的部分行为，相当于子类覆盖父类的部分实现方法。 对于对象适配器，要重定义Adaptee的行为比较困难，这种情况下，需要定义Adaptee的子类来实现重定义，然后让适配器组合子类。虽然重定义Adaptee的行为比较困难，但是想要增加一些新的行为则方便的很，而且新增加的行为可同时适用于所有的源。 对于类适配器，仅仅引入了一个对象，并不需要额外的引用来间接得到Adaptee。 对于对象适配器，需要额外的引用来间接得到Adaptee。 建议尽量使用对象适配器的实现方式，多用合成/聚合、少用继承。当然，具体问题具体分析，根据需要来选用实现方式，最适合的才是最好的。 适配器模式的优点 更好的复用性 系统需要使用现有的类，而此类的接口不符合系统的需要。那么通过适配器模式就可以让这些功能得到更好的复用。 更好的扩展性 在实现适配器功能的时候，可以调用自己开发的功能，从而自然地扩展系统的功能。 适配器模式的缺点过多的使用适配器，会让系统非常零乱，不易整体进行把握。比如，明明看到调用的是A接口，其实内部被适配成了B接口的实现，一个系统如果太多出现这种情况，无异于一场灾难。因此如果不是很有必要，可以不使用适配器，而是直接对系统进行重构。 缺省适配器模式缺省适配(Default Adapter)模式为一个接口提供缺省实现，这样子类型可以从这个缺省实现进行扩展，而不必从原有接口进行扩展。作为适配器模式的一个特例，缺省是适配模式在JAVA语言中有着特殊的应用。 实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/***鲁智深的故事* 和尚要做什么呢？吃斋、念经、打坐、撞钟、习武等。如果设计一个和尚*接口，*给出所有的和尚都需要实现的方法，那么这个接口应当如下：*/public interface 和尚 { public void 吃斋（）； public void 念经（）； public void 打坐（）； public void 撞钟（）； public void 习武（）； public String getName();}/**显然，所有的和尚类都应当实现接口所定义的全部方法，不然就根本通不过JAVA语言编辑器。像下面的鲁智深类就不行。*/public class 鲁智深 implements 和尚{ public void 习武(){ 拳打镇关西； 大闹五台山； 大闹桃花村； 火烧瓦官寺； 倒拔垂杨柳； } public String getName(){ return \"鲁智深\"; }}/*** 由于鲁智深只实现了getName()和习武()方法，而没有实现任何其他的方法。因此，它根本就通不过Java语言编译器。*鲁智深类只有实现和尚接口的所有的方法才可以通过Java语言编译器，但是这样一来鲁智深就不再是鲁智深了。*以史为鉴，可以知天下。研究一下几百年前鲁智深是怎么剃度成和尚的，会对Java编程有很大的启发。*不错，当初鲁达剃度，众僧说：“此人形容丑恶、相貌凶顽，不可剃度他\",但是长老却说：*”此人上应天星、心地刚直。虽然时下凶顽，命中驳杂，久后却得清净。证果非凡，汝等皆不及他。”*原来如此！看来只要这里也应上一个天星的话，问题就解决了！使用面向对象的语言来说，“应”者，实现也；“天星”者，抽象类也。*/public abstract class 天星 implements 和尚 { public void 吃斋(){} public void 念经(){} public void 打坐(){} public void 撞钟(){} public void 习武(){} public String getName(){ return null; }}/**鲁智深类继承抽象类“天星”*/public class 鲁智深 extends 和尚{ public void 习武(){ 拳打镇关西； 大闹五台山； 大闹桃花村； 火烧瓦官寺； 倒拔垂杨柳； } public String getName(){ return \"鲁智深\"; }} ​ 这个抽象的天星类便是一个适配器类，鲁智深实际上借助于适配器模式达到了剃度的目的。此适配器类实现了和尚接口所要求的所有方法。但是与通常的适配器模式不同的是，此适配器类给出的所有的方法的实现都是“平庸”的。这种“平庸化”的适配器模式称作缺省适配模式。 ​ 在很多情况下，必须让一个具体类实现某一个接口，但是这个类又用不到接口所规定的所有的方法。通常的处理方法是，这个具体类要实现所有的方法，那些有用的方法要有实现，那些没有用的方法也要有空的、平庸的实现。 这些空的方法是一种浪费，有时也是一种混乱。除非看过这些空方法的代码，程序员可能会以为这些方法不是空的。即便他知道其中有一些方法是空的，也不一定知道哪些方法是空的，哪些方法不是空的，除非看过这些方法的源代码或是文档。 缺省适配模式可以很好的处理这一情况。可以设计一个抽象的适配器类实现接口，此抽象类要给接口所要求的每一种方法都提供一个空的方法。就像帮助了鲁智深的“上应天星”一样，此抽象类可以使它的具体子类免于被迫实现空的方法。 缺省适配器模式的结构缺省适配模式是一种“平庸”化的适配器模式。 实现代码12345678910111213141516171819202122public interface AbstractService { public void serviceOperation1(); public int serviceOperation2(); public String serviceOperation3();}public abstract class ServiceAdapter implements AbstractService{ @Override public void serviceOperation1() { } @Override public int serviceOperation2() { return 0; } @Override public String serviceOperation3() { return null; }} 可以看到，接口AbstractService要求定义三个方法，分别是serviceOperation1()、serviceOperation2()、serviceOperation3()；而抽象适配器类ServiceAdapter则为这三种方法都提供了平庸的实现。因此，任何继承自抽象类ServiceAdapter的具体类都可以选择它所需要的方法实现，而不必理会其他的不需要的方法。 适配器模式的用意是要改变源的接口，以便于目标接口相容。缺省适配的用意稍有不同，它是为了方便建立一个不平庸的适配器类而提供的一种平庸实现。 在任何时候，如果不准备实现一个接口的所有方法时，就可以使用“缺省适配模式”制造一个抽象类，给出所有方法的平庸的具体实现。这样，从这个抽象类再继承下去的子类就不必实现所有的方法了。 参考：http://www.cnblogs.com/java-my-life/archive/2012/04/13/2442795.html","link":"/2018/12/04/Java设计模式之适配器模式.html"},{"title":"支付系统设计(转载)","text":"支付系统概述支付系统是连接消费者、商家（或平台）和金融机构的桥梁，管理支付数据，调用第三方支付平台接口，记录支付信息（对应订单号，支付金额等），金额对账等功能，根据不同公司对于支付业务的定位不同大概有几个阶段：第一阶段：支付作为一个（封闭）的、独立的应用系统，为各系统提供支付功能支持。一般来说，这个系统仅限于为公司内部的业务提供支付支持，并且和业务紧密耦合。第二阶段：支付作为一个开发的系统，为公司内外部系统、各种业务提供支付服务，支付服务本身应该是和具体的业务解耦合。 支付系统架构模块组成图 我们先来看一下用户完成一次购物需要进行那些操作： 通常消费者在手机APP或者网站都会涉及到支付相关的业务场景，用户只需要简单点击支付按钮输入支付密码，就可以完成整个支付过程，那么我就和大家一起来看看一个完整的支付系统有什么功能组成和设计时需要考虑那些问题。 支付系统的作用 从上图中我们可以看出真实的资金流向。首先当用户产生支付行为时，资金从用户端流向支付系统，退款时则相反，从支付系统回流至用户端。因此在整个交易过程中用户端与支付系统是双向资金的流动方式。对于支付系统而言，资金有进有出。从支付系统到商户端就比较简单了，在清算完成后支付系统负责将代收的资金结算给商户，通常结算的操作可以在线上来完成（采用支付公司代付接口或者银企直连接口来完成），也可以由公司财务通过线下手工转账的方式来完成，因此这种资金流动的方式是单向的。出于资金安全考虑，大多数公司通常这部分采用线下方式实现。 真实的资金流由支付公司按照约定期限（通常 T+1 ）结算到平台公司对公账户中，然后再由平台公司再按照交易明细进行二次清算后结算给对应的商户。 支付系统支付系统模块组成 完整的支付系统包括如下的功能 应用管理: 同时支持公司多个业务系统对接。 商户管理: 支持商户入驻，商户需要向平台方提供相关的资料备案。 渠道管理: 支持微信、支付宝、银联、京东支付等多种渠道。 账户管理: 渠道账户管理，支持共享账户（个人商户）及自有账户。 支付交易: 生成预支付订单、提供退款服务。 对账管理: 实现支付系统的交易数据与第三方支付渠道交易明细的自动核对（通常T+1），确保交易数据的准确性和一致性。 清算管理: 计算收款交易中商户的应收与支付系统收益。 结算管理: 根据清算结果，将资金划拨至商户对应的资金帐户中。 核心流程支付系统有几个关键的核心流程：支付流程、对账流程、结算流程 支付流程说明 用户在商城选购商品并发起支付请求； 商城将支付订单通过B2C网关收款接口传送至支付网关； 用户选择网银支付及银行，支付平台将订单转送至指定银行网关界面； 用户支付完成，银行处理结果并向平台返回处理结果； 支付平台接收处理结果，落地处理并向商户返回结果； 商城接收到支付公司返回结果，落地处理（更改订单状态）并通知用户。 一般而言支付系统会给商户设置有“可用余额”账户、“待结算”账户；系统在接收到银行返回支付成功信息会进行落地处理，一方面更改对应订单状态，另一方面在商户待结算账户记入一笔金额；该笔金额，系统会根据结算周期从待结算账户—&gt;“可用余额”账户。 退款流程说明 用户在商户平台发起退款申请，商户核实退款信息及申请； 商户登录支付平台账户/或者通过支付公司提供的退款接口向支付平台发起退款； 支付系统会对退款信息校验（退款订单对应的原订单是否支付成功？退款金额是否少于等于原订单金额？），校验商户账户余额是否充足等；校验不通过，则无法退款； 支付系统在商户可用余额账户扣除金额，并将退款订单发送至银行，银行完成退款操作。注意：对于网关收款的订单退款，各银行要求不一，有些银行提供的退款接口要求原订单有效期在90或180天，有些银行不提供退款接口；针对超期或者不支持接口退款的订单，支付公司通过代付通道完成退款操作。 对于收单金额未结算，还在“待结算”账户的订单，如果出现退款情况，业务流程和上述流程差不多，只是从待结算账户进行扣款。 对账说明​ 对账，我们一般称为勾兑，支付系统的对账，包含着两个层面： 支付系统内部间的对账，支付系统一般是分布式的，整个支付系统被拆分成了多个子系统，如交易系统、账户系统、会计系统、账户系统，每个子系统在处理各自的业务，系统间的对账，就是以上系统的核对，用于修正内部系统的数据不一致。 支付系统与渠道的对账，这里的渠道泛指所有为支付系统提供代收付业务的渠道，如：第三方支付公司、银行、清算中心、网联、银联等。 对账简易流程 支付系统与渠道间的对账系统间的对账比较好理解，这里主要讲支付系统与渠道间的对账。支付系统与渠道间的对账，又包含2个维度： 信息流勾对：即业务对账／交易对账，主要是就收单交易的支付信息与银行提供的信息流文件进行勾兑。信息流的勾地能发现支付系统与银行系统间的掉单、两边由于系统间的原因导致的同一笔交易支付金额不一致（可能性很小）或者支付状态不一致。信息流勾兑一般用来恢复掉单数据，可通过补单或者具体系统问题排查解决。 资金流勾对：即资金对账，主要就收单交易的支付信息与银行提供的资金流信息进行勾兑。资金流的勾兑能发现支付系统在银行的帐户资金实际发生的变动与应该发生的变动的差异，比如长款（银行多结算给支付系统）和短款（银行少结算给支付系统）。 说了这么多，就出现来4个对账文件，支付系统信息流文件、支付系统资金流文件、银行信息流文件、银行资金流文件。业务对账（勾兑）就是支付系统的信息流文件与银行的信息流文件勾兑，资金对账即支付系统的资金流文件与银行的资金流文件勾兑。 核对的差异处理1、信息流勾对的差异处理 支付系统信息流没有，而银行有的差异，可能是因为支付系统交易数据的丢失、银行的掉单，如果是银行的掉单，由支付公司的运营登录银行网银确认后，做补单处理，并将差异表中该记录核销。 支付系统信息流有，而银行没有的差异，此种情况一般不会发生，因为支付系统所有的交易数据都是取银行返回状态的数据。 2、资金流勾对对差异处理 支付系统资金流没有，而银行有的差异。可能原因如下：1、银行日切晚与支付系统核心账务系统；2、支付系统账务核心系统与其他系统间的掉单。一旦出现，则会出现长款（即银行不应该结算而实际结算）的现象，对于因日切导致的差异，在第二天的对账中系统会对平，其他原因的，需要技术排查。 支付系统资金流有，而银行没有的差异，可能是因为银行日切早于支付系统的核心账务系统，一旦出现，会出现短款（银行应结算而实际未结算）的现象，银行日切导致段差异，会在下一天与银行的勾对中，将此笔差异勾对上，如果是非日切导致的原因，就需要找银行追款了。 总结就是，业务对账，即信息流对账，支付系统的交易流水与银行的交易流水间核对，保障支付交易完整入账。资金对账，即资金流对账，支付系统的入账流水与银行的结算流水间核对，保障银行入账流水与实际入账资金的匹配。 结算结算流程 在清结算部分，系统按照设定好的清结算规则自动将钱款结算给商户。完善的运营会计体系帮助财务进行精细化核算，提高财务效率。与支付渠道自动进行对账，确保账务正确，在异常情况下能及时定位问题并处理。系统更是能对商户进行个性化的费率配置或账期配置，方便灵活。系统的价值不仅体现在支付清结算方面，同时更是提升了运营管理效率。支付清结算系统可以有效帮助运营、财务、开发以及管理人员。对于运营人员，系统可帮助处理平台的运营工作，包括各类支付管理，商户、会员管理，营销活动的数据统计等，全面提高运营效率。针对财务人员，可以协助完成资金对账、会计处理，出入款管理，账务差错处理等，大部分工作由系统自动处理，减少人工处理，提高资金处理效率。一套灵活便捷的配置后台供开发人员快速调整系统以适应新的业务，并能方便对系统进行维护，如渠道接入、费率配置、账期调整等，提高开发效率。系统提供资金流转过程中各个环节的数据，能够从各个维度进行核算和分析，形成对管理人员的决策支持，从而提高决策效率。 关键表设计 支付系统要点在支付系统中，支付网关和支付渠道的对接是最繁琐重要的功能之一，其中支付网关是对外提供服务的接口，所有需要渠道支持的资金操作都需要通过网关分发到对应的渠道模块上。一旦定型，后续就很少，也很难调整。而支付渠道模块是接收网关的请求，调用渠道接口执行真正的资金操作。每个渠道的接口，传输方式都不尽相同，所以在这里，支付网关相对于支付渠道模块的作用，类似设计模式中的wrapper，封装各个渠道的差异，对网关呈现统一的接口。而网关的功能是为业务提供通用接口，一些和渠道交互的公共操作，也会放置到网关中。 支付系统对其他系统，特别是交易系统，提供的支付服务包括签约，支付，退款，充值，转帐，解约等。有些地方还会额外提供签约并支付的接口，用于支持在支付过程中绑卡。 每个服务实现的流程也是基本类似，包括下单，取消订单，退单，查单等操作。每个操作实现，都包括参数校验，支付路由，生成订单，风险评估，调用渠道服务，更新订单和发送消息这7步，对于一些比较复杂的渠道服务，还会涉及到异步同通知处理的步骤。 网关前置支付网关前置是对接业务系统，为其提供支付服务的模块。它是所有支付服务接口的集成前置，将不同支付渠道提供的接口通过统一的方式呈现给业务方。这样接入方就只需要对接支付网关，增加和调整支付渠道对业务方是透明的。 支付网关前置的设计对整个支付系统的稳定性、功能、性能以及其他非功能性需求有着直接的影响。 在支付网关中需要完成大量的操作，为了保证性能，这些操作都尽量异步化来处理。支付网关前置应保持稳定，尽量减少系统重启等操作对业务方的影响。支付网关也避免不了升级和重启。这可通过基于Nginx的LBS(Load Balance System)网关来解决。LBS在这里有两个作用： 一个是实现负载均衡，一个是隔离支付网关重启对调用的影响。 支付网关也采用多台机器分布式部署，重启时，每个服务器逐个启动。某台服务器重启时，首先从LBS系统中取消注册，重启完成后，再重新注册到LBS上。这个过程对调用方是无感知的。 为了避免接口受攻击，在安全上，还得要求业务方通过HTTPS来访问接口，并提供防篡改机制。防篡改则通过接口参数签名来处理。现在主流的签名是对接口参数按照参数名称排序后，做加密和散列，参考微信的签名规范。 参数校验 所有的支付操作，都需要对输入执行参数校验，避免接口受到攻击。 验证输入参数中各字段的有效性验证，比如用户ID,商户ID,价格，返回地址等参数。 验证账户状态。交易主体、交易对手等账户的状态是处于可交易的状态。 验证订单：如果涉及到预单，还需要验证订单号的有效性，订单状态是未支付。为了避免用户缓存某个URL地址，还需要校验下单时间和支付时间是否超过预定的间隔。 验证签名。签名也是为了防止支付接口被伪造。 一般签名是使用分发给商户的key来对输入参数拼接成的字符串做MD5 Hash或者RSA加密，然后作为一个参数随其他参数一起提交到服务器端。 路由选择根据用户选择的支付方式确定用来完成该操作的合适的支付渠道。用户指定的支付方式不一定是最终的执行支付的渠道。比如用户选择通过工行信用卡来执行支付，但是我们没有实现和工行的对接，而是可以通过第三方支付，比如支付宝、微信支付、易宝支付，或者银联来完成。那如何选择合适的支付渠道，就通过支付路由来实现。支付路由会综合考虑收费、渠道的可用性等因素来选择最优方案 风险评估检查本次交易是否有风险。风控接口返回三种结果：阻断交易、增强验证和放行交易。 阻断交易，说明该交易是高风险的，需要终止，不执行第5个步骤； 增强验证，说明该交易有一定的风险，需要确认下是不是用户本人在操作。这可以通过发送短信验证码或者其他可以验证用户身份的方式来做校验，验证通过后，可以继续执行该交易。 放行交易，即本次交易是安全的，可以继续往下走。 发送消息通过消息来通知相关系统关于订单的变更。风控，信用BI等，都需要依赖这数据做准实时计算。 更新订单对于同步返回的结果，需要在主线程中更新订单的状态，标记是支付成功还是失败。对于异步返回的渠道，需要在异步程序中处理。 异步通知其中涉及到调用远程接口，其延迟不可控。如果调用方一直阻塞等待，很容易超时。引入异步通知机制，可以让调用方在主线程中尽快返回，通过异步线程来得到支付结果。对于通过异步来获取支付结果的渠道接口，也需要对应的在异步通知中将结果返回给调用方。 异步通知需要调用方提供一个回调地址，一般以http或者https的方式。这就有技术风险，如果调用失败，还需要重试。而重试不能过于频繁，需要逐步拉大每一次重试的时间间隔。 在异步处理程序中，订单根据处理结果变更状态后，也要发消息通知相关系统。 生成交易订单将订单信息持久化到数据库中。当访问压力大的时候，数据库写入会成为一个瓶颈。 交易流水和记账每一笔交易都需要记录流水，并登记到个人和机构的分户账户上，统计和分析也需要根据交易流水来更新相关数据。 而个人和机构账户总额更新、交易流水记录以及库存的处理，更是需要事务处理机制的支持。 从性能角度， 可以弱化了事务处理的要求，采用消息机制来异步化和交易相关的数据处理。 在支付网关前置的主流程中，仅记录交易流水，即将当前的请求保存到数据库中。 完成数据记录后，发送MQ出来，记账、统计、分析，都是接收MQ来完成数据处理。 涉及到本地资金支付，比如钱包支付，会需要分布式事务处理，扣减账号余额，记账，扣减库存等，每个操作失败，都要回滚。阿里有很不错的分享，这里不详细描述。 当交易量上来后，需要考虑交易表的分表分库的事情。分表分库有两个策略，按照流水号或者交易主体id来走。后者可以支持按用户来获取交易记录。我们用的是前者。后者可以走elastic，确保数据库专用。风控，信用和统计所需要的数据，通过MQ同步到历史库里面。作为支付系统最有价值的数据，在存储上做到专库专用，无可厚非，毕竟存储成本还是廉价的。 支付路由支付路由是一个复杂的话题。对支付系统来说，能支持的支付方式越多越好，不能由于支付方式的不支持断了财路。现实中的支付方式多得难以置信。用户随时甩出一张你听都没听说过的卡。如果一个银行卡只有几个用户在用，那针对这个卡开发个对接有点得不尝失。现在第三方支付的爆发，确实给开发支付系统省了不少事。但是公司不可能只对接一个第三方支付，如果这个渠道出问题了，或者闹矛盾了，把链接给掐了，老板还不欲哭无泪。总之，得对接多个渠道。对于交易量大的银行，还得考虑直联。 渠道接入对于支付渠道，首先考虑的是接入哪些渠道。要对接的渠道按优先级有： 第三方支付，对大部分应用来说，支付宝和微信支付都是必须的，一般来说，这两者可以占到90%以上的交易量。用户不需要绑卡，授权后直接支付就行。各种平台都支持，性能和稳定性都不错。对于一些特殊业务，比如游戏，企业支付，可以查看一些专用的第三方支付平台。 银联，它的存在，极大方便了和银行的对接。和第三方支付主要不同在两个地方一是需要绑卡，也就是用户先把卡号，手机，身份证号提供出来。这一步会折损不少用户。绑卡后，以后的支付操作就简单了，用户只需要输入密码就行。手机客户端不需要像第三方支付那样安装SDK，都在服务器端完成。当然，这是针对快捷支付。网银支付还是挺麻烦的。银联接入也需要ADSS认证。 银行：2018年2月9日银监会公布了最新权威数字：一共【4549家】开发性金融机构1家：国家开发银行；政策性银行2家：进出口银行、农业发展银行；5大国有银行：工、建、农、中、交；邮储银行1家；全国性股份制商业银行12家：招行、中信、兴业、民生、浦发、光大、广发、华夏、平安、浙商、渤海、恒丰；金融资产管理公司4家：信达、华融、长城、东方四大AMC；城商行134家；住房储蓄银行1家；民营银行17家，如网商银行；农商行1262家；农村合作银行33家；农村信用社965家；村镇银行1562家；贷款公司13家；农村资金互助社48家；外资法人银行39家；信托公司68家；金融租赁公司69家；企业集团财务公司247家；汽车金融公司25家；消费金融公司22家；货币经纪公司5家；其他金融机构14家。一般对接一个银行预计有3周左右的工作量，大部分银行需要专线接入，费用和带宽有关，一年也得几万费用。不同银行对接入环境有不同要求，这也是成本。 手机支付：比如苹果的In-App支付， 三星支付、华为支付等， 这些支付仅针对特定的手机型号， 支持NFC等，根据业务需要也可以接入。 总结支付系统是一个繁杂的系统，其中涉及了各种错综复杂的业务流程，以上只是简单介绍了支付系统我们能看见的一些问题和设计，还有后续的系统保障没有写出来，没写出来的才是关键部分，比如：支付系统监控（业务监控分类、渠道监控、商户监控、账户监控）文章只是引子， 架构不是静态的，而是动态演化的。只有能够不断应对环境变化的系统，才是有生命力的系统。所以即使你掌握了以上所有的业务细节，仍然需要演化式思维，在设计的同时，借助反馈和进化的力量推动架构的持续演进。 原文转载自","link":"/2018/12/01/支付系统设计-转载.html"},{"title":"Java设计模式之装饰者模式","text":"问题引入咖啡店的类设计： 一个饮料基类，各种饮料类继承这个基类，并且计算各自的价钱。 饮料中需要加入各种调料，考虑在基类中加入一些布尔值变量代表是否加入各种调料，基类的cost()中的计算各种调料的价钱，子类覆盖cost()，并且在其中调用超类的cost()，加上特定饮料的价钱，计算出子类特定饮料的价钱。 缺点：类数量爆炸、基类加入的新功能并不适用于所有的子类、调料价钱的改变、新调料的出现都会要求改变现有代码；有的子类并不适合某些调料等情况…… 设计原则 类应该对扩展开放，对修改关闭。 我们的目标是允许类容易扩展，在不修改现有代码的情况下，就可搭配新的行为。 如能实现这样的目标，有什么好处呢？这样的设计具有弹性可以应对改变，可以接受新的功能来应对改变的需求。 要让OO设计同时具备开放性和关闭性，不是一件容易的事，通常来说，没有必要把设计的每个部分都这么设计。 遵循开放-关闭原则，通常会引入新的抽象层次，增加代码的复杂度。 我们需要把注意力集中在设计中最有可能改变的地方，然后应用开放-关闭原则。 用装饰者模式解决问题解决咖啡店饮料问题的方法： 以饮料为主体，然后在运行时以调料来“装饰”饮料。 比如，顾客想要摩卡（Mocha）和奶泡（Whip）深焙咖啡（DarkRoast）： DarkRoast继承自Beverage，有一个cost()方法。 第一步，以DarkRoast对象开始； 第二步，顾客想要摩卡，所以建立一个Mocha装饰者对象，并用它将DarkRoast对象包装（wrap）起来； 第三步，顾客想要奶泡，所以建立一个Whip装饰者对象，并用它将Mocha对象包起来；（Mocha和Whip也继承自Beverage，有一个cost()方法）； 最后，为顾客算钱，通过调用最外圈装饰者（Whip）的cost()就可以。Whip()的cost()会先委托它装饰的对象（Mocha）计算出价钱，然后在加上奶泡的价钱。Mocha的cost()也是类似。 装饰者模式的特点 装饰者和被装饰对象有相同的超类型。 可以用一个或多个装饰者包装一个对象。 因为装饰者和被装饰者具有相同的类型，所以任何需要原始对象的场合，可以用装饰过的对象代替。 装饰者可以在所委托被装饰者的行为之前与/或之后，加上自己的行为，以达到特定的目的。 对象可以在任何时候被装饰，所以可以在运行时动态地、不限量地用你喜欢的装饰者来装饰对象。 装饰者模式的定义装饰者模式动态地将责任附加到对象上。若要扩展功能，装饰者提供了比继承更有弹性的替代方案。 装饰者模式的实现实现类图 装饰者和被装饰者具有共同的超类，利用继承达到“类型匹配”，而不是利用继承获得“行为”；将装饰者和被装饰者组合时，加入新的行为。 实现Java代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133// 抽象饮料类(抽象组件)public abstract class Beverage { String description = \"Unkown Beverage\"; public String getDescription() { return description; } /** * 抽象价格计算方法 * @return */ public abstract double cost();}// 浓缩饮料public class Espresso extends Beverage { public Espresso() { description = \"Espresso\"; } @Override public double cost() { return 1.99; }}// 又一饮料public class HouseBlend extends Beverage { public HouseBlend() { description = \"House Blend\"; } @Override public double cost() { return .20; }}// 抽象装饰者类public abstract class CondimentDecorator extends Beverage { /** * 为了后面的调料都能够获取到自己调料的描述 */ public abstract String getDescription();}/** * @desc Mocha调料(具体装饰者) */public class Mocha extends CondimentDecorator { Beverage beverage; public Mocha(Beverage beverage) { this.beverage = beverage; } @Override public String getDescription() { return beverage.getDescription() + \",Mocha\"; } @Override public double cost() { return .20 + beverage.cost(); }}/** * @desc Soy调料(具体装饰者) */public class Soy extends CondimentDecorator { Beverage beverage; public Soy(Beverage beverage) { this.beverage = beverage; } @Override public String getDescription() { return beverage.getDescription() + \",Soy\"; } @Override public double cost() { return .60 + beverage.cost(); }}/** * @desc Whip调料(具体装饰者) */public class Whip extends CondimentDecorator { Beverage beverage; public Whip(Beverage beverage) { this.beverage = beverage; } @Override public String getDescription() { return beverage.getDescription() + \",Whip\"; } @Override public double cost() { return .40 + beverage.cost(); }}/** * @desc 测试装饰者模式 */public class MainTest { public static void main(String[] args) { // 创建一种调料 Beverage beverage = new Espresso(); // 描述和价格 System.out.println(beverage.getDescription() + \" $\" + beverage.cost()); Beverage beverage1 = new HouseBlend(); beverage1 = new Mocha(beverage1); beverage1 = new Whip(beverage1); beverage1 = new Soy(beverage1); System.out.println(beverage1.getDescription() + \" $\" + beverage1.cost()); Beverage beverage2 = new Espresso(); beverage2 = new Mocha(beverage2); beverage2 = new Whip(beverage2); beverage2 = new Soy(beverage2); beverage2 = new Mocha(beverage2); System.out.println(beverage2.getDescription() + \" $\" + beverage2.cost()); }} 测试结果 装饰者和被装饰者具有共同的超类，利用继承达到“类型匹配”，而不是利用继承获得“行为”；将装饰者和被装饰者组合时，加入新的行为。 解决本文中饮料的具体问题时，图中Component即为Beverage（可以是抽象类或者接口），而ConcreteComponent为各种饮料，Decorator（抽象装饰者）为调料的抽象类或接口，ConcreteDecoratorX则为各种具体的调料。 因为使用对象组合，可以把饮料和调料更有弹性地加以混合与匹配。 代码外部细节： 代码中实现的时候，通过构造函数将被装饰者传入装饰者中即可，如最后的调用形式如下： 123Beverage beverage = new DarkRoast();beverage = new Mocha(beverage);beverage = new Whip(beverage); 即完成了两层包装，此时再调用beverage的cost()函数即可得到总价。 java.io包内的装饰者模式 装饰者模式的缺点：在设计中加入大量的小类，如果过度使用，会让程序变得复杂。 参考：http://www.cnblogs.com/mengdd/archive/2013/01/03/2843439.html","link":"/2018/11/29/Java设计模式之装饰者模式.html"},{"title":"Java设计模式之工厂模式","text":"工厂模式序言工厂模式在《Java与模式》中分为三类： 简单工厂模式（Simple Factory）：不利于产生系列产品； 工厂方法模式（Factory Method）：又称为多形性工厂； 抽象工厂模式（Abstract Factory）：又称为工具箱，产生产品族，但不利于产生新的产品； 这三种模式从上到下逐步抽象，并且更具一般性。GOF在《设计模式》一书中将工厂模式分为两类：工厂方法模式（Factory Method）与抽象工厂模式（Abstract Factory）。将简单工厂模式（Simple Factory）看为工厂方法模式的一种特例，两者归为一类。 简单工厂模式 简单工厂模式又称静态工厂方法模式。从命名上就可以看出这个模式一定很简单。它存在的目的很简单：定义一个用于创建对象的接口。在简单工厂模式中,一个工厂类处于对产品类实例化调用的中心位置上,它决定那一个产品类应当被实例化, 如同一个交通警察站在来往的车辆流中,决定放行那一个方向的车辆向那一个方向流动一样。 组成角色： 工厂类角色：这是本模式的核心，含有一定的商业逻辑和判断逻辑。在java中它往往由一个具体类实现。 抽象产品角色：它一般是具体产品继承的父类或者实现的接口。在java中由接口或者抽象类来实现。 具体产品角色：工厂类所创建的对象就是此角色的实例。在java中由一个具体类实现。 简单工厂模式的UML图 简单工厂模式的Java代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// 抽象接口 人类public interface Human { public void say();}// 男人public class Man implements Human { @Override public void say() { System.out.println(\"男人\"); }}// 女人public class Woman implements Human { @Override public void say() { System.out.println(\"女人\"); }}// 简单工厂public class SampleFactory { public static Human makeHuman(String type){ if(type.equals(\"man\")){ Human man = new Man(); return man; }else if(type.equals(\"womman\")){ Human woman = new Woman(); return woman; }else{ System.out.println(\"生产不出来\"); return null; } }}// 简单工厂模式反射实现public class SampleFactory1 { public static Human makeHuman(Class c){ Human human = null; try { human = (Human) Class.forName(c.getName()).newInstance(); } catch (InstantiationException e) { // TODO Auto-generated catch block System.out.println(\"不支持抽象类或接口\"); e.printStackTrace(); } catch (IllegalAccessException e) { // TODO Auto-generated catch block e.printStackTrace(); System.out.println(\"没有足够权限，即不能访问私有对象\"); } catch (ClassNotFoundException e) { // TODO Auto-generated catch block System.out.println(\"类不存在\"); e.printStackTrace(); } return human; }}// 简单工厂测试public class Client { public static void main(String[] args) { Human man = SampleFactory.makeHuman(\"man\"); man.say(); Human womman = SampleFactory.makeHuman(\"womman\"); womman.say(); Human test = SampleFactory.makeHuman(\"tttt\"); Human man = SampleFactory1.makeHuman(Man.class); man.say(); Human woman = SampleFactory1.makeHuman(Woman.class); woman.say(); }} 优缺点： 优点：工厂类是整个模式的关键.包含了必要的逻辑判断,根据外界给定的信息,决定究竟应该创建哪个具体类的对象.通过使用工厂类,外界可以从直接创建具体产品对象的尴尬局面摆脱出来,仅仅需要负责“消费”对象就可以了。而不必管这些对象究竟如何创建及如何组织的．明确了各自的职责和权利，有利于整个软件体系结构的优化。 缺点：由于工厂类集中了所有实例的创建逻辑，违反了高内聚责任分配原则，将全部创建逻辑集中到了一个工厂类中；它所能创建的类只能是事先考虑到的，如果需要添加新的类，则就需要改变工厂类了。当系统中的具体产品类不断增多时候，可能会出现要求工厂类根据不同条件创建不同实例的需求．这种对条件的判断和对具体产品类型的判断交错在一起，很难避免模块功能的蔓延，对系统的维护和扩展非常不利； 工厂方法模式 工厂方法模式是简单工厂模式的进一步抽象化和推广，工厂方法模式里不再只由一个工厂类决定那一个产品类应当被实例化,这个决定被交给抽象工厂的子类去做。 组成角色： 抽象工厂角色： 这是工厂方法模式的核心，它与应用程序无关。是具体工厂角色必须实现的接口或者必须继承的父类。在java中它由抽象类或者接口来实现。 具体工厂角色：它含有和具体业务逻辑有关的代码。由应用程序调用以创建对应的具体产品的对象。 抽象产品角色：它是具体产品继承的父类或者是实现的接口。在java中一般有抽象类或者接口来实现。 具体产品角色：具体工厂角色所创建的对象就是此角色的实例。在java中由具体的类来实现。 工厂方法模式使用继承自抽象工厂角色的多个子类来代替简单工厂模式中的“上帝类”。正如上面所说，这样便分担了对象承受的压力；而且这样使得结构变得灵活 起来——当有新的产品（即暴发户的汽车）产生时，只要按照抽象产品角色、抽象工厂角色提供的合同来生成，那么就可以被客户使用，而不必去修改任何已有的代 码。可以看出工厂角色的结构也是符合开闭原则的！ 工厂方法模式Java代码1234567891011121314151617181920212223242526272829303132333435363738394041424344//抽象产品角色public interface Moveable { void run();}//具体产品角色public class Plane implements Moveable { @Override public void run() { System.out.println(\"plane....\"); }}public class Broom implements Moveable { @Override public void run() { System.out.println(\"broom.....\"); }}//抽象工厂public abstract class VehicleFactory { abstract Moveable create();}//具体工厂public class PlaneFactory extends VehicleFactory{ public Moveable create() { return new Plane(); }}public class BroomFactory extends VehicleFactory{ public Moveable create() { return new Broom(); }}//测试类public class Test { public static void main(String[] args) { VehicleFactory factory = new BroomFactory(); Moveable m = factory.create(); m.run(); }} 可以看出工厂方法的加入，使得对象的数量成倍增长。当产品种类非常多时，会出现大量的与之对应的工厂对象，这不是我们所希望的。因为如果不能避免这种情 况，可以考虑使用简单工厂模式与工厂方法模式相结合的方式来减少工厂类：即对于产品树上类似的种类（一般是树的叶子中互为兄弟的）使用简单工厂模式来实 现。 简单工厂和工厂方法模式的比较 工厂方法模式和简单工厂模式在定义上的不同是很明显的。工厂方法模式的核心是一个抽象工厂类,而不像简单工厂模式, 把核心放在一个实类上。工厂方法模式可以允许很多实的工厂类从抽象工厂类继承下来, 从而可以在实际上成为多个简单工厂模式的综合,从而推广了简单工厂模式。 反过来讲,简单工厂模式是由工厂方法模式退化而来。设想如果我们非常确定一个系统只需要一个实的工厂类, 那么就不妨把抽象工厂类合并到实的工厂类中去。而这样一来,我们就退化到简单工厂模式了。 抽象工厂模式1234567891011121314151617181920212223242526272829303132333435//抽象工厂类public abstract class AbstractFactory { public abstract Vehicle createVehicle(); public abstract Weapon createWeapon(); public abstract Food createFood();}//具体工厂类，其中Food,Vehicle，Weapon是抽象类，public class DefaultFactory extends AbstractFactory{ @Override public Food createFood() { return new Apple(); } @Override public Vehicle createVehicle() { return new Car(); } @Override public Weapon createWeapon() { return new AK47(); }}//测试类public class Test { public static void main(String[] args) { AbstractFactory f = new DefaultFactory(); Vehicle v = f.createVehicle(); v.run(); Weapon w = f.createWeapon(); w.shoot(); Food a = f.createFood(); a.printName(); }} 在抽象工厂模式中，抽象产品 (AbstractProduct) 可能是一个或多个，从而构成一个或多个产品族(Product Family)。 在只有一个产品族的情况下，抽象工厂模式实际上退化到工厂方法模式。 总结 简单工厂模式是由一个具体的类去创建其他类的实例，父类是相同的，父类是具体的。 工厂方法模式是有一个抽象的父类定义公共接口，子类负责生成具体的对象，这样做的目的是将类的实例化操作延迟到子类中完成。 抽象工厂模式提供一个创建一系列相关或相互依赖对象的接口，而无须指定他们具体的类。它针对的是有多个产品的等级结构。而工厂方法模式针对的是一个产品的等级结构。 参考：http://www.cnblogs.com/liaoweipeng/p/5768197.html http://www.cnblogs.com/forlina/archive/2011/06/21/2086114.html","link":"/2018/11/24/Java设计模式之工厂模式.html"},{"title":"Effective-Java-2-遇到多个构造器参数时考虑用构建器","text":"遇到多个构造器参数时考虑用构建器静态工厂和构造器有个共同的局限性：它们都不能很好地扩展到大量的可选参数。当有超过20个可选域是必须的时候，对于此种情况，程序员一般考虑采用重叠构造器模式。这种模式下，提供第一个只有必要参数的构造器，第二个构造器有一个可选参数，第三个有两个可选参数，以此类推，最后一个构造器包含所有的参数。 重叠构造器模式 含有四个可选域的情况 1234567891011121314151617181920212223242526272829public class NutritionFacts{ private final int servingSize; // required private final int servings; // required private final int calories; // optional private final int fat; // optional private final int sodium; // optional private final int carbohydrate; // optional public NutritionFacts(int servingSize, int servings){ this(servingSize, servings, 0); } public NutritionFacts(int servingSize, int servings, int calories){ this(servingSize, servings, calories, 0); } public NutritionFacts(int servingSize, int servings, int calories, int fat){ this(servingSize, servings, calories, fat, 0); } public NutritionFacts(int servingSize, int servings, int calories, int fat, int sodium){ this(servingSize, servings, calories, fat, sodium, 0); } public NutritionFacts(int servingSize, int servings, int calories, int fat, int sodium, int carbohydrate){ this(servingSize, servings, calories, fat, sodium, carbohydrate); } } 当你想创建实例的时候，就利用参数列表最短的构造器。 重叠构造器模式可行，但是当有许多参数的时候，客户端代码会很难编写，并且较难阅读，使用的时候容易混淆部分参数容易出错 JavaBeans模式这种模式调用一个无参构造器来创建对象，然后用setter方法来设置必要的参数以及相关参数的值。 12345678910111213141516171819public class NutritionFacts { private int servingSize = -1; // required private int servings = -1; // required private int calories = 0; // optional private int fat = 0; // optional private int sodium = 0; // optional private int carbohydrate = 0; // optional public NutritionFacts(){ } public void setServingSize(int val) { servingSize = val; } public void setServings(int val) { servings = val; } public void setCalories(int val) { calories = val; } public void setFat(int val) { fat = val; } public void setSodium(int val) { sodium = val; } public void setCarbohydrate(int val) { carbohydrate = val; }} 这种方式弥补了重叠构造器模式的不足，创建实例容易，阅读代码也容易。 12345NutritionFacts cocaCola = new NutritionFacts();cocaCola.setServingSize(10);cocaCola.setServings(10);cocaCola.setCalories(10);cocaCola.setFat(10); 遗憾的是自身有严重的缺陷。构造过程被分到了几个调用中，构造过程中JavaBean可能处于不一致状态的对象，将会导致失败。类无法仅仅通难过校验构造器参数的有效性来保证一致性，Javabeans模式阻止了把类做成不可变的可能，需要付出额外的努力来确保它的线程安全。 Builder模式既能保证像重叠构造器模式那样的安全性，也能保证像JavaBeans模式那样的可读性。 不直接生成想要的对象，而是让客户端利用所有必要的参数调用构造器（或静态工厂），得到一个builder对象。然后客户端在builder对象上调用类似于setter的方法，来设置每个相关的可选参数。最后，客户端调用无参的build方法来生成不可变的对象，这个builder是它构建的类的静态成员类。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class NutritionFacts{ private final int servingSize; // required private final int servings; // required private final int calories; // optional private final int fat; // optional private final int sodium; // optional private final int carbohydrate; // optional public static class Builder{ private final int servingSize; // required private final int servings; // required private final int calories = 0; // optional private final int fat = 0; // optional private final int sodium = 0; // optional private final int carbohydrate = 0; // optional public Builder(int servingSize, int servings){ this.servingSize = servingSize; this.servings =servings; } public Builder calories(int val){ calories = val; return this; } public Builder fat(int val){ fat = val; return this; } public Builder sodium(int val){ sodium = val; return this; } public Builder carbohydrate(int val){ carbohydrate = val; return this; } public NutritionFacts build(){ return new NutritionFacts(this); } } private NutritionFacts(Builder builder){ servingSize = builder.servingSize; servings = builder.servingSize; calories = builder.servingSize; fat = builder.servingSize; sodium = builder.servingSize; carbohydrate = builder.servingSize; }} 注意NutritionFacts是不可变的，所有的默认参数值都单独放一个地方。builder的setter方法返回builder本身，以便可以把调用用调用链连接起来。 1NutritionFacts cocaCola = new NutritionFacts.Builder(200,20).calories(10).fat(15).sodium(10).build(); builder像个构造器一样，可以对其参数强加约束条件。build方法可以检验这些约束条件，将参数从builder拷贝到对象中之后，并在对象域而不是builder域中对他们进行校验，这一点很重要。如果违反了任何约束条件，build方法就应该抛出IllegalStateException,显示违背了哪个约束条件。 对多个参数强加约束条件的另一个方法，用多个setter方法对某个约束条件必须持有的所有参数进行检查。如果该约束条件没有得到满足，setter方法就抛出IllegalStateException，不用等到在build的时候。 设置了参数的builder生成了一个很好的抽象工厂，客户端可以将这样一个builder传给方法，使该方法能够为客户端创建一个或者多个对象。要使用这种用法，需要有个类型来表示builder，只要一个泛型就能满足所有的builder，无论他们在构建哪种类型的对象： 123public interface Builder&lt;T&gt;{ public T build();} 可以声明NutritionFacts.Builder类来实现Builder&lt;NutritionFacts&gt; 。 带有Builder实例的方法通常利用有限制的通配符类型来约束构建器的类型参数。eg.下面就是构建每个节点的方法，它利用一个客户端提供的Builder实例来构建树： 1Tree buildTree(Builder&lt;? extends Node&gt; nodeBuilder){ ... } **Builder模式还比重叠构造器模式更加冗长，因此它只有在很难参数的时候才使用，比如4个或者更多。但是你要记住，将来可能添加参数。简而言之，如果类的构造器或者静态工厂中具有多个参数，设计这种类时，Builder模式就是种不错的选择，特别是大多参数都是可选的时候。代码易于阅读编写，构建器也比JavaBeans更加安全。","link":"/2018/11/17/Effective-Java-2-遇到多个构造器参数时考虑用构建器.html"},{"title":"Java设计模式之单例模式","text":"单例模式 确保一个类只有一个实例，并提供一个全局访问点！ 饿汉式：线程安全，但效率比较低 123456789101112131415161718/** * 单例模式的实现：饿汉式,线程安全 但效率比较低 */ public class SingletonTest { // 定义一个私有的构造方法 private SingletonTest() { } // 将自身的实例对象设置为一个属性,并加上Static和final修饰符 private static final SingletonTest instance = new SingletonTest(); // 静态方法返回该类的实例 public static SingletonTest getInstancei() { return instance; } } 单例模式的实现：饱汉式，非线程安全12345678910111213141516171819/** * 单例模式的实现：饱汉式,非线程安全 * */ public class SingletonTest { // 定义私有构造方法（防止通过 new SingletonTest()去实例化） private SingletonTest() { } // 定义一个SingletonTest类型的变量（不初始化，注意这里没有使用final关键字） private static SingletonTest instance; // 定义一个静态的方法（调用时再初始化SingletonTest，但是多线程访问时，可能造成重复初始化问题） public static SingletonTest getInstance() { if (instance == null) instance = new SingletonTest(); return instance; } } 饱汉式，线程安全简单实现1234567891011121314151617181920/** * 单例模式的实现：饱汉式,线程安全简单实现 * */ public class SingletonTest { // 定义私有构造方法（防止通过 new SingletonTest()去实例化） private SingletonTest() { } // 定义一个SingletonTest类型的变量（不初始化，注意这里没有使用final关键字） private static SingletonTest instance; // 定义一个静态的方法（调用时再初始化SingletonTest，使用synchronized 避免多线程访问时，可能造成重的复初始化问题） public static synchronized SingletonTest getInstance() { if (instance == null) instance = new SingletonTest(); return instance; } } 双重锁机制：线程安全，效率高，单例模式最优方案12345678910111213141516171819202122232425262728/** * 单例模式最优方案 * 线程安全 并且效率高 * */ public class SingletonTest { // 定义一个私有构造方法 private SingletonTest() { } //定义一个静态私有变量(不初始化，不使用final关键字，使用volatile保证了多线程访问时instance变量的可见性，避免了instance初始化时其他变量属性还没赋值完时，被另外线程调用) private static volatile SingletonTest instance; //定义一个共有的静态方法，返回该类型实例 public static SingletonTest getIstance() { // 对象实例化时与否判断（不使用同步代码块，instance不等于null时，直接返回对象，提高运行效率） if (instance == null) { //同步代码块（对象未初始化时，使用同步代码块，保证多线程访问时对象在第一次创建后，不再重复被创建） synchronized (SingletonTest.class) { //未初始化，则初始instance变量 if (instance == null) { instance = new SingletonTest(); } } } return instance; } } 静态内部类方式12345678910111213/** * 静态内部类方式 * */ public class Singleton { private static class SingletonHolder { private static final Singleton INSTANCE = new Singleton(); } private Singleton (){} public static final Singleton getInstance() { return SingletonHolder.INSTANCE; } } 这种方式同样利用了classloder的机制来保证初始化instance时只有一个线程，它跟第三种和第四种方式不同的是（很细微的差别）：第三种和第四种方式是只要Singleton类被装载了，那么instance就会被实例化（没有达到lazy loading效果），而这种方式是Singleton类被装载了，instance不一定被初始化。因为SingletonHolder类没有被主动使用，只有显示通过调用getInstance方法时，才会显示装载SingletonHolder类，从而实例化instance。想象一下，如果实例化instance很消耗资源，我想让他延迟加载，另外一方面，我不希望在Singleton类加载时就实例化，因为我不能确保Singleton类还可能在其他的地方被主动使用从而被加载，那么这个时候实例化instance显然是不合适的。这个时候，这种方式相比第三和第四种方式就显得很合理。 总结【以上单例模式】传统的两私有一公开（私有构造方法、私有静态实例(懒实例化/直接实例化)、公开的静态获取方法）涉及线程安全问题（即使有多重检查锁也可以通过反射破坏单例）目前最为安全的实现单例的方法是通过内部静态enum的方法来实现，因为JVM会保证enum不能被反射并且构造器方法只执行一次。 利用反射模式获取1234567891011121314151617181920212223242526272829303132333435363738// 饿汉试单例模式public class HelloWorld { private HelloWorld(){}; private static HelloWorld hell = new HelloWorld(); public static HelloWorld getHello(){ return hell; } public void getWorld(){ System.out.println(\"hahahahah\"); }}// java反射机制 调用getWorld()方法public class HelloJava{ public static void main(String[] args){ /* HelloWorld hell = HelloWorld.getHello(); hell.getWorld(); */ try { Class class1 = Class.forName(\"cn.jr.text.HelloWorld\"); Constructor[] constructors = class1.getDeclaredConstructors(); AccessibleObject.setAccessible(constructors, true); for (Constructor con : constructors) { if (con.isAccessible()) { Object classObject = con.newInstance(); Method method = class1.getMethod(\"getWorld\"); method.invoke(classObject); } } } catch (Exception e) { e.printStackTrace(); } }} 使用枚举的单例模式123456789101112131415161718public class EnumSingleton{ private EnumSingleton(){} public static EnumSingleton getInstance(){ return Singleton.INSTANCE.getInstance(); } private static enum Singleton{ INSTANCE; private EnumSingleton singleton; //JVM会保证此方法绝对只调用一次 private Singleton(){ singleton = new EnumSingleton(); } public EnumSingleton getInstance(){ return singleton; } }} 使用枚举，static处调用，初始化一次1234567891011121314151617181920212223public class StaticInitTest { private static List&lt;Integer&gt; dataList = null; static{ dataList = Singleton.INSTANCE.init(); } private static enum Singleton { INSTANCE; private List&lt;Integer&gt; list; private Singleton(){ fillData(); } private void fillData(){ list = new ArrayList&lt;Integer&gt;(5); for(int i =1; i&lt;6; i++){ list.add(i); } } public List&lt;Integer&gt; init(){ return list; } }} 借助CAS（AtomicReference）实现单例模式：12345678910111213141516171819public class Singleton { private static final AtomicReference&lt;Singleton&gt; INSTANCE = new AtomicReference&lt;Singleton&gt;(); private Singleton() {} public static Singleton getInstance() { for (;;) { Singleton singleton = INSTANCE.get(); if (null != singleton) { return singleton; } singleton = new Singleton(); if (INSTANCE.compareAndSet(null, singleton)) { return singleton; } } }} 用CAS的好处在于不需要使用传统的锁机制来保证线程安全,CAS是一种基于忙等待的算法,依赖底层硬件的实现,相对于锁它没有线程切换和阻塞的额外消耗,可以支持较大的并行度。使用CAS实现单例只是个思路而已，只是拓展一下帮助读者熟练掌握CAS以及单例等知识、千万不要在代码中使用！！！这个代码其实有很大的优化空间。聪明的你，知道以上代码存在哪些隐患吗？ 最终总结有两个问题需要注意： 如果单例由不同的类装载器装入，那便有可能存在多个单例类的实例。假定不是远端存取，例如一些servlet容器对每个servlet使用完全不同的类 装载器，这样的话如果有两个servlet访问一个单例类，它们就都会有各自的实例。 如果Singleton实现了java.io.Serializable接口，那么这个类的实例就可能被序列化和复原。不管怎样，如果你序列化一个单例类的对象，接下来复原多个那个对象，那你就会有多个单例类的实例。 对第一个问题修复的办法是： 123456789private static Class getClass(String classname) throws ClassNotFoundException { ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); if(classLoader == null) classLoader = Singleton.class.getClassLoader(); return (classLoader.loadClass(classname)); } } 对第二个问题修复的办法是： 12345678910public class Singleton implements java.io.Serializable { public static Singleton INSTANCE = new Singleton(); protected Singleton() { } private Object readResolve() { return INSTANCE; } } 对我来说，我比较喜欢第a和e种方式，简单易懂，而且在JVM层实现了线程安全（如果不是多个类加载器环境），一般的情况下，我会使用第a种方式，只有在要明确实现lazy loading效果时才会使用第e种方式，另外，如果涉及到反序列化创建对象时我会试着使用枚举的方式来实现单例，不过，我一直会保证我的程序是线程安全的，如果有其他特殊的需求，我可能会使用第七种方式，毕竟，JDK1.5已经没有双重检查锁定的问题了。 参考资料：java单例之enum实现方式 设计模式 java设计模式–单例模式","link":"/2018/11/17/Java设计模式之单例模式.html"},{"title":"leetcode-2-Add Two Numbers","text":"Add Two Numbers You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list. You may assume the two numbers do not contain any leading zero, except the number 0 itself. Example:Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4) Output: 7 -&gt; 0 -&gt; 8 Explanation: 342 + 465 = 807. common 1234567891011121314151617181920212223242526272829/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */class Solution { public ListNode addTwoNumbers(ListNode l1, ListNode l2) { ListNode dummyHead = new ListNode(0); // 一点要赋值一个节点，进行操作 ListNode p = l1, q = l2, curr = dummyHead; int carry = 0; while (p != null || q != null) { int x = (p != null) ? p.val : 0; int y = (q != null) ? q.val : 0; int sum = carry + x + y; carry = sum / 10; curr.next = new ListNode(sum % 10); curr = curr.next; if (p != null) p = p.next; if (q != null) q = q.next; } if (carry &gt; 0) { curr.next = new ListNode(carry); } return dummyHead.next; }} best123456789101112131415161718192021222324252627282930/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ class Solution { public ListNode addTwoNumbers(ListNode l1, ListNode l2) { ListNode head = new ListNode(0); int carry = 0; while(l1!=null||l2!=null||carry&gt;0) { ListNode itr = head; while(itr.next!=null) itr = itr.next; // 寻找最后一个节点 int sum = ( (l1==null ? 0 : l1.val) + (l2==null ? 0 : l2.val) + carry); carry = sum/10; ListNode temp = new ListNode(sum%10); itr.next = temp; if(l1!=null) l1 = l1.next; if(l2!=null) l2 = l2.next; } return head.next; }}","link":"/2018/11/16/leetcode-2-Add-Two-Numbers.html"},{"title":"Java设计模式之观察者模式","text":"定义 在阎宏博士的《JAVA与模式》一书中开头是这样描述观察者（Observer）模式的：观察者模式是对象的行为模式，又叫发布-订阅(Publish/Subscribe)模式、模型-视图(Model/View)模式、源-监听器(Source/Listener)模式或从属者(Dependents)模式。观察者模式定义了一种一对多的依赖关系，让多个观察者对象同时监听某一个主题对象。这个主题对象在状态上发生变化时，会通知所有观察者对象，使它们能够自动更新自己。 推结构模式推模式相关结构说明一个软件系统里面包含了各种对象，就像一片欣欣向荣的森林充满了各种生物一样。在一片森林中，各种生物彼此依赖和约束，形成一个个生物链。一种生物的状态变化会造成其他一些生物的相应行动，每一个生物都处于别的生物的互动之中。 同样，一个软件系统常常要求在某一个对象的状态发生变化的时候，某些其他的对象做出相应的改变。做到这一点的设计方案有很多，但是为了使系统能够易于复用，应该选择低耦合度的设计方案。减少对象之间的耦合有利于系统的复用，但是同时设计师需要使这些低耦合度的对象之间能够维持行动的协调一致，保证高度的协作。观察者模式是满足这一要求的各种设计方案中最重要的一种。 下面以一个简单的示意性实现为例，讨论观察者模式的结构。 观察者模式所涉及的角色有： ● 抽象主题(Subject)角色：抽象主题角色把所有对观察者对象的引用保存在一个聚集（比如ArrayList对象）里，每个主题都可以有任何数量的观察者。抽象主题提供一个接口，可以增加和删除观察者对象，抽象主题角色又叫做抽象被观察者(Observable)角色。 ● 具体主题(ConcreteSubject)角色：将有关状态存入具体观察者对象；在具体主题的内部状态改变时，给所有登记过的观察者发出通知。具体主题角色又叫做具体被观察者(Concrete Observable)角色。 ● 抽象观察者(Observer)角色：为所有的具体观察者定义一个接口，在得到主题的通知时更新自己，这个接口叫做更新接口。 ● 具体观察者(ConcreteObserver)角色：存储与主题的状态自恰的状态。具体观察者角色实现抽象观察者角色所要求的更新接口，以便使本身的状态与主题的状态 像协调。如果需要，具体观察者角色可以保持一个指向具体主题对象的引用。 主题对象向观察者推送主题的详细信息，不管观察者是否需要，推送的信息通常是主题对象的全部或部分数据。 抽象观察者角色1234public interface Observer { void update(String state); String getName();} 具体观察者1234567891011121314151617181920212223242526public class ConcreteObserver implements Observer { private String name; private String state; public ConcreteObserver(String name) { this.name = name; } public String getState() { return state; } public void setState(String state) { this.state = state; } @Override public void update(String state) { // 更新观察 着状态 this.state = state; System.out.println(getName() + \"观察者状态更新为：\" + state); } @Override public String getName() { return name; }} 抽象主题角色1234567891011121314151617181920212223242526272829303132public abstract class Subject { /** * 保存观察者的容器 */ private List&lt;Observer&gt; list = new ArrayList&lt;Observer&gt;(); /** * 注册观察者 */ public void register(Observer o) { list.add(o); System.out.println(\"增加了一个观察者:\" + o.getName()); } /** * 移除观察者 * * @param o */ public void remove(Observer o) { System.out.println(\"移除了一个观察者:\" + o.getName()); list.remove(o); } /** * 通知观察者 * * @param newState */ public void nodifyObservers(String newState) { for (Observer observer : list) { observer.update(newState); } }} 具体主题角色12345678910111213141516public class ConcreteSubject extends Subject { /** * 状态 */ private String state; public String getState() { return state; } public void change(String newState) { state = newState; System.out.println(\"状态变为：\" + newState); System.out.println(\"开始通知观察者...\"); this.nodifyObservers(state); }} 测试类12345678910111213public class MainTest { public static void main(String[] args) { Observer o1 = new ConcreteObserver(\"o1\"); Observer o2 = new ConcreteObserver(\"o2\"); Observer o3 = new ConcreteObserver(\"o3\"); ConcreteSubject csj = new ConcreteSubject(); csj.register(o1); csj.register(o2); csj.register(o3); csj.remove(o2); csj.change(\"new State！\"); }} 输出结果 在运行时，这个客户端首先创建了具体主题类的实例，以及一个观察者对象。然后，它调用主题对象的register()方法，将这个观察者对象向主题对象登记，也就是将它加入到主题对象的聚集中去。 这时，客户端调用主题的change()方法，改变了主题对象的内部状态。主题对象在状态发生变化时，调用超类的notifyObservers()方法，通知所有登记过的观察者对象 拉模式结构说明 主题对象在通知观察者的时候，只传递少量信息。如果观察者需要更具体的信息，由观察者主动到主题对象中获取，相当于是观察者从主题对象中拉数据。一般这种模型的实现中，会把主题对象自身通过update()方法传递给观察者，这样在观察者需要获取数据的时候，就可以通过这个引用来获取了。 抽象观察者角色12345678public interface Observer { /** * 传入主题，获取中的对象 * @param subject */ void update(Subject subject); String getName();} 具体观察者123456789101112131415161718192021222324252627public class ConcreteObserver implements Observer { private String name; private String state; public ConcreteObserver(String name) { this.name = name; } public String getState() { return state; } public void setState(String state) { this.state = state; } @Override public String getName() { return name; } @Override public void update(Subject subject) { // 主动去主题里拿数据 state = ((ConcreteSubject) subject).getState(); System.out.println(getName() + \"观察者状态更新为：\" + state); }} 抽象主题角色1234567891011121314151617181920212223242526272829303132333435public abstract class Subject { /** * 保存观察者的容器 */ private List&lt;Observer&gt; list = new ArrayList&lt;Observer&gt;(); /** * 注册观察者 */ public void register(Observer o) { list.add(o); System.out.println(\"增加了一个观察者:\" + o.getName()); } /** * 移除观察者 * * @param o */ public void remove(Observer o) { System.out.println(\"移除了一个观察者:\" + o.getName()); list.remove(o); } /** * 通知观察者 * * @param newState */ public void nodifyObservers() { for (Observer observer : list) { observer.update(this); } }} 具体主题角色123456789101112131415161718public class ConcreteSubject extends Subject { /** * 状态 */ private String state; public String getState() { return state; } public void change(String newState) { state = newState; System.out.println(\"状态变为：\" + newState); System.out.println(\"开始通知观察者...\"); this.nodifyObservers(); }} 测试12345678910111213public class MainTest { public static void main(String[] args) { Observer o1 = new ConcreteObserver(\"o1\"); Observer o2 = new ConcreteObserver(\"o2\"); Observer o3 = new ConcreteObserver(\"o3\"); ConcreteSubject csj = new ConcreteSubject(); csj.register(o1); csj.register(o2); csj.register(o3); csj.remove(o2); csj.change(\"new State！\"); }} 测试结果 两种模式的比较 推模型是假定主题对象知道观察者需要的数据；而拉模型是主题对象不知道观察者具体需要什么数据，没有办法的情况下，干脆把自身传递给观察者，让观察者自己去按需要取值。 推模型可能会使得观察者对象难以复用，因为观察者的update()方法是按需要定义的参数，可能无法兼顾没有考虑到的使用情况。这就意味着出现新情况的时候，就可能提供新的update()方法，或者是干脆重新实现观察者；而拉模型就不会造成这样的情况，因为拉模型下，update()方法的参数是主题对象本身，这基本上是主题对象能传递的最大数据集合了，基本上可以适应各种情况的需要。 参考链接","link":"/2018/11/15/Java设计模式之观察者模式.html"},{"title":"Immutable Object(不可变对象)模式","text":"多线程下，一个对象会被多个线程共享，存在多线程并发地修改对象的属性，需要做些同步访问控制，如显示锁，CAS操作，会带来额外的开销和问题，如上下文切换、等待时间、ABA问题。Immutable Object模式意图通过使用对外可见的状态不可变的对象，使得天生具有线程安全性。 车辆管理系统状态可变的位置信息模型123456789101112131415161718192021public class Location { private double x; private double y; public Location(double x, double y) { this.x = x; this.y = y; } public double getX() { return x; } public double getY() { return y; } public void setXY(double x, double y) { this.x = x; this.y = y; }} 管理系统中会调用Location的setXY方法来更新位置，因为是非线程安全，并非原子操作，导致调用时会出现数据不一致的情况 改进：状态不可变的位置信息模型123456789public final class Location{ public final double x; public final double y; public Location(double x,double y){ this.x = x; this.y = y; }} 使用状态不可变的对象时，更新信息模型时，如果车辆的位置发生变动，更新的是整个位置信息的对象 更新不可变对象的位置信息123456public class VehicleTracker{ private Map&lt;String,Location&gt; locMap = new ConcurrentHashMap&lt;String, Location&gt;(); public void updateLocation(String vehicleId,Location newLocation){ locMap.put(vehicleId,newLocation); } } 一个严格意义上的不可变对象应该满足以下所有条件 类本身用final修饰 所有字段都是用final修饰，这个语意在多线程环境下由JVM保证了被修饰字段所引用对象的初始化安全，即final修饰的字段在其他线程是可见的，必定是初始化完成的。 在对象的创建过程中，this关键字没有泄露给其他类，防止其他类在对象创建过程中修改其状态 任何字段如果引用其他状态可变的对象，如集合数组，这些字段必须是private修饰的，不能暴露给外部，所有相关方法要返回这些字段值，应该防止防御性复制 实例： 某彩信网关系统 在处理由增值业务提供商VASP下发给手机终端用户的彩信信息时，需要根据彩信接收方号码的前缀选择对应的彩信中心MMSC，然后转发消息给选中的彩信中心。由其他系统将彩信信息下发给手机终端用户。选择彩信中心的过程称为 路由 ，手机前缀和彩信中心对应的关系叫路由表，在系统中多线程共享，很少改变此数据，不希望访问这些数据时进行加锁并发访问控制，避免产生不必要的开销，所以选择immutable object模型。 彩信中心路由规则管理器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/***彩信中心路由规则管理器**/public final class MMSCRouter{ // 保证多线程环境下该变量的可见性 private static volatile MMSCRouter instance = new MMSCRouter(); // 维护手机号码前缀到彩信中心之间的映射关系 private final Map&lt;String,MMSCInfo&gt; routerMap; public MMSCRouter(){ // 将数据库表中的数据加载到内存，存为Map this.routerMap = MMSCRouter.retrieveRouterMapFromDB(); } private static Map&lt;String,MMSCInfo&gt; retrieveRouterMapFromDB(){ Map&lt;String,MMSCInfo&gt; map = new HashMap&lt;&gt;(); // 省略其余代码 return map; } public static MMSCRouter getInstance(){ return instance; } /** *根据手机号前缀获取彩信中心信息 **/ public MMSCInfo getMMSC(String msisdPrefix){ return routerMap.get(msisdPrefix); } /** *更新为指定的新实例 **/ public static void setInstance(MMSCRouter newInstance){ instance = newInstance; } /** *防御性复制 **/ private static Map&lt;String,MMSCInfo&gt; deepCopy(Map&lt;String,MMSCInfo&gt; m){ Map&lt;String,MMSCInfo&gt; result = new HashMap&lt;String,MMSCInfo&gt;(); for(String key : m.keySet()){ result.put(key, new MMSCInfo(m.get(key))); } return result; } // 防止外部代码修改可变数据routerMap的值 public Map&lt;String,MMSCInfo&gt; getRouterMap(){ return Collections.unmodifiableMap(deepCopy(routerMap)); }} 彩信中心信息123456789101112131415161718192021222324252627282930public final class MMSCInfo{ private final String deviceId; private final String url; private final int maxAttachmentSizeInBytes; public MMSCInfo(String deviceId, String url, int maxAttachmentSizeInBytes){ this.deviceId = deviceId; this.url = url; this.maxAttachmentSizeInBytes = maxAttachmentSizeInBytes; } public MMSCInfo(MMSCInfo protoType){ this.deviceId = protoType.deviceId; this.url = protoType.url; this.maxAttachmentSizeInBytes = protoType.maxAttachmentSizeInBytes; } public String getDeviceId(){ return deviceId; } public String getUrl(){ return url; } public int getMaxAttachmentSizeInBytes(){ return maxAttachmentSizeInBytes; } } 彩信中心信息变更的频率也同样不高。因此，当彩信网关系统通过网络被通知到这种彩信中心信息本身或者路由变更时，网关系统会重新生成新的MMSInfo和MMSRouter来反应变更。 彩信中心、路由表的变更123456789101112131415161718public class OMCAgent extends Thread{ @Override public void run(){ boolean isTableModificationMsg = false; String updatedTableName = null; while(true){ // 省略代码 从与OMC 连接中读取信息进行解析 // 解析到数据表更新信息后，重置MMSCRouter实例 if(isTableModificationMsg){ if(\"MMSCInfo\".equals(updatedTableName)){ // new MMSCRouter() 从数据库中加载变更的信息存入 MMSCRouter.setInstance(new MMSCRouter()); } } // 省略其他代码 } }} 本列中MMSCInfo 是一个严格意义上的不可变对象，虽然MMSCRouter对象对外提供了setInstance方法用于改变静态字段instance的值，但它仍然可被视作一个等效的不可变对象。因为setInstance仅仅改变instance变量指向的对象，而instance变量采用volatile修饰保证了其余线程的可见性，所以无需加锁其他线程也能获取到最新的instance 总结Immutable Object 模型使用场景 被建模对象的状态变化不频繁 同时对一组相关的数据进行写操作，因此需要保证原子性 使用某个对象作为安全的HashMap的可以key。由于final不可变对象不变所有hashcode不变，所以适合作为HashMap 的key。 参考文献java多线程编程实战指南（设计模式篇）黄文海/著","link":"/2018/11/13/Immutable-Object-不可变对象-模式.html"},{"title":"elasticsearch6 query 全文查询与词项查询","text":"query全文查询 QueryBuilders.matchQuery(“filed”,”value”).operator(Operator.AND); // 对查询的语句进行分词，分词后的词任意一个匹配doc都能查出来 term query 查询的是词项&lt;分词后的&gt; （eg：Java编程思想） Java编程 term query 不能查到 分词后变成（Java 编程 思想） matchQuery能查到 QueryBuilders.matchPhraseQuery(“field”,”value”);对value进行分词，可以自定义分词器,满足两个条件才能被搜到： 分词后的所有词项都要匹配原字段 顺序还需要一致 QueryBuilders.matchPhrasePrefixQuery(“field”,”value”);与matchPhraseQuery类似,最后一个term支持前缀匹配eg.matchPhraseQuery 查 “hello word” matchPhrasePrefixQuery只需要查 “hello w”即可 QueryBuilders.multiMatchQuery(“value”,”field1”,”field2”); 多字段支持查询，字段可以使用通配符eg,{&quot;中国&quot;,&quot;tit*&quot;,&quot;wor?&quot;} QueryBuilders.commonTermsQuery(“哇”,”hehe”);通用查询，会自动分词为低频和高频项，先查低频，可以控制低频、高频出现概率 eg.the word the就是高频 ，可以先查 word QueryBuilders.queryStringQuery(“”);支持lucene查询语法 QueryBuilders.simpleQueryStringQuery(“”);支持lucene查询语法，具有非常完善的语法查询，解析过程中出现异常不会抛错 QueryBuilders.matchAllQuery();查所有和不写同样效果 词项查询 term query 词项检索 terms query 词项检索，可以多个词项，查到一个都能匹配结果 range query 查询范围内的 gt 大于 gte 大于等于 lt 小于 lte 小于等于 exist query 查询会返回字段中至少有一个非空空字符串也返回的doc prefix query 查询字段中给定前缀的文档 eg.{&quot;title&quot;:&quot;hel&quot;} wildcard query 查询字段通配符eg.&quot;{&quot;title&quot;:&quot;hell?/ *ell*&quot;} regexp query 正则匹配查询eg.{&quot;title&quot;:&quot;W[0-9].+&quot;} fuzzy query 模糊查询，最接近的查询，单词拼错一个字母的时候，消耗资源多 type query 指定类型的文档 ids query 查询具有指定id的文档","link":"/2018/11/13/elasticsearch6-query-全文查询与词项查询.html"},{"title":"leetcode-1-Two Sum","text":"description Given an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. Example: Given nums = [2, 7, 11, 15], target = 9, Because nums[0] + nums[1] = 2 + 7 = 9, return [0, 1]. common method 1234567891011121314class Solution { public int[] twoSum(int[] nums, int target) { int[] ret = new int[2]; for(int i =0; i&lt;nums.length-1 ;i++){ for (int j = i+1 ;j &lt; nums.length ;j++ ){ if (nums[i] + nums[j] == target){ ret = new int[]{i, j}; return ret; } } } return ret ; }} best method1234567891011121314151617class Solution { public int[] twoSum(int[] nums, int target) { int len=nums.length; HashMap&lt;Integer, Integer&gt; map=new HashMap&lt;&gt;(); map.put(nums[0], 0); for(int i=1;i&lt;len;i++){ if(map.containsKey(target-nums[i])){ int[] returnArray={map.get(target-nums[i]),i}; return returnArray; } else{ map.put(nums[i], i); } } int[] returnArray={0,0}; return returnArray; }}","link":"/2018/11/11/leetcode-1-Two-Sum.html"},{"title":"Hello blog","text":"this is a first blog.It's a very exciting time make a plan execute have a harvest come on 12345public void start(){ while(true){ System.out.println(\"struggle！\"); }}","link":"/2018/11/11/Hello-blog.html"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \" My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2018/11/11/hello-world.html"}],"tags":[{"name":"Effective-Java","slug":"Effective-Java","link":"/tags/Effective-Java/"},{"name":"读书笔记","slug":"读书笔记","link":"/tags/读书笔记/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"设计模式","slug":"设计模式","link":"/tags/设计模式/"},{"name":"elasticsearch6","slug":"elasticsearch6","link":"/tags/elasticsearch6/"},{"name":"query","slug":"query","link":"/tags/query/"},{"name":"think","slug":"think","link":"/tags/think/"},{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"arithmetic","slug":"arithmetic","link":"/tags/arithmetic/"},{"name":"工具教程","slug":"工具教程","link":"/tags/工具教程/"},{"name":"icarus主题配置","slug":"icarus主题配置","link":"/tags/icarus主题配置/"},{"name":"hexo主题","slug":"hexo主题","link":"/tags/hexo主题/"},{"name":"正则表达式","slug":"正则表达式","link":"/tags/正则表达式/"},{"name":"索引分词","slug":"索引分词","link":"/tags/索引分词/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"索引","slug":"索引","link":"/tags/索引/"},{"name":"经验成长","slug":"经验成长","link":"/tags/经验成长/"},{"name":"幂等性,restful-api","slug":"幂等性-restful-api","link":"/tags/幂等性-restful-api/"},{"name":"支付系统","slug":"支付系统","link":"/tags/支付系统/"},{"name":"支付架构","slug":"支付架构","link":"/tags/支付架构/"},{"name":"sql","slug":"sql","link":"/tags/sql/"},{"name":"法律","slug":"法律","link":"/tags/法律/"},{"name":"集合","slug":"集合","link":"/tags/集合/"},{"name":"java,并发,多线程","slug":"java-并发-多线程","link":"/tags/java-并发-多线程/"}],"categories":[{"name":"java","slug":"java","link":"/categories/java/"},{"name":"设计模式","slug":"java/设计模式","link":"/categories/java/设计模式/"},{"name":"读书笔记","slug":"java/读书笔记","link":"/categories/java/读书笔记/"},{"name":"elasticsearch6","slug":"java/elasticsearch6","link":"/categories/java/elasticsearch6/"},{"name":"java基础","slug":"java/java基础","link":"/categories/java/java基础/"},{"name":"arithmetic","slug":"java/arithmetic","link":"/categories/java/arithmetic/"},{"name":"工具教程","slug":"工具教程","link":"/categories/工具教程/"},{"name":"主题工具","slug":"工具教程/主题工具","link":"/categories/工具教程/主题工具/"},{"name":"JVM","slug":"java/JVM","link":"/categories/java/JVM/"},{"name":"基础工具类","slug":"基础工具类","link":"/categories/基础工具类/"},{"name":"数据库","slug":"数据库","link":"/categories/数据库/"},{"name":"经验成长","slug":"经验成长","link":"/categories/经验成长/"},{"name":"正则","slug":"基础工具类/正则","link":"/categories/基础工具类/正则/"},{"name":"mysql","slug":"数据库/mysql","link":"/categories/数据库/mysql/"},{"name":"架构","slug":"架构","link":"/categories/架构/"},{"name":"设计","slug":"架构/设计","link":"/categories/架构/设计/"},{"name":"法律","slug":"法律","link":"/categories/法律/"},{"name":"并发","slug":"java/并发","link":"/categories/java/并发/"}]}